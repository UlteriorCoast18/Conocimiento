\documentclass[../machine_learning_scikit.tex]{subfiles}

\begin{document}

    \chapter{A Tour of Machine Learning Classifiers Using Scikit-Learn}

    \section{Introduction}

    The goal of this chapter is to use Scikit-Learn to impelement some classifiers in order to learn how to use them, his advantages, disadvantages and different use scenarios.

    Specifically, we will take a look of 4 popular machine learning models commonly used in academia and in industry. In addition, we will take a look at the Scikit-Learn library, which offers a user-friendly and consistent interface for using those algorithms efficiently and productively.
    
    \subsection{Choosing a Classification Algorithm}

    Each algorithm has its own quirks and relies on certain assumptions. no single classifier works best across all possible scenarios (The Lack of A Priori Distinctions Between Learning Algorithms, Wolpert, David H, Neural Computation 8.7 (1996): 1341-1390).

    \begin{obs}[\textbf{Comparasion}]
        In practice, it is always recomended to compare the behaviour of different algorithms in order to find the best model suitable for a particular problem; these may differ in the number of features or examples, the amount of noise in a dataset, and whether the classes are linearly separable.
    \end{obs}

    \begin{idea}
        The performance of a classifier relies upon the data that is available for learning. The five main steps that are involved in training a supervised machine learning algorithm can be summarized as follows:
        \begin{enumerate}
            \item Selecting features and collecting labeled training examples
            \item Choosing a performance metric
            \item Choosing a learning algorithm and training a model
            \item Evaluating the performance of the model
            \item Changing the settings of the algorithm and tuning the model.
        \end{enumerate}
    \end{idea}

    We will mainly focus on the main concepts of the different algorithms in this chapter and revisit topics such as feature selection and preprocessing, performance metrics, and hyperparameter tuning for more detailed discussions later in the book.

    \subsection{First Steps with scikit-learn Training a Perceptron}

    Before we learn about two related learning algorithms: the perceptron and adaline, both implemented in Python using NumPy and other libraries by ourselves.

    \begin{center}
        Now we will take a look at the \textbf{scikit-learn API}.
    \end{center}

    \begin{obs}
        One of the advantages of using scikit-learn is that combines a user-friendly and consistent interface with a highly optimized implementation of several classification algorithms. Also, this library offers a not only a large variety of learning algorithms, but also many convenient functions to preprocess data and to fine-tune and evaluate our models.
    \end{obs}

    \subsection{Training a Model Using Scikit-Learn}

    To get started with the scikit-learn library, we will train a perceptron model similar to the one that we implemented in Chapter 2. For simplicity, we will use the already familiar Iris dataset throughout the following sections.

    \begin{obs}[\textbf{Iris Dataset and Its Uses}]
        Conveniently, the Iris dataset is already available via scikit-learn, since it is a simple yet popular dataset that is frequently used for testing and experimenting with algorithms. Similar to the previous chapter, we will only use two features from the Iris dataset for visualization purposes.
    \end{obs}

    We will assign the petal length and petal width of the 150 flower examples to the feature matrix, \lstinline|X|, and the corresponding class labels of the flower species to the vector array, \lstinline|y|:

    \begin{lstlisting}[caption={Class Labels of Matrix \lstinline|X|},label=matrix_labels]
Class labels: [0 1 2]
Class labels: [0 1 2]
    \end{lstlisting}

    The \lstinline|np.unique(y)| function returned the three unique class labels stored in \lstinline|iris.target|, and as we can see, the Iris flower class names, \lstinline|Iris-setosa|, \lstinline|Iris-versicolor|, and \lstinline|Iris-virginica|, are already stored as integers (here: \lstinline|0|, \lstinline|1|, \lstinline|2|).
    
    \begin{obs}[\textbf{Scikit-Learn with Class Labels and String Formats}]
        Although many scikit-learn functions and class methods also work with \textit{class labels in string format, using integer labels is a recommended approach to avoid technical glitches and improve computational performance due to a smaller memory footprint}; furthermore, \textit{encoding class labels as integers is a common convention among most machine learning libraries}.
    \end{obs}

    To evaluate how well a trained model performs on unseen data, we will \textit{further split the dataset into separate training and test datasets}.
    
    \begin{idea}[\textbf{More Info About Best Practices around Model Evaluation}]
        In Chapter 6, \textit{Learning Best Practices for Model Evaluation and Hyperparameter Tuning}, we \textbf{will discuss the best practices around model evaluation in more detail}.
    \end{idea}

    Using the \lstinline|train_test_split| function from scikit-learn's \lstinline|model_selection| module, we randomly split the \lstinline|X| and \lstinline|y| arrays into 30 percent test data (45 examples) and 70 percent training data (105 examples):

    \begin{lstlisting}[caption={Train Test Split Function},label=code:train_test_split]
train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)
    \end{lstlisting}

    \begin{obs}
        Note that the \lstinline|train_test_split| function already shuffles the training datasets internally before splitting; otherwise, all examples from class \lstinline|0| and class \lstinline|1| would have ended up in the training datasets, and the test dataset would consist of 45 examples from class \lstinline|2|. Via the \lstinline|random_state| parameter, we provided a fixed random seed (\lstinline|random_state=1|) for the internal pseudo-random number generator that is used for shuffling the datasets prior to splitting. Using such a fixed \lstinline|random_state| ensures that our results are reproducible.
    \end{obs}

    Lastly, we took advantage of the built-in support for stratification via \lstinline|stratify=y|. In this context, \textit{stratification means that the} \lstinline|train_test_split| \textit{method returns training and test subsets that have the same proportions of class labels as the input dataset}. We can use NumPy's \lstinline|bincount| function, which counts the number of occurrences of each value in an array, to verify that this is indeed the case:

    \begin{lstlisting}[caption={Use of \lstinline|bincount| Method in Python.},label=code:use_bicount]
>>> print('Labels counts in y:', np.bincount(y))
Labels counts in y: [50 50 50]
>>> print('Labels counts in y_train:', np.bincount(y_train))
Labels counts in y_train: [35 35 35]
>>> print('Labels counts in y_test:', np.bincount(y_test))
Labels counts in y_test: [15 15 15]
    \end{lstlisting}

    \begin{idea}[\textbf{Feature Scaling}]
        Many machine learning and optimization algorithms also require \textbf{feature scaling for optimal performance}, as we saw in the gradient descent example in Chapter 2. Here, \textbf{we will standardize the features using the StandardScaler class from scikit-learn's preprocessing module}:

        \begin{lstlisting}[caption={\lstinline|StandardScaler| Method in Python.},label=code:standard_scaler]
sc = StandardScaler()
        \end{lstlisting}

        Using the preceding code, we loaded the \lstinline|StandardScaler| class from the \lstinline|preprocessing| module and initialized a new \lstinline|StandardScaler| object that we assigned to the \lstinline|sc| variable.
    \end{idea}

    Using the \lstinline|fit| method, \lstinline|StandardScaler| estimated the parameters, $\mu$ (sample mean) and $\sigma$ (standard deviation), for each feature dimension from the training data. By calling the \lstinline|transform| method, we then standardized the training data using those estimated parameters, $\mu$ and $\sigma$. Note that we used the same scaling parameters to standardize the test dataset so that both the values in the training and test dataset are comparable with one another.

    \begin{exa}
        Having standardized the training data, we can now train a perceptron model. Most algorithms in scikit-learn already support multiclass classification by default via the one-versus-rest (OvR) method, which allows us to feed the three flower classes to the perceptron all at once.
    \end{exa}

    Finally, we have the code for the model:

    \lstinputlisting[caption={Perceptron Trained Model using Sklearn}, label={code:perceptron_sklearn}, language=Python]{Code/sklearn_first/sklearn_first.py}

    With the slight modification that we made to the \lstinline|plot_decision_regions| function, we can now specify the indices of the examples that we want to mark on the resulting plots.

    As we can see in the resulting plot, the three flower classes can't be perfectly separated by a linear decision boundary:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/Iris_perceptron_scikit_learn.png} \\
            \caption{Perceptron Trained with Iris Dataset using Scikit-Learn.}
            \label{figure:perceptron_iris_scikit_learn}
        \end{minipage}
    \end{figure}

    However, remember from our discussion in Chapter 2 that \textbf{the perceptron algorithm never converges on datasets that aren't perfectly linearly separable}, which is \textit{why the use of the perceptron algorithm is typically not recommended in practice}. In the following sections, we will look at more powerful linear classifiers that converge to a loss minimum even if the classes are not perfectly linearly separable.

    \begin{obs}[\textbf{Note}]
        The Perceptron, as well as other scikit-learn functions and classes, \textit{often has additional parameters that we omit for clarity. You can read more about those parameters using the help function in Python (for instance, \lstinline|help(Perceptron)|) or by going through the excellent scikit-learn online documentation} \href{http://scikit-learn.org/stable/}{here}.
    \end{obs}

    \newpage

    \section{Modeling Class Probabilities Via Logistic Regression}

    The biggest disadvantage of the Perceptron is that it never converges if the classes aren't linearly separable. The reason for this is that the weights are continuously updated because there is always at least one misclassified training example present in each poch.

    To make better use of our time, we will now look at another simple, yet more powerful, algorithm for linear and binary classification problems: \textbf{logistic regression}. Note that, despite its name, \textit{logistic regression is a model for classification, not regression}.

    \subsection{Logistic Regression and Conditional Probabilities}

    \begin{mydef}[\textbf{Logistic Regression}]
        \textbf{Logistic regression} is a \textit{classification model that is easy to implement and performs very well on linearly separable classes}. It is \textit{one of the most widely used algorithms for classification in industry}.
    \end{mydef}

    Similar to the perceptron and Adaline, \textit{the logistic regression model in this section is also a linear model for binary classification}.

    \begin{idea}[\textbf{Note}]
        Logistic regression can be generalized to multinomial logistic regression.
    \end{idea}

    To explain the main mechanics behind logistic regression as a probabilistic model for binary classification, let's first introduce the odds: the odds in favor of a particular event. The odds can be written as $\frac{p}{1-p}$, where $p$ stands for the probability of the positive event. The term "positive event" does not necessarily mean "good," but refers to the event that we want to predict.
    
    \begin{exa}[\textbf{Conditional Probability}]
        Consider the \textbf{probability that a patient has a certain disease given certain symptoms}; we can think of the positive event as class label $y=1$ and the symptoms as features $\boldsymbol{x}$. Hence, for brevity, we can define the probability $p$ as $p:=p(y=1|\boldsymbol{x})$, the conditional probability that a particular example belongs to class $1$ given its features, $\boldsymbol{x}$.
    \end{exa}

    We can then further define the logit function, which is simply the logarithm of the odds (log-odds):

    \begin{equation}
        \label{logit_function}
        \logit{p}=\log\left(\frac{p}{1-p}\right)
    \end{equation}

    Note that $\log$ \textit{refers to the natural logarithm, as it is the common convention in computer science}. The $\textup{logit}$ function \textit{takes input values in the range 0 to 1 and transforms them into values over the entire real-number range}.

    Under the logistic model, we \textbf{assume that there is a linear relationship between the weighted inputs and the log-odds}:
    \begin{equation}
        \label{logit_function_assumption}
        \logit{p}=w_1x_1+\cdots+w_nx_n+b=\sum_{i=1}^n w_ix_i+b=\boldsymbol{w}^T\boldsymbol{x}+b
    \end{equation}

    \begin{obs}
        While the preceding describes an assumption we make about the linear relationship between the log-odds and the net inputs, what \textbf{we are actually interested in is the probability} $p$, the \textit{class‐membership probability of an example given its features}. While the logit function maps the probability to a real‐number range, \textit{we can consider the inverse of this function to map the real‐number range back to a} $[0,1)$ range for the probability $p$.
    \end{obs}

    This inverse of the logit function is typically called the \textbf{logistic sigmoid function}, which is \textit{sometimes simply abbreviated to sigmoid function due to its characteristic $S$-shape}:

    \begin{equation}
        \label{sigmoid_function}
        \sigma(z)=\frac{1}{1+e^{-z}},\quad\forall z\in\bbm{R}
    \end{equation}
    Here, $z$ is the \textit{net input, the linear combination of weights and the inputs} (that is, the features associated with the training examples):
    \begin{equation}
        \label{z_sigmoid_function}
        z=\boldsymbol{w}^T\boldsymbol{x}+b
    \end{equation}

    \begin{exa}[\textbf{Graph of Sigmoid Function}]
        Graph of Sigmoid Function:

        \begin{lstlisting}[caption={Sigmoid Function Graph.},label=figure:sigmoid_function_graph]
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

z = np.arange(-7, 7, 0.1)
sigma_z = sigmoid(z)
plt.plot(z, sigma_z)
plt.axvline(0.0, color='k')
plt.ylim(-0.1, 1.1)
plt.xlabel('z')
plt.ylabel('$\\sigma (z)$')
# y-axis ticks and gridline
plt.yticks([0.0, 0.5, 1.0])
ax = plt.gca()
ax.yaxis.grid(True)
plt.tight_layout()
plt.show()
        \end{lstlisting}
    \end{exa}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/sigmoid_function_graph.png} \\
            \caption{Sigmoid Function Graph.}
            \label{figure:sigmoid_function_graph}
        \end{minipage}
    \end{figure}

    As a result of executing the previous code, you should now see the $S$-shaped (sigmoidal) curve.  

    \begin{obs}[\textbf{Comparasion with Adaline}]
        To build some understanding of the logistic regression model, we can relate it to Adaline. In Adaline, we used the identity function, $\sigma(z)=z$ as the activation function. In logistic regression, \textit{this activation function simply becomes the sigmoid function defined earlier}.
    \end{obs}

    The only difference between Adaline and logistic regression is the activation function.  

    \begin{mydef}[\textbf{Purpose of an Activation Function}]
        In simple terms, the purpose of an \textbf{activation function} is \textit{to decide whether a neuron in a neural network should be "activated" or not, and to what degree. It's what allows neural networks to learn and model complex, non-linear relationships}.
    \end{mydef}

    The output of the sigmoid function is then interpreted as the probability of a particular example belonging to class 1, $\sigma(z) = p(y=1 | \boldsymbol{x}; \boldsymbol{w},b)$, given its features $\boldsymbol{x}$, and parameterized by the weights and bias, $\boldsymbol{w}$ and $b$. 

    For example, if we compute $\sigma(z) = 0.8$ for a particular flower, it means that the chance that this example is an \lstinline|Iris-versicolor| flower is 80 percent. Therefore, the probability that this flower is an \lstinline|Iris-setosa| can be calculated as $p(y=0 |\boldsymbol{x}; \boldsymbol{w},b) = 1 - p(y=1 | \boldsymbol{x}; \boldsymbol{w},b) = 0.2$, or 20 percent.

    The predicted probability can then be converted into a binary outcome via a threshold function:  

    \begin{equation*}
        \hat{y}=\left\{
            \begin{array}{lr}
                1 & \textup{ if }\sigma(z)\geq0.5\\
                0 & \textup{ otherwise} \\
            \end{array}
        \right.
    \end{equation*}

    If we look at the preceding plot of the sigmoid function, this is equivalent to the following:  

    \begin{equation*}
        \hat{y}=\left\{
            \begin{array}{lr}
                1 & \textup{ if }z\geq0\\
                0 & \textup{ otherwise} \\
            \end{array}
        \right.
    \end{equation*}

    \begin{obs}[\textbf{Interest in Predicted Class Labels}]
        In many applications, \textbf{we are not only interested in the predicted class labels but also in the class‐membership probabilities produced by the sigmoid function before applying the threshold}.
        
        Logistic regression is \textit{used in weather forecasting, for example, not only to predict whether it will rain on a particular day but also to report the chance of rain}. Similarly, logistic regression \textit{can be used to predict the chance that a patient has a particular disease given certain symptoms, which is why logistic regression enjoys great popularity in the field of medicine}.
    \end{obs}

    \section{Learning the Model Weights via the Logistic Loss Function}

    \textit{You have learned how we can use the logistic regression model to predict probabilities and class labels}; now, let's briefly talk about how \textit{we fit the parameters of the model, for instance, the weights and bias unit, $\boldsymbol{w}$ and $b$}. Previously, we defined the mean squared error loss function as follows:

    \begin{equation*}
        L(\boldsymbol{w},b|\boldsymbol{x})=\frac{1}{n}\sum_{ i=1}^n\left(y^{(i)}-\sigma(z^{(i)})\right)^2
    \end{equation*}

    \textit{We minimized this function in order to learn the parameters for our Adaline classification model}. To explain how we can derive the loss function for logistic regression, \textit{let's first define the likelihood}, $\mathcal{L}$, that we \textbf{want to maximize when we build a logistic regression model}, assuming that the individual examples in our dataset are independent of one another. The formula is as follows:

    \begin{equation}
    L(w, b \mid x) = p(y|x;w,b) = \prod_{i=1}^{n} p(y^{(i)} \mid x^{(i)}; w, b) = \prod_{i=1}^{n} \left(\sigma(z^{(i)})\right)^{y^{(i)}} \left(1 - \sigma(z^{(i)})\right)^{(1-y^{(i)})}
    \end{equation}

    This is the \textbf{likelihood function} $L(w, b \mid x)$ in \textit{terms of a product over $n$ terms}. It is expressed as the product of probabilities $p(y^{(i)} \mid x^{(i)}; w, b)$ for $i$ from 1 to $n$, which is further expanded to the product of $(\sigma(z^{(i)}))^{y^{(i)}}$ times $(1 - \sigma(z^{(i)}))^{(1-y^{(i)})}$, where $\sigma$ is the sigmoid function.

    \begin{obs}
        In practice, it is easier to maximize the (natural) log of this equation, which is called the log-likelihood function:
    
        \begin{equation}
        l(w, b \mid x) = \log\left(\mathcal{L}\left(w, b \mid x\right) \right) = \sum_{i=1}^{n} \left[ y^{(i)} \log(\sigma(z^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(z^{(i)})) \right]
        \end{equation}

        Applying the log function reduces the potential for numerical underflow if the likelihoods are very small. In addition, the product of factors becomes a summation of factors, which makes it easier to obtain the derivative of this function via the addition trick.
    \end{obs}

    \subsection{Deriving the likelihood function}

    We can obtain the expression for the likelihood of the model given the data, $\mathcal{L}(w, b \mid x)$, as follows. Given that we have a binary classification problem with class labels 0 and 1, we can think of the label 1 as a Bernoulli variable—it can take on two values, 0 and 1, with the probability $p$ of being 1: $Y \sim \mathrm{Bernoulli}(p)$. 

    For a single data point, we can write this probability as:
    \begin{align}
    P(Y=1 \mid X=x^{(i)}) &= \sigma(z^{(i)}) \\
    P(Y=0 \mid X=x^{(i)}) &= 1 - \sigma(z^{(i)})
    \end{align}

    Putting these two expressions together, and using the shorthand $P(Y=y^{(i)} \mid X=x^{(i)}) = p(y^{(i)} \mid x^{(i)})$, we get the probability mass function of the Bernoulli variable:
    \begin{equation*}
        p\left(y^{(i)}\Big|x^{(i)}\right)=\left(\sigma(z^{(i)})\right)^{(y^{(i)})}\left(1-\sigma(z^{(i)})\right)^{1-y^{(i)}}
    \end{equation*}
    Substituting the probability mass function of the Bernoulli variable, we arrive at the expression for the likelihood, which we attempt to maximize by changing the model parameters:
    \begin{equation*}
        \mathcal{L}(w,b\Big|x)=\prod_{ i=1}^n \left(\sigma(z^{(i)})\right)^{(y^{(i)})}\left(1-\sigma(z^{(i)})\right)^{1-y^{(i)}}
    \end{equation*}

    \begin{obs}
        We could use an optimization algorithm such as \textbf{gradient ascent} to maximize this log-likelihood function. (\textit{Gradient ascent works the same way as gradient descent explained earlier, except that gradient ascent maximizes a function instead of minimizing it.}).
    \end{obs}

    Alternatively, let's rewrite the log-likelihood as a loss function, $L$, that can be minimized using gradient descent:
    \begin{equation*}
        L(w,b)=\sum_{i=1}^{n} \left[-y^{(i)} \log(\sigma(z^{(i)})) - (1 - y^{(i)}) \log(1 - \sigma(z^{(i)})) \right]
    \end{equation*}

    \begin{exa}
        For one single training example, looking at the equation, we can see that the first term becomes zero if $y = 0$, and the second term becomes zero if $y = 1$:
        \begin{equation*}
            L(\sigma(z),y,w,b)=\left\{
                \begin{array}{lr}
                    -\log\left(\sigma(z)\right) & \textup{ if }y=0\\
                    -\log\left(1-\sigma(z)\right) & \textup{ if }y=1\\
                \end{array}
            \right.
        \end{equation*}

        Let's write a short code snippet to create a plot that illustrates the loss of classifying a single training example for different values of $\sigma(z)$. The resulting plot shows the sigmoid activation on the x axis in the range 0 to 1 (the inputs to the sigmoid function were z values in the range -10 to 10) and the associated logistic loss on the y axis:
    \end{exa}

    \begin{figure}[h]
            \begin{minipage}{\textwidth}
                \centering
                \includegraphics[scale=1]{images/sigomoid_regression.png} \\
                \caption{Loss Function.}
                \label{figure:loss_function}
            \end{minipage}
        \end{figure}

    We can see that the loss approaches 0 (continuous line) if we correctly predict that an example belongs to class 1. Similarly, the loss also approaches 0 if we correctly predict y = 0 (dashed line). However, if the prediction is wrong, the loss goes toward infinity. The main point is that we penalize incorrect predictions with an increasingly larger loss.

    \section{Converting an Adaline Implementation Into an Algorithm for Logistic Regression}

    If we implement logistic regression from scratch, we can substitute the loss function, $L$, in the earlier Adaline implementation with the new loss function:

    \begin{equation*}
        L(w,b)=\frac{1}{n}\sum_{i=1}^{n} \left[-y^{(i)} \log(\sigma(z^{(i)})) - (1 - y^{(i)}) \log(1 - \sigma(z^{(i)})) \right]
    \end{equation*}

    \begin{obs}
        We use this to compute the loss of classifying all training examples per epoch. We also swap the linear activation function for the sigmoid. With those changes, the Adaline code becomes a working logistic regression implementation.
    \end{obs}

    The following example uses full-batch gradient descent (the same changes can be applied to the stochastic variant):  

    \lstinputlisting[caption={Implementation of Logistic Regression in Python.},label=code:logistic_regression_implementation, language=Python]{Code/Logistic Regression/LogisticRegressionGD.py}

    \begin{center}
        \textit{When fitting a logistic regression model, remember that it only works for binary classification tasks.}
    \end{center}

    The resulting decision region plot looks as follows:

    \section{Gradient Descent}

    One observation we can make on the latter code is that the partial derivative of the loss function $L$ is really simple to compute, this is due to the fact:

    \begin{equation*}
        \frac{\partial L}{\partial w_j}=\frac{\partial L}{\partial \sigma}\cdot\frac{\partial \sigma}{\partial z}\cdot\frac{\partial z}{\partial w_j}
    \end{equation*}
    where:
    \begin{equation*}
        \begin{split}
            \frac{\partial L}{\partial \sigma}&=\frac{\sigma-y}{\sigma\cdot(1-\sigma)}\\
            \frac{\partial \sigma}{\partial z}&=\sigma(1-\sigma)\\
            \frac{\partial z}{\partial w_j}&=x_j\\
        \end{split}
    \end{equation*}
    So,
    \begin{equation*}
        \frac{\partial L}{\partial w_j}=-(y-\sigma)x_j
    \end{equation*}

    Recall that gradient descent takes steps in the opposite direction of the gradient. Hence, we flip $\nabla L(w)$ and update the j-th weight as follows, including the learning rate $\eta$:
    \begin{equation*}
        w_j=w_j-\eta\cdot\frac{\partial L}{\partial w_j}(w)
    \end{equation*}
    While the partial derivative of the loss function with respect to the bias unit is not shown, bias derivation follows the same concept.

    \begin{obs}
        We should compute the mean in all this process, but due we are working in this example with a single record for simplicity.
    \end{obs}

    \begin{idea}[\textbf{Relation between Adaline and Logistical Regression}]
        Both the weight and bias updates are the same as for Adaline, but the change is in the activation function.
    \end{idea}

    \section{Training a Logistic Regression Model with Scikit-Learn}

    We just went through useful coding and math exercises in the previous subsection, which helped to illustrate the conceptual differences between Adaline and logistic regression. Now, let's learn how to use scikit-learn's more optimized implementation of logistic regression, which also supports multiclass settings off the shelf.
    
    \begin{obs}
        Note that in recent versions of scikit-learn, the technique used for multiclass classification—multinomial or OvR—is chosen automatically.
    \end{obs}
    
    In the following code example, we will use the \lstinline|sklearn.linear_model.LogisticRegression| class as well as the familiar fit method to train the model on all three classes in the standardized flower training dataset. Also, we set \lstinline|multi_class='ovr'| for illustration purposes. As an exercise, you may want to compare the results with \lstinline|multi_class='multinomial'|. Note that the \lstinline|multinomial| setting is now the default choice in scikit-learn's \lstinline|LogisticRegression| class and recommended in practice for mutually exclusive classes, such as those found in the Iris dataset.
    
    \begin{obs}
        Here, "mutually exclusive" means that each training example can belong to only a single class (in contrast to multilabel classification, where a training example can be a member of multiple classes).
    \end{obs}

    Now, let's look at the code example:

    \lstinputlisting[caption={Logistic Regression Iris Dataset Python.},label=code:logistic_regression_iris_dataset]{Code/Logistic Regression/logistic_regression_example.py}

    Which produces an output like this:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/logistic_regression_iris.png} \\
            \caption{Logistic Regression on the Iris Dataset.}
            \label{figure:logistic_regression_iris_dataset}
        \end{minipage}
    \end{figure}

    \subsection{Algorithms for convex optimization}
    
    Many different algorithms exist for solving optimization problems. For minimizing convex loss functions, such as the logistic regression loss, it is recommended to use more advanced approaches than regular \textbf{stochastic gradient descent} (\textbf{SGD}). Scikit-learn implements a range of such optimization algorithms, which can be specified via the solver parameter: \lstinline|'newton-cg'|, \lstinline|'lbfgs'|, \lstinline|'liblinear'|, \lstinline|'sag'|, and \lstinline|'saga'|.

    While the logistic regression loss is convex, most optimization algorithms should converge to the global loss minimum with ease. However, there are certain advantages to using one algorithm over another. For example, in earlier versions (for instance, v0.21), scikit-learn used \lstinline|'liblinear'| as the default, which cannot handle the multinomial loss and is limited to the OvR scheme for multiclass classification. In scikit-learn v0.22, the default solver was changed to \lstinline|'lbfgs'|, which stands for the \href{https://en.wikipedia.org/wiki/Limited-memory_BFGS}{limited-memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm}  and is more flexible in this regard.

    \begin{obs}
        Looking at the preceding code that we used to train the \lstinline|LogisticRegression| model, you might now be wondering, "What is this mysterious parameter \lstinline|C|?" We will discuss this parameter in the next subsection, where we will introduce the concepts of overfitting and regularization. However, before we move on to those topics, let's finish our discussion of class membership probabilities.
    \end{obs}

    The probability that training examples belong to a certain class can be computed using the \lstinline|predict_proba| method.

    \begin{exa}
        For example, we can predict the probabilities of the first three examples in the test dataset as follows:

        \begin{lstlisting}[caption={Predicting Probabilities.},label=code:predicting_probabilities]
lr.predict_proba(X_test[:3, :])
        \end{lstlisting}
    \end{exa}

    This code snippet returns the following array:

    \begin{lstlisting}
array([[3.81527885e-09, 1.44792866e-01, 8.55207131e-01],
       [8.34020679e-01, 1.65979321e-01, 3.25737138e-13],
       [8.48831425e-01, 1.51168575e-01, 2.62277619e-14]])
    \end{lstlisting}

    The first row corresponds to the class membership probabilities of the first flower, the second row corresponds to the second flower, and so forth. Notice that the column-wise sum in each row is 1, as expected (you can confirm this by executing \lstinline|lr.predict_proba(X_test_std[:3, :]).sum(axis=1)|).

    The highest value in the first row is approximately \lstinline|0.85|, which means that the first example belongs to class 3 (\lstinline|Iris-virginica|) with a predicted probability of 85 percent.

    \begin{obs}
        As you may have noticed, we can obtain the predicted class labels by identifying the largest column in each row, for example, by using NumPy's \lstinline|argmax| function:

        \begin{lstlisting}
lr.predict_proba(X_test_std[:3, :]).argmax(axis=1)
        \end{lstlisting}
        
        The returned class indices (corresponding to \lstinline|Iris-virginica|, \lstinline|Iris-setosa|, and \lstinline|Iris-setosa|) are:

        \begin{lstlisting}
array([2, 0, 0])
        \end{lstlisting}

        In the preceding code example, we converted conditional probabilities into class labels manually by using NumPy's \lstinline|argmax| function. In practice, the more convenient way of obtaining class labels when using scikit-learn is to call the \lstinline|predict| method directly:  

        \begin{lstlisting}
lr.predict(X_test_std[:3, :])
array([2, 0, 0])
        \end{lstlisting}

        Lastly, if you want to predict the class label of a single flower example, scikit-learn expects a two-dimensional array as input. One way to convert a single row entry into a two-dimensional data array is to use NumPy's \lstinline|reshape| method to add a new dimension, as shown here:

        \begin{lstlisting}
lr.predict(X_test[0, :].reshape(1, -1))
array([2])
        \end{lstlisting}

    \end{obs}

    \section{Tackling Overfitting via Regularization}

    \begin{mydef}[\textbf{Overfitting}]
        \textbf{Overfitting} is a \textit{common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data)}.
    \end{mydef}
    
    If a model suffers from overfitting, we also say that the model has \textbf{high variance}, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data.
    
    \begin{obs}[\textbf{Underfitting}]
        Similarly, a model can suffer from \textbf{underfitting} (\textbf{high bias}), which means \textit{it is not complex enough to capture the pattern in the training data well and therefore also performs poorly on unseen data}.
    \end{obs}

    Although we have only encountered linear models for classification so far, the \textit{problems of overfitting and underfitting can be illustrated by comparing a linear decision boundary to more complex, nonlinear decision boundaries}.

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/underfitting_normal_overfitting_examples.png} \\
            \caption{Examples of Underfitting, Good Compromise and Overfitting.}
            \label{figure:underfitting_and_overfitting_examples}
        \end{minipage}
    \end{figure}

    \subsection{The Bias-Variance Tradeoff}

    \begin{mydef}[\textbf{Tradeoff}]
        \textbf{Tradeoff} is a \textit{balance achieved between two desirable but incompatible features; a compromise}.
    \end{mydef}

    Researchers often use the terms \textbf{bias} and \textbf{variance} or \textbf{bias-variance tradeoff} to d\textit{escribe model performance—that is, you may encounter talks, courses, or articles where people say that a model has high variance or high bias}.
    
    \begin{center}
        In general, \textit{high variance is proportional to overfitting and high bias is proportional to underfitting}.
    \end{center}

    \begin{obs}[\textbf{Variance in Machine Learning}]
        In the context of machine learning models, \textit{variance measures the consistency} (or variability) \textit{of the model prediction for classifying a particular example if we retrain the model multiple times on different subsets of the training dataset}.
        
        In contrast, \textit{bias measures how far off the predictions are from the correct values in general if we rebuild the model multiple times on different training datasets}; \textbf{bias is the measure of the systematic error that is not due to randomness}.
    \end{obs}

    One way of finding a good \textbf{bias-variance tradeoff is to tune the complexity of the model via regularization}.
    
    \begin{mydef}[\textbf{Regularization}]
        \textbf{Regularization} is a \textit{useful method for handling collinearity (high correlation among features), filtering out noise from data, and eventually preventing overfitting}.
    \end{mydef}

    \begin{center}
        The concept behind regularization is to \textit{introduce additional information to penalize extreme parameter (weight) values}.
    \end{center}
    
    \begin{obs}[\textbf{L2 Regularization}]
        The most common form of regularization is \textbf{L2 regularization} (sometimes called L2 shrinkage or weight decay), which can be written as follows:
        \begin{equation*}
            \frac{\lambda}{2n}\|w\|^2=\frac{\lambda}{2n}\sum_{ j=1}^{m}w_j^2
        \end{equation*}\
        Here, $\lambda$ is the \textbf{regularization parameter}. The $2n$ in the denominator is merely a scaling factor, such that cancels then
    \end{obs}




\end{document}