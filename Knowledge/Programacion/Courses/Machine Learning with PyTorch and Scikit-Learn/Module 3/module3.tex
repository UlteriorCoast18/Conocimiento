\documentclass[../machine_learning_scikit.tex]{subfiles}

\begin{document}

    \chapter{A Tour of Machine Learning Classifiers Using Scikit-Learn}

    \section{Introduction}

    The goal of this chapter is to use Scikit-Learn to impelement some classifiers in order to learn how to use them, his advantages, disadvantages and different use scenarios.

    Specifically, we will take a look of 4 popular machine learning models commonly used in academia and in industry. In addition, we will take a look at the Scikit-Learn library, which offers a user-friendly and consistent interface for using those algorithms efficiently and productively.
    
    \subsection{Choosing a Classification Algorithm}

    Each algorithm has its own quirks and relies on certain assumptions. no single classifier works best across all possible scenarios (The Lack of A Priori Distinctions Between Learning Algorithms, Wolpert, David H, Neural Computation 8.7 (1996): 1341-1390).

    \begin{obs}[\textbf{Comparasion}]
        In practice, it is always recomended to compare the behaviour of different algorithms in order to find the best model suitable for a particular problem; these may differ in the number of features or examples, the amount of noise in a dataset, and whether the classes are linearly separable.
    \end{obs}

    \begin{idea}
        The performance of a classifier relies upon the data that is available for learning. The five main steps that are involved in training a supervised machine learning algorithm can be summarized as follows:
        \begin{enumerate}
            \item Selecting features and collecting labeled training examples
            \item Choosing a performance metric
            \item Choosing a learning algorithm and training a model
            \item Evaluating the performance of the model
            \item Changing the settings of the algorithm and tuning the model.
        \end{enumerate}
    \end{idea}

    We will mainly focus on the main concepts of the different algorithms in this chapter and revisit topics such as feature selection and preprocessing, performance metrics, and hyperparameter tuning for more detailed discussions later in the book.

    \subsection{First Steps with scikit-learn Training a Perceptron}

    Before we learn about two related learning algorithms: the perceptron and adaline, both implemented in Python using NumPy and other libraries by ourselves.

    \begin{center}
        Now we will take a look at the \textbf{scikit-learn API}.
    \end{center}

    \begin{obs}
        One of the advantages of using scikit-learn is that combines a user-friendly and consistent interface with a highly optimized implementation of several classification algorithms. Also, this library offers a not only a large variety of learning algorithms, but also many convenient functions to preprocess data and to fine-tune and evaluate our models.
    \end{obs}

    \subsection{Training a Model Using Scikit-Learn}

    To get started with the scikit-learn library, we will train a perceptron model similar to the one that we implemented in Chapter 2. For simplicity, we will use the already familiar Iris dataset throughout the following sections.

    \begin{obs}[\textbf{Iris Dataset and Its Uses}]
        Conveniently, the Iris dataset is already available via scikit-learn, since it is a simple yet popular dataset that is frequently used for testing and experimenting with algorithms. Similar to the previous chapter, we will only use two features from the Iris dataset for visualization purposes.
    \end{obs}

    We will assign the petal length and petal width of the 150 flower examples to the feature matrix, \lstinline|X|, and the corresponding class labels of the flower species to the vector array, \lstinline|y|:

    \begin{lstlisting}[caption={Class Labels of Matrix \lstinline|X|},label=matrix_labels]
Class labels: [0 1 2]
Class labels: [0 1 2]
    \end{lstlisting}

    The \lstinline|np.unique(y)| function returned the three unique class labels stored in \lstinline|iris.target|, and as we can see, the Iris flower class names, \lstinline|Iris-setosa|, \lstinline|Iris-versicolor|, and \lstinline|Iris-virginica|, are already stored as integers (here: \lstinline|0|, \lstinline|1|, \lstinline|2|).
    
    \begin{obs}[\textbf{Scikit-Learn with Class Labels and String Formats}]
        Although many scikit-learn functions and class methods also work with \textit{class labels in string format, using integer labels is a recommended approach to avoid technical glitches and improve computational performance due to a smaller memory footprint}; furthermore, \textit{encoding class labels as integers is a common convention among most machine learning libraries}.
    \end{obs}

    To evaluate how well a trained model performs on unseen data, we will \textit{further split the dataset into separate training and test datasets}.
    
    \begin{idea}[\textbf{More Info About Best Practices around Model Evaluation}]
        In Chapter 6, \textit{Learning Best Practices for Model Evaluation and Hyperparameter Tuning}, we \textbf{will discuss the best practices around model evaluation in more detail}.
    \end{idea}

    Using the \lstinline|train_test_split| function from scikit-learn's \lstinline|model_selection| module, we randomly split the \lstinline|X| and \lstinline|y| arrays into 30 percent test data (45 examples) and 70 percent training data (105 examples):

    \begin{lstlisting}[caption={Train Test Split Function},label=code:train_test_split]
train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)
    \end{lstlisting}

    \begin{obs}
        Note that the \lstinline|train_test_split| function already shuffles the training datasets internally before splitting; otherwise, all examples from class \lstinline|0| and class \lstinline|1| would have ended up in the training datasets, and the test dataset would consist of 45 examples from class \lstinline|2|. Via the \lstinline|random_state| parameter, we provided a fixed random seed (\lstinline|random_state=1|) for the internal pseudo-random number generator that is used for shuffling the datasets prior to splitting. Using such a fixed \lstinline|random_state| ensures that our results are reproducible.
    \end{obs}

    Lastly, we took advantage of the built-in support for stratification via \lstinline|stratify=y|. In this context, \textit{stratification means that the} \lstinline|train_test_split| \textit{method returns training and test subsets that have the same proportions of class labels as the input dataset}. We can use NumPy's \lstinline|bincount| function, which counts the number of occurrences of each value in an array, to verify that this is indeed the case:

    \begin{lstlisting}[caption={Use of \lstinline|bincount| Method in Python.},label=code:use_bicount]
>>> print('Labels counts in y:', np.bincount(y))
Labels counts in y: [50 50 50]
>>> print('Labels counts in y_train:', np.bincount(y_train))
Labels counts in y_train: [35 35 35]
>>> print('Labels counts in y_test:', np.bincount(y_test))
Labels counts in y_test: [15 15 15]
    \end{lstlisting}

    \begin{idea}[\textbf{Feature Scaling}]
        Many machine learning and optimization algorithms also require \textbf{feature scaling for optimal performance}, as we saw in the gradient descent example in Chapter 2. Here, \textbf{we will standardize the features using the StandardScaler class from scikit-learn's preprocessing module}:

        \begin{lstlisting}[caption={\lstinline|StandardScaler| Method in Python.},label=code:standard_scaler]
sc = StandardScaler()
        \end{lstlisting}

        Using the preceding code, we loaded the \lstinline|StandardScaler| class from the \lstinline|preprocessing| module and initialized a new \lstinline|StandardScaler| object that we assigned to the \lstinline|sc| variable.
    \end{idea}

    Using the \lstinline|fit| method, \lstinline|StandardScaler| estimated the parameters, $\mu$ (sample mean) and $\sigma$ (standard deviation), for each feature dimension from the training data. By calling the \lstinline|transform| method, we then standardized the training data using those estimated parameters, $\mu$ and $\sigma$. Note that we used the same scaling parameters to standardize the test dataset so that both the values in the training and test dataset are comparable with one another.

    \begin{exa}
        Having standardized the training data, we can now train a perceptron model. Most algorithms in scikit-learn already support multiclass classification by default via the one-versus-rest (OvR) method, which allows us to feed the three flower classes to the perceptron all at once.
    \end{exa}

    Finally, we have the code for the model:

    \lstinputlisting[caption={Perceptron Trained Model using Sklearn}, label={code:perceptron_sklearn}, language=Python]{Code/sklearn_first/sklearn_first.py}

    With the slight modification that we made to the \lstinline|plot_decision_regions| function, we can now specify the indices of the examples that we want to mark on the resulting plots.

    As we can see in the resulting plot, the three flower classes can't be perfectly separated by a linear decision boundary:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/Iris_perceptron_scikit_learn.png} \\
            \caption{Perceptron Trained with Iris Dataset using Scikit-Learn.}
            \label{figure:perceptron_iris_scikit_learn}
        \end{minipage}
    \end{figure}

    However, remember from our discussion in Chapter 2 that \textbf{the perceptron algorithm never converges on datasets that aren't perfectly linearly separable}, which is \textit{why the use of the perceptron algorithm is typically not recommended in practice}. In the following sections, we will look at more powerful linear classifiers that converge to a loss minimum even if the classes are not perfectly linearly separable.

    \begin{obs}[\textbf{Note}]
        The Perceptron, as well as other scikit-learn functions and classes, \textit{often has additional parameters that we omit for clarity. You can read more about those parameters using the help function in Python (for instance, \lstinline|help(Perceptron)|) or by going through the excellent scikit-learn online documentation} \href{http://scikit-learn.org/stable/}{here}.
    \end{obs}

    \newpage

    \section{Modeling Class Probabilities Via Logistic Regression}

    The biggest disadvantage of the Perceptron is that it never converges if the classes aren't linearly separable. The reason for this is that the weights are continuously updated because there is always at least one misclassified training example present in each poch.

    To make better use of our time, we will now look at another simple, yet more powerful, algorithm for linear and binary classification problems: \textbf{logistic regression}. Note that, despite its name, \textit{logistic regression is a model for classification, not regression}.

    \subsection{Logistic Regression and Conditional Probabilities}

    \begin{mydef}[\textbf{Logistic Regression}]
        \textbf{Logistic regression} is a \textit{classification model that is easy to implement and performs very well on linearly separable classes}. It is \textit{one of the most widely used algorithms for classification in industry}.
    \end{mydef}

    Similar to the perceptron and Adaline, \textit{the logistic regression model in this section is also a linear model for binary classification}.

    \begin{idea}[\textbf{Note}]
        Logistic regression can be generalized to multinomial logistic regression.
    \end{idea}

    To explain the main mechanics behind logistic regression as a probabilistic model for binary classification, let's first introduce the odds: the odds in favor of a particular event. The odds can be written as $\frac{p}{1-p}$, where $p$ stands for the probability of the positive event. The term "positive event" does not necessarily mean "good," but refers to the event that we want to predict.
    
    \begin{exa}[\textbf{Conditional Probability}]
        Consider the \textbf{probability that a patient has a certain disease given certain symptoms}; we can think of the positive event as class label $y=1$ and the symptoms as features $\boldsymbol{x}$. Hence, for brevity, we can define the probability $p$ as $p:=p(y=1|\boldsymbol{x})$, the conditional probability that a particular example belongs to class $1$ given its features, $\boldsymbol{x}$.
    \end{exa}

    We can then further define the logit function, which is simply the logarithm of the odds (log-odds):

    \begin{equation}
        \label{logit_function}
        \logit{p}=\log\left(\frac{p}{1-p}\right)
    \end{equation}

    Note that $\log$ \textit{refers to the natural logarithm, as it is the common convention in computer science}. The $\textup{logit}$ function \textit{takes input values in the range 0 to 1 and transforms them into values over the entire real-number range}.

    Under the logistic model, we \textbf{assume that there is a linear relationship between the weighted inputs and the log-odds}:
    \begin{equation}
        \label{logit_function_assumption}
        \logit{p}=w_1x_1+\cdots+w_nx_n+b=\sum_{i=1}^n w_ix_i+b=\boldsymbol{w}^T\boldsymbol{x}+b
    \end{equation}

    \begin{obs}
        While the preceding describes an assumption we make about the linear relationship between the log-odds and the net inputs, what \textbf{we are actually interested in is the probability} $p$, the \textit{class‐membership probability of an example given its features}. While the logit function maps the probability to a real‐number range, \textit{we can consider the inverse of this function to map the real‐number range back to a} $[0,1)$ range for the probability $p$.
    \end{obs}

    This inverse of the logit function is typically called the \textbf{logistic sigmoid function}, which is \textit{sometimes simply abbreviated to sigmoid function due to its characteristic $S$-shape}:

    \begin{equation}
        \label{sigmoid_function}
        \sigma(z)=\frac{1}{1+e^{-z}},\quad\forall z\in\bbm{R}
    \end{equation}
    Here, $z$ is the \textit{net input, the linear combination of weights and the inputs} (that is, the features associated with the training examples):
    \begin{equation}
        \label{z_sigmoid_function}
        z=\boldsymbol{w}^T\boldsymbol{x}+b
    \end{equation}

    \begin{exa}[\textbf{Graph of Sigmoid Function}]
        Graph of Sigmoid Function:

        \begin{lstlisting}[caption={Sigmoid Function Graph.},label=figure:sigmoid_function_graph]
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))

z = np.arange(-7, 7, 0.1)
sigma_z = sigmoid(z)
plt.plot(z, sigma_z)
plt.axvline(0.0, color='k')
plt.ylim(-0.1, 1.1)
plt.xlabel('z')
plt.ylabel('$\\sigma (z)$')
# y-axis ticks and gridline
plt.yticks([0.0, 0.5, 1.0])
ax = plt.gca()
ax.yaxis.grid(True)
plt.tight_layout()
plt.show()
        \end{lstlisting}
    \end{exa}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/sigmoid_function_graph.png} \\
            \caption{Sigmoid Function Graph.}
            \label{figure:sigmoid_function_graph}
        \end{minipage}
    \end{figure}

    As a result of executing the previous code, you should now see the $S$-shaped (sigmoidal) curve.  

    \begin{obs}[\textbf{Comparasion with Adaline}]
        To build some understanding of the logistic regression model, we can relate it to Adaline. In Adaline, we used the identity function, $\sigma(z)=z$ as the activation function. In logistic regression, \textit{this activation function simply becomes the sigmoid function defined earlier}.
    \end{obs}

    The only difference between Adaline and logistic regression is the activation function.  

    \begin{mydef}[\textbf{Purpose of an Activation Function}]
        In simple terms, the purpose of an \textbf{activation function} is \textit{to decide whether a neuron in a neural network should be "activated" or not, and to what degree. It's what allows neural networks to learn and model complex, non-linear relationships}.
    \end{mydef}

    The output of the sigmoid function is then interpreted as the probability of a particular example belonging to class 1, $\sigma(z) = p(y=1 | \boldsymbol{x}; \boldsymbol{w},b)$, given its features $\boldsymbol{x}$, and parameterized by the weights and bias, $\boldsymbol{w}$ and $b$. 

    For example, if we compute $\sigma(z) = 0.8$ for a particular flower, it means that the chance that this example is an \lstinline|Iris-versicolor| flower is 80 percent. Therefore, the probability that this flower is an \lstinline|Iris-setosa| can be calculated as $p(y=0 |\boldsymbol{x}; \boldsymbol{w},b) = 1 - p(y=1 | \boldsymbol{x}; \boldsymbol{w},b) = 0.2$, or 20 percent.

    The predicted probability can then be converted into a binary outcome via a threshold function:  

    \begin{equation*}
        \hat{y}=\left\{
            \begin{array}{lr}
                1 & \textup{ if }\sigma(z)\geq0.5\\
                0 & \textup{ otherwise} \\
            \end{array}
        \right.
    \end{equation*}

    If we look at the preceding plot of the sigmoid function, this is equivalent to the following:  

    \begin{equation*}
        \hat{y}=\left\{
            \begin{array}{lr}
                1 & \textup{ if }z\geq0\\
                0 & \textup{ otherwise} \\
            \end{array}
        \right.
    \end{equation*}

    \begin{obs}[\textbf{Interest in Predicted Class Labels}]
        In many applications, \textbf{we are not only interested in the predicted class labels but also in the class‐membership probabilities produced by the sigmoid function before applying the threshold}.
        
        Logistic regression is \textit{used in weather forecasting, for example, not only to predict whether it will rain on a particular day but also to report the chance of rain}. Similarly, logistic regression \textit{can be used to predict the chance that a patient has a particular disease given certain symptoms, which is why logistic regression enjoys great popularity in the field of medicine}.
    \end{obs}

    \section{Learning the Model Weights via the Logistic Loss Function}

    \textit{You have learned how we can use the logistic regression model to predict probabilities and class labels}; now, let's briefly talk about how \textit{we fit the parameters of the model, for instance, the weights and bias unit, $\boldsymbol{w}$ and $b$}. Previously, we defined the mean squared error loss function as follows:

    \begin{equation*}
        L(\boldsymbol{w},b|\boldsymbol{x})=\frac{1}{n}\sum_{ i=1}^n\left(y^{(i)}-\sigma(z^{(i)})\right)^2
    \end{equation*}

    \textit{We minimized this function in order to learn the parameters for our Adaline classification model}. To explain how we can derive the loss function for logistic regression, \textit{let's first define the likelihood}, $\mathcal{L}$, that we \textbf{want to maximize when we build a logistic regression model}, assuming that the individual examples in our dataset are independent of one another. The formula is as follows:

    \begin{equation}
    L(w, b \mid x) = p(y|x;w,b) = \prod_{i=1}^{n} p(y^{(i)} \mid x^{(i)}; w, b) = \prod_{i=1}^{n} \left(\sigma(z^{(i)})\right)^{y^{(i)}} \left(1 - \sigma(z^{(i)})\right)^{(1-y^{(i)})}
    \end{equation}

    This is the \textbf{likelihood function} $L(w, b \mid x)$ in \textit{terms of a product over $n$ terms}. It is expressed as the product of probabilities $p(y^{(i)} \mid x^{(i)}; w, b)$ for $i$ from 1 to $n$, which is further expanded to the product of $(\sigma(z^{(i)}))^{y^{(i)}}$ times $(1 - \sigma(z^{(i)}))^{(1-y^{(i)})}$, where $\sigma$ is the sigmoid function.

    \begin{obs}
        In practice, it is easier to maximize the (natural) log of this equation, which is called the log-likelihood function:
    
        \begin{equation}
        l(w, b \mid x) = \log\left(\mathcal{L}\left(w, b \mid x\right) \right) = \sum_{i=1}^{n} \left[ y^{(i)} \log(\sigma(z^{(i)})) + (1 - y^{(i)}) \log(1 - \sigma(z^{(i)})) \right]
        \end{equation}

        Applying the log function reduces the potential for numerical underflow if the likelihoods are very small. In addition, the product of factors becomes a summation of factors, which makes it easier to obtain the derivative of this function via the addition trick.
    \end{obs}

    \subsection{Deriving the likelihood function}

    We can obtain the expression for the likelihood of the model given the data, $\mathcal{L}(w, b \mid x)$, as follows. Given that we have a binary classification problem with class labels 0 and 1, we can think of the label 1 as a Bernoulli variable—it can take on two values, 0 and 1, with the probability $p$ of being 1: $Y \sim \mathrm{Bernoulli}(p)$. 

    For a single data point, we can write this probability as:
    \begin{align}
    P(Y=1 \mid X=x^{(i)}) &= \sigma(z^{(i)}) \\
    P(Y=0 \mid X=x^{(i)}) &= 1 - \sigma(z^{(i)})
    \end{align}

    Putting these two expressions together, and using the shorthand $P(Y=y^{(i)} \mid X=x^{(i)}) = p(y^{(i)} \mid x^{(i)})$, we get the probability mass function of the Bernoulli variable:
    \begin{equation*}
        p\left(y^{(i)}\Big|x^{(i)}\right)=\left(\sigma(z^{(i)})\right)^{(y^{(i)})}\left(1-\sigma(z^{(i)})\right)^{1-y^{(i)}}
    \end{equation*}
    Substituting the probability mass function of the Bernoulli variable, we arrive at the expression for the likelihood, which we attempt to maximize by changing the model parameters:
    \begin{equation*}
        \mathcal{L}(w,b\Big|x)=\prod_{ i=1}^n \left(\sigma(z^{(i)})\right)^{(y^{(i)})}\left(1-\sigma(z^{(i)})\right)^{1-y^{(i)}}
    \end{equation*}

    \begin{obs}
        We could use an optimization algorithm such as \textbf{gradient ascent} to maximize this log-likelihood function. (\textit{Gradient ascent works the same way as gradient descent explained earlier, except that gradient ascent maximizes a function instead of minimizing it.}).
    \end{obs}

    Alternatively, let's rewrite the log-likelihood as a loss function, $L$, that can be minimized using gradient descent:
    \begin{equation*}
        L(w,b)=\sum_{i=1}^{n} \left[-y^{(i)} \log(\sigma(z^{(i)})) - (1 - y^{(i)}) \log(1 - \sigma(z^{(i)})) \right]
    \end{equation*}

    \begin{exa}
        For one single training example, looking at the equation, we can see that the first term becomes zero if $y = 0$, and the second term becomes zero if $y = 1$:
        \begin{equation*}
            L(\sigma(z),y,w,b)=\left\{
                \begin{array}{lr}
                    -\log\left(\sigma(z)\right) & \textup{ if }y=0\\
                    -\log\left(1-\sigma(z)\right) & \textup{ if }y=1\\
                \end{array}
            \right.
        \end{equation*}

        Let's write a short code snippet to create a plot that illustrates the loss of classifying a single training example for different values of $\sigma(z)$. The resulting plot shows the sigmoid activation on the x axis in the range 0 to 1 (the inputs to the sigmoid function were z values in the range -10 to 10) and the associated logistic loss on the y axis:
    \end{exa}

    \begin{figure}[h]
            \begin{minipage}{\textwidth}
                \centering
                \includegraphics[scale=0.75]{images/sigomoid_regression.png} \\
                \caption{Loss Function.}
                \label{figure:loss_function}
            \end{minipage}
        \end{figure}

    We can see that the loss approaches 0 (continuous line) if we correctly predict that an example belongs to class 1. Similarly, the loss also approaches 0 if we correctly predict y = 0 (dashed line). However, if the prediction is wrong, the loss goes toward infinity. The main point is that we penalize incorrect predictions with an increasingly larger loss.

    \section{Converting an Adaline Implementation Into an Algorithm for Logistic Regression}

    If we implement logistic regression from scratch, we can substitute the loss function, $L$, in the earlier Adaline implementation with the new loss function:

    \begin{equation*}
        L(w,b)=\frac{1}{n}\sum_{i=1}^{n} \left[-y^{(i)} \log(\sigma(z^{(i)})) - (1 - y^{(i)}) \log(1 - \sigma(z^{(i)})) \right]
    \end{equation*}

    \begin{obs}
        We use this to compute the loss of classifying all training examples per epoch. We also swap the linear activation function for the sigmoid. With those changes, the Adaline code becomes a working logistic regression implementation.
    \end{obs}

    The following example uses full-batch gradient descent (the same changes can be applied to the stochastic variant):  

    \lstinputlisting[caption={Implementation of Logistic Regression in Python.},label=code:logistic_regression_implementation, language=Python]{Code/Logistic Regression/LogisticRegressionGD.py}

    \begin{center}
        \textit{When fitting a logistic regression model, remember that it only works for binary classification tasks.}
    \end{center}

    The resulting decision region plot looks as follows:

    \section{Gradient Descent}

    One observation we can make on the latter code is that the partial derivative of the loss function $L$ is really simple to compute, this is due to the fact:

    \begin{equation*}
        \frac{\partial L}{\partial w_j}=\frac{\partial L}{\partial \sigma}\cdot\frac{\partial \sigma}{\partial z}\cdot\frac{\partial z}{\partial w_j}
    \end{equation*}
    where:
    \begin{equation*}
        \begin{split}
            \frac{\partial L}{\partial \sigma}&=\frac{\sigma-y}{\sigma\cdot(1-\sigma)}\\
            \frac{\partial \sigma}{\partial z}&=\sigma(1-\sigma)\\
            \frac{\partial z}{\partial w_j}&=x_j\\
        \end{split}
    \end{equation*}
    So,
    \begin{equation*}
        \frac{\partial L}{\partial w_j}=-(y-\sigma)x_j
    \end{equation*}

    Recall that gradient descent takes steps in the opposite direction of the gradient. Hence, we flip $\nabla L(w)$ and update the j-th weight as follows, including the learning rate $\eta$:
    \begin{equation*}
        w_j=w_j-\eta\cdot\frac{\partial L}{\partial w_j}(w)
    \end{equation*}
    While the partial derivative of the loss function with respect to the bias unit is not shown, bias derivation follows the same concept.

    \begin{obs}
        We should compute the mean in all this process, but due we are working in this example with a single record for simplicity.
    \end{obs}

    \begin{idea}[\textbf{Relation between Adaline and Logistical Regression}]
        Both the weight and bias updates are the same as for Adaline, but the change is in the activation function.
    \end{idea}

    \section{Training a Logistic Regression Model with Scikit-Learn}

    We just went through useful coding and math exercises in the previous subsection, which helped to illustrate the conceptual differences between Adaline and logistic regression. Now, let's learn how to use scikit-learn's more optimized implementation of logistic regression, which also supports multiclass settings off the shelf.
    
    \begin{obs}
        Note that in recent versions of scikit-learn, the technique used for multiclass classification—multinomial or OvR—is chosen automatically.
    \end{obs}
    
    In the following code example, we will use the \lstinline|sklearn.linear_model.LogisticRegression| class as well as the familiar fit method to train the model on all three classes in the standardized flower training dataset. Also, we set \lstinline|multi_class='ovr'| for illustration purposes. As an exercise, you may want to compare the results with \lstinline|multi_class='multinomial'|. Note that the \lstinline|multinomial| setting is now the default choice in scikit-learn's \lstinline|LogisticRegression| class and recommended in practice for mutually exclusive classes, such as those found in the Iris dataset.
    
    \begin{obs}
        Here, "mutually exclusive" means that each training example can belong to only a single class (in contrast to multilabel classification, where a training example can be a member of multiple classes).
    \end{obs}

    Now, let's look at the code example:

    \lstinputlisting[caption={Logistic Regression Iris Dataset Python.},label=code:logistic_regression_iris_dataset]{Code/Logistic Regression/logistic_regression_example.py}

    Which produces an output like this:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/logistic_regression_iris.png} \\
            \caption{Logistic Regression on the Iris Dataset.}
            \label{figure:logistic_regression_iris_dataset}
        \end{minipage}
    \end{figure}

    \subsection{Algorithms for convex optimization}
    
    Many different algorithms exist for solving optimization problems. For minimizing convex loss functions, such as the logistic regression loss, it is recommended to use more advanced approaches than regular \textbf{stochastic gradient descent} (\textbf{SGD}). Scikit-learn implements a range of such optimization algorithms, which can be specified via the solver parameter: \lstinline|'newton-cg'|, \lstinline|'lbfgs'|, \lstinline|'liblinear'|, \lstinline|'sag'|, and \lstinline|'saga'|.

    While the logistic regression loss is convex, most optimization algorithms should converge to the global loss minimum with ease. However, there are certain advantages to using one algorithm over another. For example, in earlier versions (for instance, v0.21), scikit-learn used \lstinline|'liblinear'| as the default, which cannot handle the multinomial loss and is limited to the OvR scheme for multiclass classification. In scikit-learn v0.22, the default solver was changed to \lstinline|'lbfgs'|, which stands for the \href{https://en.wikipedia.org/wiki/Limited-memory_BFGS}{limited-memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm}  and is more flexible in this regard.

    \begin{obs}
        Looking at the preceding code that we used to train the \lstinline|LogisticRegression| model, you might now be wondering, "What is this mysterious parameter \lstinline|C|?" We will discuss this parameter in the next subsection, where we will introduce the concepts of overfitting and regularization. However, before we move on to those topics, let's finish our discussion of class membership probabilities.
    \end{obs}

    The probability that training examples belong to a certain class can be computed using the \lstinline|predict_proba| method.

    \begin{exa}
        For example, we can predict the probabilities of the first three examples in the test dataset as follows:

        \begin{lstlisting}[caption={Predicting Probabilities.},label=code:predicting_probabilities]
lr.predict_proba(X_test[:3, :])
        \end{lstlisting}
    \end{exa}

    This code snippet returns the following array:

    \begin{lstlisting}
array([[3.81527885e-09, 1.44792866e-01, 8.55207131e-01],
       [8.34020679e-01, 1.65979321e-01, 3.25737138e-13],
       [8.48831425e-01, 1.51168575e-01, 2.62277619e-14]])
    \end{lstlisting}

    The first row corresponds to the class membership probabilities of the first flower, the second row corresponds to the second flower, and so forth. Notice that the column-wise sum in each row is 1, as expected (you can confirm this by executing \lstinline|lr.predict_proba(X_test_std[:3, :]).sum(axis=1)|).

    The highest value in the first row is approximately \lstinline|0.85|, which means that the first example belongs to class 3 (\lstinline|Iris-virginica|) with a predicted probability of 85 percent.

    \begin{obs}
        As you may have noticed, we can obtain the predicted class labels by identifying the largest column in each row, for example, by using NumPy's \lstinline|argmax| function:

        \begin{lstlisting}
lr.predict_proba(X_test_std[:3, :]).argmax(axis=1)
        \end{lstlisting}
        
        The returned class indices (corresponding to \lstinline|Iris-virginica|, \lstinline|Iris-setosa|, and \lstinline|Iris-setosa|) are:

        \begin{lstlisting}
array([2, 0, 0])
        \end{lstlisting}

        In the preceding code example, we converted conditional probabilities into class labels manually by using NumPy's \lstinline|argmax| function. In practice, the more convenient way of obtaining class labels when using scikit-learn is to call the \lstinline|predict| method directly:  

        \begin{lstlisting}
lr.predict(X_test_std[:3, :])
array([2, 0, 0])
        \end{lstlisting}

        Lastly, if you want to predict the class label of a single flower example, scikit-learn expects a two-dimensional array as input. One way to convert a single row entry into a two-dimensional data array is to use NumPy's \lstinline|reshape| method to add a new dimension, as shown here:

        \begin{lstlisting}
lr.predict(X_test[0, :].reshape(1, -1))
array([2])
        \end{lstlisting}

    \end{obs}

    \section{Tackling Overfitting via Regularization}

    \begin{mydef}[\textbf{Overfitting}]
        \textbf{Overfitting} is a \textit{common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data)}.
    \end{mydef}
    
    If a model suffers from overfitting, we also say that the model has \textbf{high variance}, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data.
    
    \begin{obs}[\textbf{Underfitting}]
        Similarly, a model can suffer from \textbf{underfitting} (\textbf{high bias}), which means \textit{it is not complex enough to capture the pattern in the training data well and therefore also performs poorly on unseen data}.
    \end{obs}

    Although we have only encountered linear models for classification so far, the \textit{problems of overfitting and underfitting can be illustrated by comparing a linear decision boundary to more complex, nonlinear decision boundaries}.

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/underfitting_normal_overfitting_examples.png} \\
            \caption{Examples of Underfitting, Good Compromise and Overfitting.}
            \label{figure:underfitting_and_overfitting_examples}
        \end{minipage}
    \end{figure}

    \subsection{The Bias-Variance Tradeoff}

    \begin{mydef}[\textbf{Tradeoff}]
        \textbf{Tradeoff} is a \textit{balance achieved between two desirable but incompatible features; a compromise}.
    \end{mydef}

    Researchers often use the terms \textbf{bias} and \textbf{variance} or \textbf{bias-variance tradeoff} to d\textit{escribe model performance—that is, you may encounter talks, courses, or articles where people say that a model has high variance or high bias}.
    
    \begin{center}
        In general, \textit{high variance is proportional to overfitting and high bias is proportional to underfitting}.
    \end{center}

    \begin{obs}[\textbf{Variance in Machine Learning}]
        In the context of machine learning models, \textit{variance measures the consistency} (or variability) \textit{of the model prediction for classifying a particular example if we retrain the model multiple times on different subsets of the training dataset}.
        
        In contrast, \textit{bias measures how far off the predictions are from the correct values in general if we rebuild the model multiple times on different training datasets}; \textbf{bias is the measure of the systematic error that is not due to randomness}.
    \end{obs}

    One way of finding a good \textbf{bias-variance tradeoff is to tune the complexity of the model via regularization}.
    
    \begin{mydef}[\textbf{Regularization}]
        \textbf{Regularization} is a \textit{useful method for handling collinearity (high correlation among features), filtering out noise from data, and eventually preventing overfitting}.
    \end{mydef}

    \begin{center}
        The concept behind regularization is to \textit{introduce additional information to penalize extreme parameter (weight) values}.
    \end{center}
    
    \begin{obs}[\textbf{L2 Regularization}]
        The most common form of regularization is \textbf{L2 regularization} (sometimes called L2 shrinkage or weight decay), which can be written as follows:
        \begin{equation*}
            \frac{\lambda}{2n}\|w\|^2=\frac{\lambda}{2n}\sum_{ j=1}^{m}w_j^2
        \end{equation*}\
        Here, $\lambda$ is the \textbf{regularization parameter}. The $2$ in the denominator is merely a scaling factor, such that cancels when computing the loss gradient. The sample size $n$ added to scale the regularization term similarly to the loss.
    \end{obs}

    \begin{obs}[\textbf{Importance of Feature Scaling}]
        Regularization is another reason why feature scaling such as standaraization is important. For regularization to work properly, all features must be on comparable scales.
    \end{obs}

    The loss function for logistic regression can be regularized by adding a regularization term, which shrinks the weighs during model training:

    \begin{equation*}
        \frac{\partial L}{\partial w_j}=\frac{1}{n}\sum_{ i=1}^{n}\left(\sigma\left(w^Tx^{(i)}-y^{(i)}\right)x_j^{(i)}\right)
    \end{equation*}
    by adding the regularization term to the loss changes the partial derivative to the following form:
    \begin{equation*}
        \frac{\partial L}{\partial w_j}=\frac{1}{n}\sum_{ i=1}^{n}\left(\sigma\left(w^Tx^{(i)}-y^{(i)}\right)x_j^{(i)}\right)+\frac{\lambda}{n}w_j
    \end{equation*}

    \begin{obs}[\textbf{Information About the Regularization Parameter $\lambda$}]
        Vía the regularization parameter $\lambda$ we can control how closely we fit the training data while keeping the weights small. By increasing $\lambda$, we increase the regularization strength. Note that the bias unit, essentially an intercept term or negative threshold, is usually not regularized.
    \end{obs}

    \subsection{Implementation of Regularization}

    In the following example, the paramenter \lstinline|c| implemented for the \lstinline|LogisticRegression| class in scikit-learn comes from a convention in support vector machines. The term \lstinline|c| is inversely proportional to the regularization parameter $\lambda$. Consecuently, decreasing \lstinline|c| increases the value of $\lambda$ and therefore, increases the regularization strength.
    
    \begin{obs}
        We can visualize this behaviour by plotting the $L2$ regularization path for the two weight coefficients.
    \end{obs}

    \lstinputlisting[caption={Logistic Regression Regularized with different values of \lstinline|c| on the Iris Dataset Python.},label=code:logistic_regression_regularization_iris_dataset]{Code/Logistic Regression Regularized/logistic_regression_regularized.py}

    Executing this code fits 10 logistic regression models with different values for the inverse regularization parameter \lstinline|c|. For illustration purposes, we only collected the weight coefficients of class 1 (the second class on the iris dataset, Iris-versicolor) versus all classifiers.

    \begin{obs}
        Here, we are using OvR technique for multiclass classification.
    \end{obs}

    As shown in the resulting plot, the wright coefficients shrink when \lstinline|c| decreases-that is, when the regularization strength increases:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/coefficient_c_regularization_plot.png} \\
            \caption{Weights for Different Values of \lstinline|c|.}
            \label{figure:weights_different_c_values_regularization}
        \end{minipage}
    \end{figure}

    We have to keep in mind that:

    \begin{center}
        Increasing the regularization strength \textbf{can reduce overfitting}, but if \textit{the strength is too high and the weight coefficients approach zero}, the \textbf{model can perform very poorly due to underfitting}.
    \end{center}

    \section{Maximum Margin Classification with Support Vector Machines}

    Another powerful and widely used learning algorithm is the \textbf{support vector machine (SVM)}, which can be considered an extension of the perceptron.

    \begin{obs}[\textbf{Key Idea of Support Vector Machines}]
        Using the perceptron algorithm, we minimized misclassification errors. In SMVs, our optimization objective is to \textbf{maximize} the margin.
    \end{obs}

    \begin{mydef}[\textbf{Margin}]
        The \textbf{margin} is defined as \textit{the distance between the separating hyperplane (called desicion boundary) and the training examples that are closest to this hyperplane}, which are the so called \textbf{support vectors}.
    \end{mydef}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/_6cbdc5fdb236429185f6b1c14c53cbc2_9781801819312_03_42.png} \\
            \caption{SVM and Perceptron Various Boundaries.}
            \label{figure:svm_perceptron_comparasion}
        \end{minipage}
    \end{figure}

    \begin{idea}[\textbf{Reduce Overfitting}]
        The key idea behind having decision bouyndaries with large margins is that they tend to have a lower generalization error, whereas models with small margines are more prone to overfitting.
    \end{idea}

    While the main intuition behind SVMs is relatively simple, the mathematics behind them is advanced and requieres sound knowledge of constrained optimization.

    \begin{obs}
        The following resources are recomended if interested in learning more:
        \begin{itemize}
            \item Chris J.C. Burges' explanation in \textit{A Tutorial on Support Vector Machines for Pattern Recognition} (Data Mining and Knowledge Discovery, 2(2): 121-167, 1998)
            \item Vladimir Vapnik's book \textit{The Nature of Statistical Learning Theory}, Springer Science+Business Media, Vladimir Vapnik, 2000
            \item \href{https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf}{Andrew Ng's lecture notes}.
        \end{itemize}
    \end{obs}

    \subsection{Dealing with a Nonlinearly Separable Case Using Slack Variables}

    Let's mention the slack variable introduced by Vladimir Vapnik in 1995 and led to the so-called \textbf{soft-margin classification}.
    
    \begin{idea}
        The motivation for introducing the slack variable was that the \textit{linear constraints in the SVM optimization are objective need to be relaxed for nonlinearly separable data to allow the converegence of the optimization in the presence of misclassifications}, under appropiate loss penalization.
    \end{idea}

    The use of slack variable introduces a parameter commonly referred to as $C$ in SVM. We can consider $C$ as a hyperparameter controlling the penalty for misclassification.
    
    Large values of $C$ correspond to \textit{large error penalties, whereas we are less strict about misclassification errors if we choose smaller values for $C$}. We can then use the $C$ parameter to \textit{control the width of the margin and therefore tune the bias-variance trade-off}.

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/_fb26d00b0b4e41adb2b4474fea7b06a6_9781801819312_03_43.png} \\
            \caption{Parameter $C$ in the Bias-Variance Trade-off.}
            \label{figure:c_parameter}
        \end{minipage}
    \end{figure}

    \begin{idea}
        This concept is related to regularization, which we discussed in the previous section in the context of regularized regression, where \textit{decreasing the value of $C$ increases the bias (underfitting) and lowers the variance (overfitting) of the model}.
    \end{idea}

    \subsection{Training a Model with a Support Vector Machine}

    Now that we have learned the basic concepts behind a linear SVM, let's train an SVM model to classify the different flowers in our Iris dataset:

    \lstinputlisting[caption={Implementation of SVM on Python and Graph of Regions.},label=code:logistic_regression_regularization_iris_dataset]{Code/Support Vector Machines/svm.py}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/smv_implementation.png} \\
            \caption{Implementation of SVM and Plot of Regions.}
            \label{figure:smv_implementation}
        \end{minipage}
    \end{figure}

    \subsection{Alternative Implementations in Scikit-learn}

    The scikit-learn library's \lstinline|LogisticRegression| class, which we used in the previous sections, can make use of the \lstinline|LIBLINEAR| library by setting \lstinline|solver='liblinear'|. \lstinline|LIBLINEAR| is a highly optimized C/C++ library developed at the National Taiwan University (\href{http://www.csie.ntu.edu.tw/~cjlin/liblinear/}{Liblinear}).

    Similarly, the \lstinline|SVC| class that we used to train an \lstinline|SVM| makes use of \lstinline|LIBSVM|, which is an equivalent C/C++ library specialized for \lstinline|SVMs| (\href{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}{Libsvm}).

    The advantage of using \lstinline|LIBLINEAR| and \lstinline|LIBSVM| over, for example, native Python implementations is that they allow the \textbf{extremely quick training of large amounts of linear classifiers}. However, \textit{sometimes our datasets are too large to fit into computer memory}. Thus, scikit-learn also offers alternative implementations via the \lstinline|SGDClassifier| class, which also supports online learning via the \lstinline|partial_fit| method. The concept behind the \lstinline|SGDClassifier| class is \textit{similar to the stochastic gradient algorithm previously implemented for Adaline}.

    \begin{obs}
        We could initialize the SGD version of the perceptron (\lstinline|loss='perceptron'|), logistic regression (\lstinline|loss='log'|), and an SVM with default parameters (\lstinline|loss='hinge'|) as follows:

        \begin{lstlisting}[caption={caption},label=DescriptiveLabel]
from sklearn.linear_model import SGDClassifier

ppn = SGDClassifier(loss='perceptron')
lr = SGDClassifier(loss='log')
svm = SGDClassifier(loss='hinge')
        \end{lstlisting}
    \end{obs}
    
    \begin{mydef}[\textbf{SGDClassifier}]
        \textbf{SGDClassifier} es una \textit{clase de scikit-learn que implementa clasificadores lineales entrenados usando Stochastic Gradient Descent (SGD)}.
    \end{mydef}

    \section{Solving Nonlinear Problems Using a Kernel SVM}

    Another reason why SVMs enjoy high popularity among machine learning practitioners is that they can be easily \textbf{kernelized} to solve nonlinear classification problems.

    Before we discuss the main concept behind the so-called kernel SVM, the most common variant of SVMs, let's first create a synthetic dataset to see what such a nonlinear classification problem may look like.

    \subsection{Kernel Methods for Linearly Inseparable Data}

    Using the following code, we will create a simple dataset that has the form of an \lstinline|XOR| gate using the \lstinline|logical_xor| function from NumPy, where 100 examples will be assigned the class label 1, and 100 examples will be assigned the class label -1:

    \lstinputlisting[caption={\lstinline|XOR| in Python.},label=code:xor_graph]{Code/Logical XOR/logical_xor_code.py}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/logical_xor_image.png} \\
            \caption{\lstinline|XOR| Graph in Python.}
            \label{figure:xor_graph_python}
        \end{minipage}
    \end{figure}

    Obviously, we would not be able to separate the examples from the positive and negative class very well using a linear hyperplane as a decision boundary via the linear logistic regression or linear SVM model that we discussed earlier.

    \begin{idea}
        The basic idea behind kernel methods for dealing with such linearly inseparable data is to create nonlinear combinations of the original features to project them onto a higher-dimensional space via a mapping function, $\varphi$, where the data becomes linearly separable. We can transform a two-dimensional dataset into a new three-dimensional feature space, where the classes become separable via the following projection:
        \begin{equation*}
            \begin{array}{rccl}
                \phi:&\bbm{R}^2&\to&\bbm{R}^3\\
                &(x_1,x_2)&\mapsto&(x_1,x_2,x_1^2+x_2^2)=(z_1,z_2,z_3)\\
            \end{array}
        \end{equation*}
    \end{idea}

    This idea allows us to separate the two classes shown in the plot via a linear hyperplane that becomes a nonlinear decision boundary if we project it back onto the original feature space, as illustrated with this dataset:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/_8d14bc1fc7764c5c99da905f854c8172_9781801819312_03_48.png} \\
            \caption{Dataset and Projection of Decision Boundary.}
            \label{figure:projection_decision_boundary}
        \end{minipage}
    \end{figure}

    \subsection{Using the Kernel Trick to Find Separating Hyperplanes in a High-Dimensional Space}

    To solve a nonlinear problem using an SVM, we would \textit{transform the training data into a higher-dimensional feature space via a mapping function, $\phi$, and train a linear SVM model to classify the data in this new feature space}. Then, we could use the same mapping function, $\phi$, to transform new, unseen data to classify it using the linear SVM model.  

    \begin{obs}[\textbf{Problem with Construction of New Features}]
        However, one problem with this mapping approach is that \textit{the construction of the new features is computationally very expensive, especially if we are dealing with high-dimensional data}.
    \end{obs}

    This is where the so-called \textbf{kernel trick} comes into play. Although we did not go into much detail about how to solve the quadratic programming task to train an SVM, in practice, we just need to replace the dot product $x^{(i)T} x^{(j)}$ by $\phi(x^{(i)})^T \phi(x^{(j)})$. To save the expensive step of calculating this dot product between two points explicitly, we define a so-called kernel function:

    \begin{equation*}
        k(x^{i},x^{j})=\phi(x^{i})^T \phi(x^{j})
    \end{equation*}

    One of the most widely used kernels is the radial basis function (RBF) kernel, which can simply be called the Gaussian kernel:

    \begin{equation*}
    k(x^{i}, x^{j}) = \exp\left(-\frac{\|x^{i} - x^{j}\|^2}{2\sigma^2}\right)
    \end{equation*}

    This is often simplified to:

    \begin{equation*}
    k(x^{i}, x^{j}) = \exp\left(-\gamma \|x^{i} - x^{j}\|^2\right)
    \end{equation*}

    Here, $\gamma = \frac{1}{2\sigma^2}$ is a free parameter to be optimized.

    \begin{obs}
        Roughly speaking, the term "kernel" can be interpreted as a \textit{similarity function between a pair of examples}.
        
        The minus sign inverts the distance measure into a similarity score, and, due to the exponential term, \textit{the resulting similarity score will fall into a range between 1} (for exactly similar examples) \textit{and 0} (for very dissimilar examples).
    \end{obs}

    Now that we have covered the big picture behind the kernel trick, let's see if we can train a kernel SVM that is able to draw a nonlinear decision boundary that separates the XOR data well. Here, we simply use the SVC class from scikit-learn that we imported earlier and replace the \lstinline|kernel='linear'| parameter with \lstinline|kernel='rbf'|.

    Note: Fit the SVM model and plot decision regions.

    As we can see in the resulting plot, the kernel SVM separates the XOR data relatively well:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/xor_training.png} \\
            \caption{XOR Separation Using RBF.}
            \label{figure:rbf_training}
        \end{minipage}
    \end{figure}

    \lstinputlisting[caption={Decision Boundary using RBF Kernel on the XOR Dataset.},label=code:rbf_kernel_xor]{Code/Logical XOR/logical_xor_rbf.py}

    The $\gamma$ parameter, which we set to $\text{gamma}=0.1$, can be understood as a \textit{cut-off parameter for the Gaussian sphere}; increasing \textit{$\gamma$ increases the influence of the training examples}, leading to a tighter and bumpier decision boundary. To get a better understanding of $\gamma$, let's apply an RBF kernel SVM to our Iris flower dataset:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/iris_svc.png} \\
            \caption{Iris SVC using RBF.}
            \label{figure:iris_sbf}
        \end{minipage}
    \end{figure}

    \lstinputlisting[caption={Decision Boundary on the Iris Dataset using RBF.},label=code:iris_rbf]{Code/SVC/svc_iris.py}

    If we increase the value of gamma, the following happens:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=0.75]{images/iris_gamma_1.png} \\
            \caption{Iris Dataset using RBF Kernel SVM with $\gamma=1$.}
            \label{figure:iris_svc_gamma_1}
        \end{minipage}
    \end{figure}

    \begin{obs}
        Although the model fits the training dataset very well, \textit{such a classifier will likely have a high generalization error on unseen data}. This illustrates that the \textit{$\gamma$ parameter also plays an important role in controlling overfitting or variance} when the algorithm is too sensitive to fluctuations in the training dataset.
    \end{obs}

    \newpage

    \section{Decision Tree Learning}

    \begin{mydef}[\textbf{Decision Tree}]
        \textbf{Decision tree} classifiers are attractive models if we care about interpretability. As the name \textbf{decision tree} suggests, we can think of this model as \textit{breaking down our data by making a decision based on asking a series of questions}.
    \end{mydef}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/_c260573abced4dbd8aeadcf1689420db_9781801819312_03_60.png} \\
            \caption{Decision Tree}
            \label{Texto}
        \end{minipage}
    \end{figure}

    Based on the features in our training dataset, the decision tree model learns a series of questions to infer the class labels of the examples.

    \begin{obs}
        Although the concept of a decision tree based on categorical variables is illustrated, the same concept applies if our features are real numbers, like in the Iris dataset.
        
        For example, we could simply define a cut-off value along the sepal width feature axis and ask a binary question: \textit{Is the sepal width $\geq$ 2.8 cm?}
    \end{obs}

    Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest information gain ($IG$), which will be explained in more detail in the following section.

    In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure. This means that the training examples at each node all belong to the same class.

    \begin{idea}
        In practice, this can result in a very deep tree with many nodes, which can easily lead to overfitting. Thus, we typically want to prune the tree by setting a limit for the maximum depth of the tree.
    \end{idea}

    \subsection{Maximizing IG Getting the Most Bang for Your Buck}

    To split the nodes at the most informative features, we need to define an objective function to optimize via the tree learning algorithm. Here, our objective function is to maximize the $IG$ at each split, which we define as follows:

    \begin{equation*}
        IG(D_p,f)=I(D_p)-\sum_{ j=1}^{m}\frac{ N_j}{N_p}I(D_j)
    \end{equation*}

    Here:
    \begin{itemize}
        \item $f$ is the feature to perform the split.
        \item $D_p$ and $D_j$ are the datasets of the parent and $j$-th child node.
        \item $I$ is our \textbf{impurity measure}.
        \item $N_p$ is the total number of training examples at the parent node.
        \item $N_j$ is the number of examples in the $j$-th node.
    \end{itemize}
    The information gain (function $IG$) is simply the difference between the impurity of the parent node and the sum of the child node impurities — the lower the impurities of the child nodes, the larger the information gain.

    \begin{obs}
        However, for simplicity and to reduce the combinatorial search space, most libraries (including scikit-learn) implement binary decision trees. This means that each parent node is split into two child nodes: $D_{ left}$ and $D_{ right}$.
        \begin{equation*}
            IG(D_p,f) = I(D_p) - \frac{N_{left}}{N_p}I(D_{ left}) - \frac{N_{right }}{N_p}I(D_{ right})
        \end{equation*}
    \end{obs}

    The three impurity measures ($I$) or splitting criteria that are commonly used in binary decision trees are:
    \begin{itemize}
        \item \textbf{Gini impurity} ($I_G$).
        \item \textbf{Entropy} ($I_H$).
        \item \textbf{Classification error} ($I_E$).
    \end{itemize}
    
    \begin{mydef}[\textbf{Entropy $I_H$}]
        The definition of \textbf{entropy} for all non-empty classes is given by:
        \begin{equation*}
            \left\{
                \begin{array}{l}
                    p( i | t) \neq 0 \\
                    I_H(t)=-\displaystyle\sum_{ i=1}^{c}p( i | t)\log_2 p( i | t)
                \end{array}
            \right.
        \end{equation*}
        Here:
        \begin{itemize}
            \item $p( i | t)$ is the proportion of examples that belong to class $i$ for a particular node $t$.
        \end{itemize}
    \end{mydef}

    \begin{obs}
        The entropy is therefore zero if all examples at a node belong to the same class, and the entropy is maximal if we have a uniform class distribution.
    \end{obs}

    \begin{exa}[\textbf{Objective of the Entropy Criterion}]
        In a binary class setting, the entropy is 0 is $p( i = 0 | t) = 1$ or $p( i = 0| t) = 0$. If the classes are distributed uniformly with $p( i = 0 | t) = \frac{1}{2}$ or $p( i = 0| t) = \frac{1}{2}$, the entropy is 1. Therefore, we can say that \textbf{the entropy criterion attempts to maximize tue mutual information in the tree}.
    \end{exa}

    To provide a visual intuition, let us visualize the entropy values for different class distributions via the following code:

    \lstinputlisting[caption={Entropy Computation.},label=code:entropy_computation]{Code/Decision Tree/example.py}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=.6]{images/entropy.png} \\
            \caption{Entropy Visualization}
            \label{Texto}
        \end{minipage}
    \end{figure}

    \newpage

    \begin{mydef}[Gini Impurity]
        The Gini impurity is given by:
        \begin{equation*}
            I_G(t)=\sum_{ i=1}^{c}p(i|t)\left(1-p(i|t)\right)=1-\sum_{ i=1}^{c}p(i|t)^2
        \end{equation*}
    \end{mydef}

    \begin{obs}
        That is, the Gini Impurity can be understood as a \textbf{criterion to minimize the probability of misclassification}.
    \end{obs}

    \begin{obs}[\textbf{Relation Between Gini Impurity and Entropy}]
        Similar to entropy, Gini Impurity is maximal if the classes are perfectly mixed, for example, in a binary class setting (where $c=2$):
        \begin{equation*}
            I_G(t)=1-\sum_{ i=1}^{c}\left(\frac{1}{2}\right) ^2=\frac{1}{2}
        \end{equation*}
        In practice, \textbf{both the Gini impurity and entropy typically yield very similar results}, and it is often not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs.
    \end{obs}

    \begin{mydef}[\textbf{Pruning}]
        \textbf{Pruning} means \textit{removing parts of a model that are unnecessary or harmful, with the goal of making the model simpler, faster, and better at generalizing}.
    \end{mydef}

    \begin{mydef}[\textbf{Classification Error}]
        Another impurity measure is the \textbf{classification error}:
        \begin{equation*}
            I_E(t)=1-\max\left\{p(i|t)\right\}
        \end{equation*}
    \end{mydef}

    \textbf{This is a useful criterion for pruning}, but not recommended for growing a decision tree, since it \textbf{is less sensitive to changes in the class probabilities of the nodes}. We can illustrate this by looking at the two possible splitting scenarios shown below:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/_93c228db20884e308b9cc38981f46125_9781801819312_03_69.png} \\
            \caption{Decision Trees $A$ and $B$.}
            \label{Texto}
        \end{minipage}
    \end{figure}

    \begin{exa}
        We start with a dataset $D_p$, at the parent node, which consists of 40 examples from class 1 and 40 examples from class 2, that we split into two datasets, $D_{ left}$ and $D_{ right}$.
        \begin{itemize}
            \item The information gain using the \textbf{classification error} as a splitting criterion would be the same (since $IG=0.25$) in both scenarios, $A$ and $B$:
            \begin{equation*}
            \left\{
            \begin{array}{l}
            I_E(D_p) = 1 - 0.5 = 0.5 \\[6pt]

            A:\; I_E(D_{\text{left}}) = 1 - \frac{3}{4} = 0.25 \\
            A:\; I_E(D_{\text{right}}) = 1 - \frac{3}{4} = 0.25 \\
            A:\; IG_E = 0.5 - \frac{4}{8}\,0.25 - \frac{4}{8}\,0.25 = 0.25 \\[6pt]

            B:\; I_E(D_{\text{left}}) = 1 - \frac{4}{6} = \frac{1}{3} \\
            B:\; I_E(D_{\text{right}}) = 1 - 1 = 0 \\
            B:\; IG_E = 0.5 - \frac{6}{8}\times \frac{1}{3} - 0 = 0.25
            \end{array}
            \right.
            \end{equation*}
            \item However, the \textbf{Gini impurity} would favor the split in scenario $B$ ($IG_G \approx 0.1667$) over scenario $A$ ($IGG = 0.125$), which is indeed purer:
            \begin{equation*}
            \left\{
            \begin{array}{l}
            I_G(D_p) = 1 - \left(0.5^2 + 0.5^2\right) = 0.5 \\[6pt]

            A:\; I_G(D_{\text{left}}) = 1 - \left(\left(\frac{3}{4}\right)^2 + \left(\frac{1}{4}\right)^2\right) 
            = \frac{3}{8} = 0.375 \\

            A:\; I_G(D_{\text{right}}) = 1 - \left(\left(\frac{1}{4}\right)^2 + \left(\frac{3}{4}\right)^2\right) 
            = \frac{3}{8} = 0.375 \\

            A:\; IG_G = 0.5 - \frac{4}{8}\,0.375 - \frac{4}{8}\,0.375 = 0.125 \\[6pt]

            B:\; I_G(D_{\text{left}}) = 1 - \left(\left(\frac{2}{6}\right)^2 + \left(\frac{4}{6}\right)^2\right) 
            = \frac{4}{9} = 0.\overline{4} \\

            B:\; I_G(D_{\text{right}}) = 1 - (1^2 + 0^2) = 0 \\

            B:\; IG_G = 0.5 - \frac{6}{8}\,0.\overline{4} - 0 = 0.1\overline{6}
            \end{array}
            \right.
            \end{equation*}
            \item Similarly, the \textbf{entropy criterion} would also favor scenario $B$ ($IG_H = 0.31$) over scenario $A$ ($IG_H = 0.19$):
            \begin{equation*}
            \left\{
            \begin{array}{l}
            I_H(D_p) = -\left(0.5 \log_2(0.5) + 0.5 \log_2(0.5)\right) = 1 \\[6pt]

            A:\; I_H(D_{\text{left}}) 
            = -\left(\frac{3}{4}\log_2\!\left(\frac{3}{4}\right) 
            + \frac{1}{4}\log_2\!\left(\frac{1}{4}\right)\right) = 0.81 \\

            A:\; I_H(D_{\text{right}}) 
            = -\left(\frac{1}{4}\log_2\!\left(\frac{1}{4}\right) 
            + \frac{3}{4}\log_2\!\left(\frac{3}{4}\right)\right) = 0.81 \\

            A:\; IG_H = 1 - \frac{4}{8}\,0.81 - \frac{4}{8}\,0.81 = 0.19 \\[6pt]

            B:\; I_H(D_{\text{left}}) 
            = -\left(\frac{2}{6}\log_2\!\left(\frac{2}{6}\right) 
            + \frac{4}{6}\log_2\!\left(\frac{4}{6}\right)\right) = 0.92 \\

            B:\; I_H(D_{\text{right}}) = 0 \\

            B:\; IG_H = 1 - \frac{6}{8}\,0.92 - 0 = 0.31
            \end{array}
            \right.
            \end{equation*}
        \end{itemize}
    \end{exa}

    For a more visual comparison of the three different impurity criteria that we discussed previously, let's plot the impurity indices for the probability range $[0, 1]$ for class $1$. Note that we will also add a scaled version of the entropy (entropy / 2) to observe that the Gini impurity is an intermediate measure between entropy and the classification error. The code is as follows: 

    \lstinputlisting[caption={Entropy Computation.},label=code:gini_entropy_error]{Code/Decision Tree/gini_entropy_error.py}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/gini_entropy_error.png} \\
            \caption{Caption}
            \label{Texto}
        \end{minipage}
    \end{figure}

    \begin{idea}
        This example illustrates that, although two splits may appear equivalent under a coarse criterion such as classification error, more sensitive criteria like Gini impurity and entropy better capture the true purity of the resulting nodes, favoring splits that produce highly homogeneous subsets. In summary:
        
        \begin{longtable}{
        >{\centering\arraybackslash}p{2.5cm}
        >{\centering\arraybackslash}p{2.5cm}
        >{\centering\arraybackslash}p{2.5cm}
        p{6cm}
        }
        \toprule
        \textbf{Key Point} & \textbf{Criterion} & \textbf{Sensitivity} & \textbf{Interpretation} \\
        \midrule
        \endfirsthead

        \toprule
        \textbf{Key Point} & \textbf{Criterion} & \textbf{Sensitivity} & \textbf{Interpretation} \\
        \midrule
        \endhead

        \bottomrule
        \caption{Comparison of impurity criteria in decision tree splitting.}
        \label{tab:impurity_comparison}
        \endlastfoot

        (1) Informative power
        & Classification Error
        & Low
        & Insensitive to changes in class distribution; only considers the majority class and may assign the same information gain to qualitatively different splits. \\

        (1) Informative power
        & Gini Impurity
        & Medium
        & Sensitive to class proportions and able to distinguish between moderately and highly pure nodes. \\

        (1) Informative power
        & Entropy
        & High
        & Highly sensitive to uncertainty; strongly rewards splits that produce nodes with high class concentration. \\

        \midrule

        (2) Preference for pure nodes
        & Gini / Entropy
        & High
        & Both criteria favor splits that generate highly pure (or perfectly pure) child nodes, even if the remaining child node becomes less pure. \\

        \midrule

        (3) Relative position
        & Classification Error
        & Very Low
        & Nearly flat behavior as a function of class probability; provides a coarse measure of impurity. \\

        (3) Relative position
        & Gini Impurity
        & Medium
        & Acts as an intermediate measure between entropy and classification error, balancing sensitivity and stability. \\

        (3) Relative position
        & Entropy
        & Very High
        & Exhibits strong curvature near probabilities close to 0 or 1, heavily penalizing uncertainty and strongly favoring purity. \\

        \end{longtable}
    \end{idea}

    \section{Building a Decision Tree}

    Decision trees can build complex decision boundaries by dividing the feature space into rectangles.

    \begin{obs}
        However, we have to be careful since the deeper the decision tree, the more complex the decision boundary becomes, which can easily result in overfitting.
    \end{obs}

    Using scikit-learn, we will now train a decision tree with a maximum depth of 4, using the Gini impurity as a criterion for impurity.

    Although \textit{feature scaling may be desired for visualization purposes, note that feature scaling is not a requirement for decision tree algorithms}. The code is as follows:

    \lstinputlisting[caption={Code for Decision Tree Implementation.},label=code:gini_entropy_error]{Code/Decision Tree/exa_decision_tree.py}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/decision_tree_visualized.png} \\
            \caption{Decision Tree Boundary Visualized.}
            \label{Texto}
        \end{minipage}
    \end{figure}

    \begin{obs}
        A nice feature in scikit-learn is that it allows us to readily visualize the decision tree model after training via the following code:
        \begin{lstlisting}[caption={caption},label=DescriptiveLabel]
from sklearn import tree
feature_names = ['Sepal length', 'Sepal width',
                 'Petal length', 'Petal width']
tree.plot_tree(tree_model,
               feature_names=feature_names,
               filled=True)
plt.show()
        \end{lstlisting}
    \end{obs}

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/tree_visualization.png} \\
            \caption{Tree Visualized.}
            \label{Texto}
        \end{minipage}
    \end{figure}

    Setting \lstinline|filled=True| in the \lstinline|plot_tree| function we called colors the nodes by the majority class label at that node.

    \begin{obs}
        There are many additional options available, which you can find in the documentation at \href{https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html}{Plot Tree}.
    \end{obs}

    Looking at the root node, it starts with \lstinline|105| examples at the top. The first split uses a sepal width $cut-off \leq 0.75$ cm for splitting the root node into two child nodes with 35 examples (left child node) and \lstinline|70| examples (right child node). After the first split, we can see that the left child node is already pure and only contains examples from the Iris-setosa class (\lstinline|Gini impurity = 0|). The further splits on the right are then used to separate the examples from the Iris-versicolor and Iris-virginica class.

    \begin{idea}
        The decision tree does a very good job of separating the flower classes. Unfortunately, scikit-learn currently does not implement functionality to manually post-prune a decision tree. However, we could go back to our previous code example, change the \lstinline|max_depth| of our decision tree to 3, and compare it to our current model, but we leave this as an exercise for the interested reader.
    \end{idea}

    \begin{obs}[\textbf{Cost Complexity Post-Pruning}]
        Alternatively, scikit-learn provides an automatic cost complexity post-pruning procedure for decision trees. Interested readers can find more information about this more advanced topic in the following tutorial: \href{https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html}{Plot Cost Complexity Pruning}.
    \end{obs}

    \subsection{Combining Multiple Decision Trees via Random Forests}

    Ensemble methods have gained huge popularity in applications of machine learning during the last decade due to their good classification performance and robustness toward overfitting.

    Let's discuss the decision tree-based random forest algorithm, which is known for its good scalability and ease of use. 

    \begin{mydef}[\textbf{Random Forest}]
        A \textbf{random forest} can be considered as \textit{an ensemble of decision trees}.
        
        The idea behind a random forest is to \textit{average multiple (deep) decision trees that individually suffer from high variance to build a more robust model that has a better generalization performance and is less susceptible to overfitting}.
    \end{mydef}

    The random forest algorithm can be summarized in four simple steps:
    \begin{enumerate}[label = \textit{\arabic*)}]
        \item Draw a random bootstrap sample of size $n$ (randomly choose $n$ examples from the training dataset with replacement).
        \item Grow a decision tree from the bootstrap sample. At each node:
        \begin{itemize}
            \item Randomly select $d$ features without replacement.
            \item Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain.
        \end{itemize}
        \item Repeat steps \textit{1)}-\textit{2)} $k$ times.
        \item Aggregate the prediction by each tree to assign the class label by majority vote.
    \end{enumerate}

    \begin{obs}
        We should note one slight modification in step \textit{2)} when we are training the individual decision trees: instead of evaluating all features to determine the best split at each node, we only consider a random subset of those.
    \end{obs}

    %TODO

    \section{$K$-Nearest Neighbours}

    The last supervised learning algorithm that we want to discuss in this section is the k-nearest neighbor (KNN) classifier, which is particularly interesting because it is fundamentally different from the learning algorithms discussed so far.

    \begin{obs}
        KNN is a typical example of a lazy learner. It is called \textit{lazy} not because of its apparent simplicity, but because \textit{it doesn't learn a discriminative function from the training data but memorizes the training dataset instead}.
    \end{obs}

    KNN belongs to a subcategory of non-parametric models described as instance-based learning. \textit{Models based on instance-based learning memorize the training dataset, and lazy learning is a special case of instance-based learning that incurs no cost during the learning process}.

    \begin{itemize}
        \item \textbf{Parametric} models estimate parameters.
        \item \textbf{Non-parametric} models adapt to data.
    \end{itemize}

    The KNN algorithm itself is straightforward and can be summarized by the following steps:
    \begin{enumerate}[label = \textit{\arabic*)}]
        \item Choose the number of $k$ and a distance metric
        \item Find the $k$-nearest neighbors of the data record to classify
        \item Assign the class label by majority vote
    \end{enumerate}

    Based on the chosen distance metric, the KNN algorithm finds the $k$ examples in the training dataset that are closest (most similar) to the point that we want to classify.
 
    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/_b2d8b536d6b341f2a434158299f570ad_9781801819312_03_78.png} \\
            \caption{$k$-Nearest Neighbours Algorithm}
            \label{Texto}
        \end{minipage}
    \end{figure}

    \begin{obs}
        The class label of the data point is then determined by a majority vote among its k nearest neighbors.
    \end{obs}

    \subsection{Advantages and disadvantages of memory-based approaches}

    \begin{itemize}
        \item \textbf{Advantage} of a memory-based approach is that the classifier immediately adapts as new training data are collected.
        
        \item \textbf{Downside} is that the computational complexity for classifying new examples grows linearly with the number of examples in the training dataset in the worst-case scenario—unless the dataset has very few dimensions and the algorithm uses efficient data structures for querying the training data.
    \end{itemize}

    Such data structures include $k-d$ tree and ball tree, which are both supported in scikit-learn.
    
    In addition to computational costs, large datasets can be problematic in terms of storage requirements.

    \begin{idea}
        For relatively small to medium-sized datasets, memory-based methods can provide good predictive and computational performance and are therefore suitable for many real-world problems.
    \end{idea}

    By executing the following code, we implement a KNN model in scikit-learn using a Euclidean distance metric:

    \lstinputlisting[caption={Code for $k$-Nearest Neighbours.},label=code:k_nearest_neighbours.py]{Code/k_nearest_neighbours/k_nearest_neighbours.py}

    By specifying five neighbors, we obtain a relatively smooth decision boundary:

    \begin{figure}[h]
        \begin{minipage}{\textwidth}
            \centering
            \includegraphics[scale=1]{images/k_neighbours.png} \\
            \caption{Caption}
            \label{Texto}
        \end{minipage}
    \end{figure}

    \begin{obs}
        In the case of a tie, scikit-learn prefers neighbors with a closer distance. If the neighbors have identical distances, the algorithm chooses the class label that appears first in the training dataset.
    \end{obs}

    \begin{idea}
        Choosing $k$ is crucial to finding a good balance between overfitting and underfitting. A suitable distance metric is also essential. A simple Euclidean distance measure is often used for real-valued features, such as the centimeters-based measurements in the Iris dataset.
    \end{idea}

    When using Euclidean distance, standardizing the data is important so that each feature contributes equally to the distance. The minkowski distance used in the preceding code is a generalization of the Euclidean and Manhattan distances and can be written as follows:

    \begin{equation*}
        d_{p}(x,y) = \left( \sum_{i=1}^{n} |x_{i} - y_{i}|^{p} \right)^{1/p}, p\in\mathbb{N}
    \end{equation*}

    It becomes the Euclidean distance when \lstinline|p=2| and the Manhattan distance when \lstinline|p=1|. Many other distance metrics are available in scikit-learn and can be provided to the metric parameter.

    \begin{obs}
        KNN is susceptible to overfitting due to the curse of dimensionality, where the feature space becomes increasingly sparse as dimensionality grows. Even the closest neighbors can be too far away in a high-dimensional space to provide a good estimate.
    \end{obs}

    Regularization helps avoid overfitting in models such as logistic regression, but it is not applicable to decision trees and KNN. Instead, feature selection and dimensionality reduction techniques help mitigate the curse of dimensionality.

    \begin{obs}
        Note: Use RAPIDS' cuML for GPU-accelerated machine learning with large datasets.
    \end{obs}

\end{document}