\documentclass[12pt]{report}

% --- Idioma y codificación ---
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

% --- Matemáticas ---
\usepackage{amsmath, amssymb, amsthm}

% --- Gráficos y figuras ---
\usepackage{graphics, graphicx, subfigure}
\usepackage{tikz, pgffor, ifthen}

% --- Tablas y estructuras ---
\usepackage{array, multicol, longtable, booktabs}

% --- Listas y enumeraciones ---
\usepackage{enumerate, enumitem}

% --- Márgenes y geometría ---
\usepackage[a4paper, margin=1.5cm]{geometry}

% --- Diseño y marco ---
\usepackage[framemethod=TikZ]{mdframed}

% --- Texto y contenido de prueba ---
\usepackage{lipsum}

\usepackage{subfiles}

% --- Hipervínculos ---
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{bbm}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=cyan
}

% --- Código fuente (listings) ---
\usepackage{listings}
\usepackage{xcolor}

\definecolor{listing-background}{HTML}{F7F7F7}
\definecolor{listing-rule}{HTML}{B3B2B3}
\definecolor{listing-numbers}{HTML}{B3B2B3}
\definecolor{listing-text-color}{HTML}{000000}
\definecolor{listing-keyword}{HTML}{435489}
\definecolor{listing-keyword-2}{HTML}{1284CA}
\definecolor{listing-keyword-3}{HTML}{9137CB}
\definecolor{listing-identifier}{HTML}{435489}
\definecolor{listing-string}{HTML}{00999A}
\definecolor{listing-comment}{HTML}{8E8E8E}

\lstdefinestyle{myStyle}{
    language=Python,
    alsolanguage=C++,
    numbers=left,
    xleftmargin=2.7em,
    framexleftmargin=2.5em,
    backgroundcolor=\color{gray!15},
    basicstyle=\color{listing-text-color}\linespread{1.0}\ttfamily,
    breaklines=true,
    frameshape={RYR}{Y}{Y}{RYR},
    rulecolor=\color{black},
    tabsize=2,
    numberstyle=\color{listing-numbers}\linespread{1.0}\small\ttfamily,
    aboveskip=1.0em,
    belowskip=0.1em,
    abovecaptionskip=0em,
    belowcaptionskip=1.0em,
    keywordstyle={\color{listing-keyword}\bfseries},
    keywordstyle={[2]\color{listing-keyword-2}\bfseries},
    keywordstyle={[3]\color{listing-keyword-3}\bfseries\itshape},
    sensitive=true,
    identifierstyle=\color{listing-identifier},
    commentstyle=\color{listing-comment},
    stringstyle=\color{listing-string},
    showstringspaces=false,
    label=lst:bar,
    captionpos=b
}
\lstset{style=myStyle}

% --- Marca de agua ---
\usepackage{eso-pic}
\AddToHook{shipout/foreground}{
    \begin{tikzpicture}[remember picture,overlay]
        \node at (current page.center){
            \includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{watermark-1.png}
        };
    \end{tikzpicture}
}

% --- Redefiniciones de encabezados de capítulo y sección ---
\makeatletter
% Capítulo (estilo original conservado)
\def\@makechapterhead#1{%
  {\parindent \z@ \raggedright
    \reset@font
    \hrule
    \vspace*{10\p@}%
    \par
    \center \LARGE \scshape \@chapapp{} \huge \thechapter
    \vspace*{10\p@}%
    \par\nobreak
    \vspace*{10\p@}%
    \par
    \vspace*{1\p@}%
    \hrule
    \vspace*{30\p@}  % Espaciado reducido
    \centering\Huge \scshape #1\par\nobreak  % Centrado y scshape
    \vskip 30\p@  % Espaciado reducido
  }}


% Sección
\renewcommand{\section}{\@startsection{section}{1}{\z@}%
  {-2.5ex \@plus -0.5ex \@minus -0.1ex}%  % Espaciado superior reducido
  {1ex \@plus 0.1ex}%                     % Espaciado inferior reducido
  {\normalfont\Large\sectionstyle}}
\newcommand{\sectionstyle}[1]{%
  \par\noindent\hrule
  \vspace{0.2ex}%   % Espaciado entre líneas reducido
  {\scshape{#1}\par}%  % Centrado perfecto y scshape
  \vspace{0.4ex}%   % Espaciado entre líneas reducido
  \hrule
}

% Subsección
\renewcommand{\subsection}{\@startsection{subsection}{2}{\z@}%
  {-2ex \@plus -0.4ex \@minus -0.1ex}%  % Espaciado superior reducido
  {0.8ex \@plus 0.1ex}%                 % Espaciado inferior reducido
  {\normalfont\large\subsectionstyle}}
\newcommand{\subsectionstyle}[1]{%
  \par\noindent\hrule
  \vspace{-0.4ex}%  % Espaciado entre líneas reducido
  {\scshape #1\par}%  % Centrado perfecto y scshape
  \vspace{0.4ex}%  % Espaciado entre líneas reducido
  \hrule
}

% Subsubsección
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{\z@}%
  {-1.5ex \@plus -0.3ex \@minus -0.1ex}%  % Espaciado superior reducido
  {0.5ex \@plus 0.1ex}%                   % Espaciado inferior reducido
  {\normalfont\normalsize\subsubsectionstyle}}
\newcommand{\subsubsectionstyle}[1]{%
  \par\noindent\hrule
  \vspace{0.4ex}%   % Espaciado entre líneas reducido
  {\scshape #1\par}%  % Centrado perfecto y scshape
  \vspace{0.4ex}%   % Espaciado entre líneas reducido
  \hrule
}
\makeatother

% --- Entornos personalizados ---
% (aquí puedes definir tus theorems, definiciones, etc.)


% --- Entornos personalizados ---
\newtheoremstyle{largebreak}{}{ }{\normalfont}{}{\bfseries}{}{\newline}{}
\theoremstyle{largebreak}

\newmdtheoremenv[hidealllines=true,roundcorner=5pt,backgroundcolor=gray!60!red!30]{exa}{Example}[section]
\newmdtheoremenv[hidealllines=true,roundcorner=5pt,backgroundcolor=gray!50!blue!30]{obs}{Observation}[section]
\newmdtheoremenv[hidealllines=true,roundcorner=5pt,backgroundcolor=green!50!blue!30]{preg}{Question}[section]
\newmdtheoremenv[hidealllines=true,roundcorner=5pt,backgroundcolor=yellow!40]{idea}{Idea}[section]
\newmdtheoremenv[rightline=false,leftline=false]{theor}{Theorm}[section]
\newmdtheoremenv[rightline=false,leftline=false]{propo}{Proposition}[section]
\newmdtheoremenv[rightline=false,leftline=false]{cor}{Corollary}[section]
\newmdtheoremenv[rightline=false,leftline=false]{lema}{Lemma}[section]
\newmdtheoremenv[roundcorner=5pt,backgroundcolor=gray!30,hidealllines=true]{mydef}{Definition}[section]
\newmdtheoremenv[roundcorner=5pt]{excer}{Excercise}[section]

% --- Comandos auxiliares ---
\def\proof{\paragraph{Proof:\\}}
\def\endproof{\hfill$\blacksquare$}
\def\sol{\paragraph{Solution:\\}}
\def\endsol{\hfill$\square$}

\newcommand\logit[1]{\ensuremath{\textup{logit}\left(#1\right)}}
\newcommand\abs[1]{\ensuremath{\left|#1\right|}}
\newcommand\divides{\ensuremath{\bigm|}}
\newcommand\cf[3]{\ensuremath{#1:#2\rightarrow#3}}
\newcommand\contradiction{\ensuremath{\#_c}}
\newcommand\natint[1]{\ensuremath{\left[\big|#1\big|\right]}}
\newcommand\bbm[1]{\ensuremath{\mathbbm{#1}}}

\newcounter{figcount}
\setcounter{figcount}{1}

\renewcommand{\lstlistingname}{Code}
\renewcommand{\lstlistlistingname}{{\lstlistingname} List}

% --- Comienzo del documento ---
\begin{document}
    \setlength{\parskip}{5pt}
    \setlength{\parindent}{12pt}
    \title{Random Variables}
    \author{Cristo Daniel Alvarado}
    \maketitle

    \tableofcontents

    \lstlistoflistings

    \chapter{Random Variables}

    \section{Definition and Types of Random Variables}

    It frequently occurs that in performing an experiment we are mainly interested in some functions of the outcome as opposed to the outcome itself.

    \begin{exa}
      In the experiment of tossing two dices, we may be interested in the sum of the two dices and not on the actual outcome of the experiment.

      This is, we may not be interested in whether the outcome is $(1, 6)$ or $(2, 5)$ or $(3, 4)$ or $(4, 3)$ or $(5, 2)$ or $(6, 1)$.
    \end{exa}

    These quantities of interest, which are real-valued functions defined on a sample space, are called \textbf{random variables}.

    \begin{mydef}[\textbf{Random Variables}]
      Let $S$ be a sample space. A \textbf{random variable}, denoted by $X$ is a \textit{real-valued function defined on the sample space of an experiment}.
    \end{mydef}

    \begin{obs}[\textbf{Sample Space}]
      This \textit{set of all possible outcomes of an experiment} is known as the \textbf{sample space} of the experiment and is denoted by $S$. Any subset $E$ of the sample space $S$ is called an \textbf{event}.
    \end{obs}

    \begin{exa}
      Let $X$ be a random variable defined as the sum of two fair dice. Then, $X$ can only take integer values between $2$ and $12$. It happens that:

      \begin{equation*}
        1=P\left(\bigcup_{ n=2}^{12} P(X=n)\right)=\sum_{ n=2}^{12} P(X=n)
      \end{equation*}

    \end{exa}

    \begin{exa}
      Suppose that we toss a coin having a probability $p$ of coming up heads, until the first head appears. Letting $N$ denote the number of flips required, then assuming that the outcome of successive flips are independent, $N$ is a random variable taking on one of the values $1, 2, 3, \dots ,$ with respective probabilities:
      \begin{equation*}
        \begin{split}
          P(N=1)&=p\\
          P(N=2)&=(1-p)p\\
          P(N=3)&=(1-p)^2p\\
          \vdots & \vdots \\
          P(N=3)&=(1-p)^{n-1}p\\
        \end{split}
      \end{equation*}

      It follows that:
      \begin{equation*}
        \sum_{ n=1}^{\infty} P(N=n)=\sum_{ n=1}^{\infty} (1-p)^{n-1}p=\frac{p}{1-(1-p)}=1
      \end{equation*}
        
    \end{exa}

    \begin{exa}
      Suppose that our experiment consists of seeing how long a battery can operate before wearing down. Suppose also that we are not primarily interested in the actual lifetime of the battery but are concerned only about whether or not the battery lasts at least two years. In this case, we may define the random variable $I$ by:
      \begin{equation*}
        I=\left\{
          \begin{array}{lr}
            1, & \text{if the battery lasts at least 2 years}\\
            0, & \text{otherwise}\\
          \end{array}
        \right.
      \end{equation*}

      If $E$ denotes the event that the battery lasts two or more years, then we have that the random variable $I$ is know as the \textbf{indicator random variable} of the event $E$ (sometimes denoted by $I_E$).  
    \end{exa}

    \begin{obs}
      In all of the preceding examples, the random variables of interest took on either a finite or a countable number of possible values. Such random variables are called \textbf{discrete random variables}.
    \end{obs}

    There are also \textbf{contiuous random variables}.

    \begin{obs}
      One example of a continuous random variable is the random variable denoting the lifetime of a car, when the car’s lifetime is assumed to take on any value in some interval $(a, b)$.
    \end{obs}

    \begin{mydef}[\textbf{Cummulative Distribution Function}]
      Let $X$ be a random variable. The \textbf{cummulative distrubution function} (called \textbf{CDF}) of $X$, is defined for any real number $b$, $-\infty<b<\infty$ as:
      \begin{equation*}
        F(b)=P(X\leq b)
      \end{equation*}
    \end{mydef}

    In other words, the CDF of a random variable denotes the probability that the random variable $X$ takes on a value of less than or equal to $b$.

    We verify that the CDF $F$ of a random variable $X$ satisfies the following properties:

    \begin{propo}[\textbf{Properties of CDF}]
      Let $X$ be a random variable with CDF $F$. Then:
      \begin{enumerate}[label=(\alph*)]
        \item $F$ is a non-decreasing function.
        \item $0\leq F(b)\leq 1$ for all $b\in \mathbb{R}$
        \item If $b_1<b_2$, then $F(b_1)\leq F(b_2)$
        \item $\lim_{ b\to -\infty} F(b)=0$ and $\lim_{ b\to \infty} F(b)=1$
      \end{enumerate}
    \end{propo}

    Turns out that all probability questions about the random variable $X$ can be answered if we know its CDF $F$.

    \begin{exa}
      If $a,b\in\bbm{R}$, then:
      \begin{equation*}
        P(a<X\leq b)=F(b)-F(a)
      \end{equation*}
      and,
      \begin{equation*}
        P(X<b)=\lim_{h\to0^+}P(X\leq b-h)=\lim_{ x\to b^-} F(x)
      \end{equation*}
    \end{exa}

    \section{Discrete Random Variables}

    A random variable that can take on at most a countable number of possible values is said to be discrete.

    \begin{mydef}[\textbf{Probability Mass Function}]
      Let $X$ be a discrete random variable, we define the \textbf{probability mass function} $p$ of $X$ by:
      \begin{equation*}
        p(x)=P(X=x)
      \end{equation*}
    \end{mydef}

    \begin{propo}
      Let $X$ be a discrete random variable with probability mass function $p$. Then there exists a countable subset $A\subseteq\bbm{R}$, say $A=\left\{a_1,\dots,a_n,\dots\right\}$ such that:
      \begin{enumerate}[label=(\alph*)]
        \item $p(a_i)>0$, for all $i\in\bbm{N}$
        \item $p(x)=0$, for all $x\in\bbm{R}\setminus A$.
      \end{enumerate}
      And,
      \begin{equation*}
        \sum_{ i=1}^\infty p(a_i)=1
      \end{equation*}
    \end{propo}

    \begin{obs}
      Also, the cummulative distribution function $F$ can be expressed in terms of $p(a)$ by:
      \begin{equation*}
        F(a)=\sum_{ x\leq a} p(x)
      \end{equation*}
    \end{obs}

    Discrete random variables are often classiﬁed according to their probability mass functions. We now consider some of these random variables.

    \subsection{Bernoulli Random Variable}

    Lets suppose that the trial of an experiment, whose outcome can be classified as a success or failure is performed. Let $X$ be the random variable defined by $X=1$ if the outcome is a success and $X=0$ if the outcome is a failure.

    Then, the probability mass function of $X$ is given by:
    \begin{equation*}
      p(x)=\left\{
        \begin{array}{lr}
          p, & \text{if } x=1\\
          1-p, & \text{if } x=0\\
          0, & \text{otherwise}\\
        \end{array}
      \right.
    \end{equation*}
    where $p$ is the probability of a success.

    \begin{mydef}[\textbf{Bernoulli Random Variable}]
      The random variable $X$ is called a \textbf{Bernoulli random variable} with parameter $p\in]0,1[$.
    \end{mydef}

    \subsection{Binomial Random Variable}

    Suppose that $n$ independent trials, each of which results in a “success” with probability $p$ and in a “failure” with probability $1 - p$, are to be performed.
    
    If $X$ represents the number of successes that occur in the $n$ trials, then $X$ is said to be a \textbf{binomial random variable with parameters $(n, p)$}.

    %Put some more things about random variables

    \newpage

    \section{Continuous Random Variables}

    Now, our focus is random variables whose set of possible values is uncountable.

    \begin{mydef}[\textbf{Random Variables}]
      Let $X$ be a random variable. We say that $X$ is a \textbf{continuous random variable} if there exists a nonnegative function $f(x)$ defined for all real $x\in\bbm{R}$, having the property that for any set $B$ of real numbers:
      \begin{equation*}
        P( X\in B)=\int_{ B}f(x)\:dx
      \end{equation*}
      The function $f$ is called the \textbf{probability density function of the random variable $X$}.
    \end{mydef}

    This is, given a continuous random variable $X$, we can obtain the probability of $X\in B$ given its probability density function.

    \begin{obs}
      If $X$ is given as in the latter definition, then:
      \begin{equation*}
        \int_{ \mathbb{R}}f(x)\:dx=1
      \end{equation*}
    \end{obs}

    This means that all probability statements about $X$ can be answered in terms of $f$. For instance, if $a,b\in\bbm{R}$ are such that $a<b$, then:
    \begin{equation*}
      P(a\leq X\leq b)=\int_{ a}^b f(x)\:dx
    \end{equation*}

    \begin{obs}[\textbf{Zero Values of Probability Density Function}]
      If $A\subseteq\bbm{R}$ is a set of measure zero, then $P(X\in A)=0$.
    \end{obs}

    \begin{propo}
      Let $X$ be a continuous random variable with probability density function $f$. Then:
      \begin{equation*}
        \frac{dF}{da}(x)=f(x)
      \end{equation*}
      where $F$ is the cummulative distrubution of $X$.
    \end{propo}

    \begin{proof}
      The result is immediate, due to the fact that:
      \begin{equation*}
        F(a)=P(X\leq a)=\int_{-\infty}^{a}f(x)\:dx
      \end{equation*}
      Using the fundamental theorem of calculus, and the fact that $\lim_{ x\to-\infty}f(x)=0$, we find that:
      \begin{equation*}
        \frac{dF}{da}(x)=f(x)
      \end{equation*}
      for all $x\in\bbm{R}$.
    \end{proof}

    \begin{obs}
      A more intuitive interpretation of the densitive function is obtained using this equation:
      \begin{equation*}
        P\left(a-\frac{\varepsilon}{2}\leq X\leq a+\frac{\varepsilon}{2}\right)\approx \varepsilon f(a)
      \end{equation*}
    \end{obs}

    \subsection{Uniform Random Variable}

    \begin{mydef}[\textbf{Random Variable Uniformly Distrubuted}]
      A continuous random variable is said to be \textbf{uniformly distributed} over the interval $(0,1)$ if its probability density function is given by:
      \begin{equation*}
        f(x)=\left\{
          \begin{array}{lr}
          1, & 0<x<1 \\
          0, & \textup{otherwise} \\  
          \end{array}
        \right.
      \end{equation*}
    \end{mydef}

    In general, we say that $X$ is a uniform random variable on the interval $(\alpha.\beta)$ if its probability density function is given by:

    \begin{equation*}
      f(x)=\left\{
        \begin{array}{lr}
        \frac{1}{\beta-\alpha}, & \alpha<x<\beta \\
        0, & \textup{otherwise} \\  
        \end{array}
      \right.
    \end{equation*}

    \begin{obs}
      If $X$ is a continuous random variable uniformly distrubuted over $(\alpha,\beta)$, then:
      \begin{equation*}
        F(x)=\left\{
        \begin{array}{lr}
          0, & x\leq\alpha \\  
          \frac{x-\alpha}{\beta-\alpha}, & \alpha<x<\beta \\
          1, & \beta\leq x \\  
        \end{array}
      \right.,\quad\forall x\in\bbm{R}
      \end{equation*}
      Where $F$ is the cumulative distrubution function of $X$.
    \end{obs}
    
    \subsection{Exponential Random Variable}

    %TODO

    \subsection{Gamma Random Variable}

    %TODO

    \subsection{Normal Random Variable}

    %TODO

    \section{Expectation of a Random Variable}

    \subsection{Discrete Case}

    \begin{mydef}[Nombre]
      Let $X$ be a discrete random variable with a probability mass fucntion $p(x)$. Then, the \textbf{expected value} of $X$ is defined as:
      \begin{equation*}
        E[X]=\sum_{ x|p(x)>0}xp(x)
      \end{equation*}
    \end{mydef}

    In other words, the expected value of $X$ is a weighted average of the possible values that $X$ can take on, each value being weighted by the probability that $X$ assumes that value.

    \begin{exa}
      If $X$ is a random variable with probabilty mass function given by:
      \begin{equation*}
        p(1)=p(2)=\frac{1}{2}
      \end{equation*}
      then, the expected value of $X$ is:
      \begin{equation*}
        E[X]=1\times\frac{1}{2}+2\times\frac{1}{2}=\frac{3}{2}
      \end{equation*}
    \end{exa}

    \begin{exa}[\textbf{Expected Value of Bernoulli Random Variable}]
      Let $X$ be a Bernoulli random variable with parameter $p$. Then, its expected value of $X$ is given by:
      \begin{equation*}
        E[X]=0(1-p)+1(p)=p
      \end{equation*}
    \end{exa}

    \begin{exa}[\textbf{Expected Value of Binomial Random Variable}]
      Let $X$ be a binomial random variable with parameters $n$ and $p$. Then, its expected value is:
      \begin{equation*}
        \begin{aligned}
          E[X]
          &= \sum_{i=0}^{n} i \binom{n}{i} p^i (1-p)^{n-i} \\
          &= \sum_{i=1}^{n} i \binom{n}{i} p^i (1-p)^{n-i} \\
          &= \sum_{i=1}^{n} n \binom{n-1}{i-1} p^i (1-p)^{n-i} \\
          &= np \sum_{k=0}^{n-1} \binom{n-1}{k} p^k (1-p)^{n-1-k} \\
          &= np\left(p+(1-p)\right)^{ n-1} \\
          &= np.
        \end{aligned}
      \end{equation*}
      Thus, the expected number of successes in $n$ independent trials equals $n$ times the probability of success in a single trial.
    \end{exa}

    \begin{exa}[\textbf{Expected Value of Geometric Random Variable}]
      Let $X$ be a geometric random variable with parameter $p$. Its expected value is given by:
      \begin{equation*}
        \begin{aligned}
          E[X]
          &= \sum_{n=1}^{\infty} n p (1-p)^{n-1} \\
          &= p \sum_{n=1}^{\infty} n q^{\,n-1}, \quad \text{where } q = 1-p.
        \end{aligned}
      \end{equation*}
      Using the identity $\sum_{n=1}^{\infty} n q^{n-1} = \frac{1}{(1-q)^2}$, we obtain
      \begin{equation*}
        E[X] = \frac{1}{p}.
      \end{equation*}
      Hence, the expected number of trials until the first success is the reciprocal of the success probability.
    \end{exa}

    \begin{exa}[\textbf{Expected Value of Poisson Random Variable}]
      Let $X$ be a Poisson random variable with parameter $\lambda$. Then, its expected value is:
      \begin{equation*}
        \begin{aligned}
          E[X]
          &= \sum_{i=0}^{\infty} i e^{-\lambda} \frac{\lambda^i}{i!} \\
          &= \sum_{i=1}^{\infty} e^{-\lambda} \frac{\lambda^i}{(i-1)!} \\
          &= \lambda e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda^k}{k!} \\
          &= \lambda.
        \end{aligned}
      \end{equation*}
    \end{exa}

    \subsection{Continuous Case}

    

    \subsection{Expectation of a Function of a Random Variable}

    \chapter*{Bibliography}

    \begin{itemize}
      \item Ross, S. M. (2014). \textit{Introduction to Probability Models}. Academic Press.
    \end{itemize}

    \newpage

\end{document}