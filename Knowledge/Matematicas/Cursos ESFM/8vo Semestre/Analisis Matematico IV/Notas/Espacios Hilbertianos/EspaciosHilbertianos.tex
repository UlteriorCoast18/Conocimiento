\documentclass[12pt]{report}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{array}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[a4paper, margin = 1.5cm]{geometry}

%En esta parte se hacen redefiniciones de algunos comandos para que resulte agradable el verlos%

\renewcommand{\theenumii}{\roman{enumii}}

\def\proof{\paragraph{Demostración:\\}}
\def\endproof{\hfill$\blacksquare$}

\def\sol{\paragraph{Solución:\\}}
\def\endsol{\hfill$\square$}

%En esta parte se definen los comandos a usar dentro del documento para enlistar%

\newtheoremstyle{largebreak}
  {}% use the default space above
  {}% use the default space below
  {\normalfont}% body font
  {}% indent (0pt)
  {\bfseries}% header font
  {}% punctuation
  {\newline}% break after header
  {}% header spec

\theoremstyle{largebreak}

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!60!red!30
]{exa}{Ejemplo}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!50!blue!30
]{obs}{Observación}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{theor}{Teorema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{propo}{Proposición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{cor}{Corolario}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{lema}{Lema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    roundcorner=5pt,
    backgroundcolor = gray!30,
    hidealllines = true
]{mydef}{Definición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    roundcorner=5pt
]{excer}{Ejercicio}[section]

%En esta parte se colocan comandos que definen la forma en la que se van a escribir ciertas funciones%

\newcommand\divides{\ensuremath{\bigm|}}
\newcommand\cf[3]{\ensuremath{#1:#2\rightarrow#3}}
\newcommand\contradiction{\ensuremath{\#_c}}
\newcommand\abs[1]{\ensuremath{\big|#1\big|}}
\newcommand\norm[1]{\ensuremath{\|#1\|}}
\newcommand\ora[1]{\ensuremath{\vec{#1}}}
\newcommand\pint[2]{\ensuremath{\left(#1\big| #2\right)}}
\newcommand\conj[1]{\ensuremath{\overline{#1}}}
\newcommand{\N}[2]{\ensuremath{\mathcal{N}_{#1}\left(#2\right)}}

%recuerda usar \clearpage para hacer un salto de página

\begin{document}
    \setlength{\parskip}{5pt} % Añade 5 puntos de espacio entre párrafos
    \setlength{\parindent}{12pt} % Pone la sangría como me gusta
    \title{Espacios Hilbertianos}
    \author{Cristo Daniel Alvarado}
    \maketitle

    \tableofcontents %Con este comando se genera el índice general del libro%

    \chapter{Espacios Hilbertianos}
    
    %apostol de análisis matemático, lang de análisis real

    \section{Conceptos básicos. Proyecciones ortogonales}

    \begin{mydef}
        Sea $H$ un espacio vectorial sobre el campo $\mathbb{K}$. Decimos que $H$ es un \textbf{espacio prehilbertiano} si está dotado de una aplicación $(\ora{x},\ora{y})\mapsto \pint{\ora{x}}{\ora{y}}$ con las propiedades siguientes:
        \begin{enumerate}
            \item $\forall \ora{y}\in H$ fijo, $\ora{x}\mapsto\pint{\ora{x}}{\ora{y}}$ es una aplicación lineal de $H$ en $\mathbb{K}$, o sea
            \begin{equation*}
                \begin{split}
                    \pint{\ora{x_1}+\ora{x_2}}{\ora{y}}=&\pint{\ora{x_1}}{\ora{y}}+\pint{\ora{x_2}}{\ora{y}}\\
                    \pint{\alpha\ora{x}}{\ora{y}}=&\alpha\cdot\pint{\ora{x}}{\ora{y}}\\
                \end{split}
            \end{equation*}
            para todo $\ora{x},\ora{x_1},\ora{x_2}\in H$ y $\alpha\in\mathbb{K}$.
            \item $(\ora{y}\big| \ora{x})=\conj{\pint{\ora{x}}{\ora{y}}}$, para todo $\ora{x}\in H$.
            \item $\pint{\ora{x}}{\ora{x}}\geq0$, para todo $\ora{x}\in H$.
            \item $\pint{\ora{x}}{\ora{x}}=0$ si y sólo si $\ora{x}=0$.
        \end{enumerate}
    \end{mydef}

    \begin{obs}
        Si $\mathbb{K}=\mathbb{R}$, entonces 1) y 2) implican que $\forall \ora{x}\in H$ fijo, la aplicación $\ora{y}\mapsto\pint{\ora{x}}{\ora{y}}$ de $H$ en $\mathbb{R}$ es lineal. En este caso se dice que $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ es una \textbf{forma bilineal sobre $H$}.

        Si $\mathbb{K}=\mathbb{C}$, entonces
        \begin{equation*}
            \begin{split}
                \pint{\ora{x}}{\ora{y_1}+\ora{y_2}}=&\pint{\ora{x}}{\ora{y_1}}+\pint{\ora{x}}{\ora{y_2}}\\
                \pint{\ora{x}}{\alpha\ora{y}}=&\conj{\alpha}\pint{\ora{x}}{\ora{y}}\\
            \end{split}
        \end{equation*}
        Se dice que $\ora{y}\mapsto\pint{\ora{x}}{\ora{y}}$ es entonces \textbf{semilineal} y que $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ es \textbf{sesquilineal} ($1\frac{1}{2}$-lineal).

        La aplicación $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ se llama \textbf{producto escalar sobre $H$}.
    \end{obs}

    \begin{mydef}
        Para todo $\ora{x}\in H$ se define la \textbf{norma de $\ora{x}$} como: $\norm{\ora{x}}=\sqrt{\pint{\ora{x}}{\ora{x}}}$.
    \end{mydef}

    \begin{exa}
        Sea $H=\mathbb{K}^n$
        %El producto interior usual en ese espacio ya conocido y es prehilbertiano
    \end{exa}

    \begin{exa}
        Sea $S\subseteq\mathbb{R}^n$ medible y sea $H=L_2(S,\mathbb{K})$. Para todo $f,g\in H$ se define
        \begin{equation*}
            \pint{f}{g}=\int_Sf\conj{g}
        \end{equation*}
        La integral existe por Hölder con $p=p^*=2$. Este es un producto escalar sobre $H$ y, en este caso:
        \begin{equation*}
            \norm{f}=\left[\int_S\big|f\big|^2 \right]^{\frac{1}{2}}=\mathcal{N}_2(f),\quad \forall f\in H
        \end{equation*}
    \end{exa}

    \begin{exa}
        Sea $H=l_2(\mathbb{K})$ el espacio de sucesiones en $\mathbb{K}$ que son cuadrado sumables. Se sabe que $\ora{x}=(x_1,x_2,...)\in l_2(\mathbb{K})$ si y sólo si
        \begin{equation*}
            \sum_{i=1}^{\infty}|x_i|^2<\infty
        \end{equation*}
        $l_2(\mathbb{K})$ es un espacio prehilbertiano con el producto escalar:
        \begin{equation*}
            \pint{\ora{x}}{\ora{y}}=\sum_{i=1}^{\infty}x_i\conj{y_i}
        \end{equation*}
        donde la serie es convergente por Hölder. En este caso:
        \begin{equation*}
            \norm{\ora{x}}=\left[\sum_{i=1}^{\infty}\abs{x_i}^2\right]^{\frac{1}{2}}=\mathcal{N}_2(\ora{x}),\quad\forall\ora{x}\in l_2(\mathbb{K})
        \end{equation*}
    \end{exa}

    \begin{theor}[Desigualdad de Cauchy-Schwartz]
        Sea $H$ un espacio prehilbertiano. Entonces:
        \begin{enumerate}
            \item Se cumple la desigualdad de Cauchy-Schwartz:
            \begin{equation*}
                \abs{\pint{\vec{x}}{\vec{y}}}\leq \norm{\vec{x}}\norm{\vec{y}}, \quad\forall\vec{x},\vec{y}\in H
            \end{equation*}
            y, la igualdad se da si y sólo si los vectores son linealmente dependientes.
            \item Se cumple la desigualdad triangular:
            \begin{equation*}
                \norm{\vec{x}+\vec{y}}\leq\norm{\vec{x}}+\norm{\vec{y}},\quad\forall\vec{x},\vec{y}\in H
            \end{equation*}
            y la igualdad se da si y sólo si uno de los vectores es múltiplo no negativo del otro.
        \end{enumerate}
    \end{theor}

    \begin{proof}
        De 1): Se supondrá que $\mathbb{K}=\mathbb{C}$ (el caso en que sea $\mathbb{R}$ es similar y se deja como ejercicio).

        Sean $\vec{x},\vec{y}\in H$. En el caso de que alguno de los vectores sea $\vec{0}$, el resultado es inmediato (ambos miembros de la desigualdad son cero). Por lo cual, supongamos que ambos son no cero. Se tiene para todo $\lambda\in\mathbb{K}$ que
        \begin{equation}
            \begin{split}
                0\leq& \pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}\\
                =& \pint{\vec{x}}{\vec{x}}+\conj{\lambda}\pint{\vec{x}}{\vec{y}}+\lambda\pint{\vec{y}}{\vec{x}}+\lambda\conj{\lambda}\pint{\vec{y}}{\vec{y}}\\
                =& \pint{\vec{x}}{\vec{x}}+\conj{\lambda\pint{\vec{y}}{\vec{x}}}+\lambda\pint{\vec{y}}{\vec{x}}+\lambda\conj{\lambda}\pint{\vec{y}}{\vec{y}}\\
                &= \norm{\vec{x}}^2+2\Re{\lambda\pint{\vec{y}}{\vec{x}}}+\abs{\lambda}^2\norm{\vec{y}}^2\\
            \end{split}
        \end{equation}
        En particular, para
        \begin{equation*}
            \lambda(t)=\left\{\begin{array}{lcr}
                    t\frac{\pint{\vec{x}}{\vec{y}}}{\abs{\pint{\vec{x}}{\vec{y}}}} & \textup{si} & \pint{\vec{x}}{\vec{y}}\neq0\\
                    t & \textup{si} & \pint{\vec{x}}{\vec{y}}=0
                \end{array}
            \right.
        \end{equation*}
        con $t\in\mathbb{R}$, la desigualdad (1) se convierte en
        \begin{equation}
            0\leq\norm{\vec{x}}^2+2t\abs{\pint{\vec{y}}{\vec{x}}}+t^2\norm{\vec{y}}^2
        \end{equation}
        El trinomio anterior es mayor o igual a cero si y sólo si su discriminante:
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}^2-\norm{\vec{x}}^2\norm{\vec{y}}^2\leq0
        \end{equation*}
        es decir
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}\leq\norm{\vec{x}}\norm{\vec{y}}
        \end{equation*}
        Si $\abs{\pint{\vec{x}}{\vec{y}}}=\norm{\vec{x}}^2\norm{\vec{y}}^2$, entonces el trinomio en (2) tiene una raíz doble. Luego, existe $\lambda\in\mathbb{C}$ tal que
        \begin{equation*}
            \pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}=0
        \end{equation*}
        pero lo anterior solo sucede si y sólo si $\vec{x}+\lambda\vec{y}=0$, es decir si $\vec{x}$ y $\vec{y}$ son linealmente dependientes.

        De 2): Se tiene lo siguiente:
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}+\vec{y}}^2=&\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}\\
                =&\norm{\vec{x}}+2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}}^2\\
                \leq&\norm{\vec{x}}+2\abs{\pint{\vec{y}}{\vec{x}}}+\norm{\vec{y}}^2\\
                \leq &\norm{\vec{x}}+2\norm{\vec{x}}\norm{\vec{y}}+\norm{\vec{y}}^2\\
                =& (\norm{\vec{x}}+\norm{\vec{y}})^2\\
            \end{split}
        \end{equation*}
        lo cual implica la desigualdad que se quiere probar. Ahora, la igualdad se cumple si y sólo si
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}=\Re\pint{\vec{x}}{\vec{y}}\textup{ y }\abs{\pint{\vec{x}}{\vec{y}}}=\norm{\vec{x}}\norm{\vec{y}}
        \end{equation*}
        la primera igualdad implica que $\pint{\vec{x}}{\vec{y}}$ es real (en particular, $\geq0$ por el valor absoluto) y la segunda implica que $\vec{x}$ y $\vec{y}$ son linealmente dependientes. Es decir, si y sólo si un vector es multiplo no negativo del otro.
    \end{proof}

    Se concluye del teorema anterior que $\norm{\cdot}$ es una norma sobre $H$. En lo sucesivo se consdierará a $H$ como espacio normado dotado de esta norma.

    \begin{propo}
        La aplicación $(\vec{x},\vec{y})\mapsto\pint{\vec{x}}{\vec{y}}$
        es una función continua del espacio normado producto $H\times H$ en $\mathbb{K}$.
    \end{propo}

    \begin{proof}
        Sean $\vec{x},\vec{y}\in H$ y, $\left\{\vec{x_n} \right\}_{n=1}^\infty$ y $\left\{\vec{y_n} \right\}_{n=1}^\infty$ dos sucesiones que convergen a $\vec{x}$ y $\vec{y}$, respectivamente. Se probará que $\left\{\pint{\vec{x_n}}{\vec{y_n}} \right\}_{n=1}^\infty$ converge a $\pint{\vec{x}}{\vec{y}}$ en $\mathbb{K}$.
        Se tiene que
        \begin{equation}
            \begin{split}
                \abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}
                \leq&\abs{\pint{\vec{x}-\vec{x_n}}{\vec{y}}}+\abs{\pint{\vec{x_n}}{\vec{y}-\vec{y_n}}}\\
                \leq&\norm{\vec{x}-\vec{x_n}}\norm{\vec{y}}+\norm{\vec{x_n}}\norm{\vec{y}-\vec{y_n}}\\
            \end{split}
        \end{equation}
        para todo $n\in\mathbb{N}$. Como $\left\{\vec{x_n} \right\}$ es convergente, es acotada. Luego existe $M>0$ tal que
        \begin{equation*}
            \norm{\vec{x_n}}\leq M,\quad\forall n\in\mathbb{N}
        \end{equation*}
        Se sigue de (3) que
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}
            \leq\norm{\vec{x}-\vec{x_n}}\norm{\vec{y}}+M\norm{\vec{y}-\vec{y_n}}
        \end{equation*}
        y, por ende
        \begin{equation*}
            \lim_{n\rightarrow\infty}\abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}=0
        \end{equation*}
        con lo que se tiene el resultado.
    \end{proof}

    \begin{mydef}
        Decimos que un espacio prehilbertiano se llama \textbf{Hilbertiano}, si la norma $\norm{\cdot}$ hace de él un espacio normado completo (o sea, un espacio normado de Banach).
    \end{mydef}

    \begin{exa}
        Los espacios $L_2(S,\mathbb{K})$, $l_2(\mathbb{K})$ y todo espacio prehilbertiano de dimensión finita ($\mathbb{K}^n$) son hilbretianos (ya que, todo espacio prehilbertiano de dimensión finita es isomorfo a $\mathbb{R}^k$, para algún $k\in\mathbb{N}$).
    \end{exa}

    De ahora en adelante, $H$ denotará siempre a un espacio prehilbertiano (a menos que se indique lo contrario).

    \begin{mydef}
        Sean $\vec{x},\vec{y}\in H$. Se dice que \textbf{$\vec{x}$ y $\vec{y}$ son ortogonales} y se escribe $\vec{x}\perp\vec{y}$, si $\pint{\vec{x}}{\vec{y}}=0$.
    \end{mydef}

    \begin{obs}
        La condición $\vec{x}\perp\vec{y}$ para todo $\vec{x}\in H$ implica que $\vec{y}=\vec{0}$, pues en particular $\pint{\vec{y}}{\vec{y}}=0\Rightarrow\vec{y}=\vec{0}$.
    \end{obs}

    \begin{theor}[Teorema de Pitágoras]
        Si $\left(\vec{x_1},\dots,\vec{x_n} \right)$ es un sistema de vectores ortogonales (a pares), entonces
        \begin{equation*}
            \norm{\vec{x_1}+\dots+\vec{x_n}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2
        \end{equation*}
    \end{theor}

    \begin{proof}
        Se procederá por inducción sobre $n$. Veamos el caso $n=2$. En este caso, veamos que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x_1}+\vec{x_2}}^2=&\pint{\vec{x_1}+\vec{x_2}}{\vec{x_1}+\vec{x_2}}\\
                =&\norm{\vec{x_1}}^2+\pint{\vec{x_1}}{\vec{x_2}}+\pint{\vec{x_2}}{\vec{x_1}}+\norm{\vec{x_2}}^2\\
                =&\norm{\vec{x_1}}^2+\norm{\vec{x_2}}^2\\
            \end{split}
        \end{equation*}
        Suponga que el resultado se cumple para $n\geq2$. Sea $\vec{x_1},...\vec{x_{n+1}}\in H$ un sistema de vectores ortogonales. Observemos que
        \begin{equation*}
            \begin{split}
                \pint{\vec{x_1}+\cdots+\vec{x_n}}{\vec{x_{n+1}}}&=\pint{\vec{x_1}}{\vec{x_{n+1}}}+\cdots+\pint{\vec{x_n}}{\vec{x_{n+1}}}\\
                &=0+\cdots+0\\
                &=0\\
            \end{split}
        \end{equation*}
        por lo cual, $x_{n+1}\perp\vec{x_1}+\cdots+\vec{x_n}$. Por el caso $n=2$ se sigue que:
        \begin{equation*}
            \norm{\vec{x_1}+\cdots+\vec{x_{n+1}}}^2=\norm{\vec{x_1}+\cdots+\vec{x_n}}^2+\norm{\vec{x_{n+1}}}^2
        \end{equation*}
        Pero, por hipótesis de inducción:
        \begin{equation*}
            \norm{\vec{x_1}+\dots+\vec{x_n}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2
        \end{equation*}
        Por lo cual:
        \begin{equation*}
            \norm{\vec{x_1}+\cdots+\vec{x_{n+1}}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2+\norm{\vec{x_{n+1}}}^2
        \end{equation*}
        Aplicando inducción se sigue el resultado.
    \end{proof}

    \begin{propo}[Identidad del paralelogramo]
        Para todo $\vec{x},\vec{y}\in H$ se cumple la identidad del paralelogramo:
        \begin{equation*}
            \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2=2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)
        \end{equation*}
    \end{propo}

    \begin{proof}
        Sean $\vec{x},\vec{y}\in H$. Veamos que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2
                =&\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}+\pint{\vec{x}-\vec{y}}{\vec{x}-\vec{y}}\\
                =&\norm{\vec{x}}^2+2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}^2}+\norm{\vec{x}}^2-2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}}^2\\
                =&2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)\\
            \end{split}
        \end{equation*}
    \end{proof}

    Este resultado anterior es importante, pues en espacios donde la norma no venga de un producto escalar, no necesariamente se cumple la igualdad.

    \begin{exa}
        Los vectores $\chi_{[0,1]}$ y $\chi_{[1,2]}$ son ortogonales en $L_2(\mathbb{R},\mathbb{R})$ (es inmediato del producto escalar en $L_2(\mathbb{R},\mathbb{R})$).
    \end{exa}

    \begin{exa}
        Los vectores $\sen$ y $\cos$ son ortogonales en $L_2([-\pi,\pi[,\mathbb{R})$. En efecto, veamos que
        \begin{equation*}
            \pint{\sen}{\cos}=\int_{-\pi}^{\pi}\sen x\cos xdx=\frac{1}{2}\int_{-\pi}^{\pi}\sen 2xdx=0
        \end{equation*}
        En particular, por Pitágoras se tiene que
        \begin{equation*}
            \int_{-\pi}^{\pi}\abs{\sen x+\cos x}^2dx=\int_{-\pi}^{\pi}\abs{\sen x}^2dx+\int_{-\pi}^{\pi}\abs{\cos x}^2dx
        \end{equation*}
    \end{exa}

    \begin{exa}
        Si $\vec{x}=(1,1,\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{3}{3},...)$ y $\vec{x}=(1,-1,\frac{1}{2},-\frac{1}{2},\frac{1}{2},-\frac{3}{3},)$ son elementos de $l_2(\mathbb{R})$, se tiene que $\vec{x}\perp\vec{y}$. En efecto, veamos que
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{y}}=&\sum_{n=1}^{\infty}x_n\conj{y_n}\\
                =&\lim_{n\rightarrow\infty}s_n
            \end{split}
        \end{equation*}
        donde $\left\{s_n\right\}_{n=1}^\infty$ es la sucesión de sumas parciales, siendo $s_{2m}=0$ y $s_{2m-1}=\frac{1}{m}$. Por lo cual
        \begin{equation*}
            \pint{\vec{x}}{\vec{y}}=\lim_{n\rightarrow\infty}s_n=0
        \end{equation*}
    \end{exa}

    \begin{theor}
        Sea $M$ un subespacio de un espaco prehilbertiano $H$ y sea $\vec{x}\in H$.
        \begin{enumerate}
            \item Suponiendo que existe $\vec{x_0}\in M$ tal que $\vec{x}-\vec{x_0}\perp M$, es decir que $\vec{x}-\vec{x_0}\perp\vec{y}$, para todo $\vec{y}\in M$, se tiene
            \begin{equation*}
                \norm{\vec{x}-\vec{x_0}}<\norm{\vec{x}-\vec{y}},\quad\forall\vec{y}\in M,\vec{y}\neq\vec{x_0}
            \end{equation*}
            Así pues, si existe $\vec{x_0}$, tal vector es único y es llamado \textbf{la proyección ortogonal de $\vec{x}$ sobre $M$}. Además
            \begin{equation*}
                d(\vec{x},M)^2=\norm{\vec{x}-\vec{x_0}}^2=\norm{\vec{x}}^2-\norm{\vec{x_0}}^2
            \end{equation*}
            \item Recíprocamente, si existe un $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$, entonces $\vec{x_0}$ es la proyección ortogonal de $\vec{x}$ sobre $M$. En particular, si $\vec{x}\in M$ entonces $\vec{x}=\vec{x_0}$, es decir que $\vec{x}$ es su propia proyección ortogonal sobre $M$.
        \end{enumerate}
    \end{theor}

    \begin{proof}
        De 1): Suponga que existe $\vec{x_0}\in M$ con la condición especificada. Sea $\vec{y}\in M$ distinto de $\vec{x_0}$. Como $\vec{x_0}-\vec{x}\perp\vec{x_0}-\vec{y}$, por el Teorema de Pitágoras se tiene que
        \begin{equation}
            \norm{\vec{x}-\vec{y}}^2=\norm{\vec{x}-\vec{x_0}}^2+\vec{x_0}-\vec{y}^2>\norm{\vec{x}-\vec{x_0}}^2
        \end{equation}
        pues $\vec{x_0}\neq \vec{y}$. Así pues, $\vec{x_0}$ es único. Además $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Aplicando la ecuación 4) con $\vec{y}=\vec{0}$ se tiene que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}}^2=&\norm{\vec{x}-\vec{x_0}}^2+\norm{\vec{x_0}}^2\\
                \Rightarrow d(\vec{x},M)^2=\norm{\vec{x}-\vec{x_0}}^2=&\norm{\vec{x}}^2-\norm{\vec{x_0}}^2\\
            \end{split}
        \end{equation*}
        
        De 2) Si existe $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$, entonces $\vec{x_0}$ debe ser la proyección ortogonal de $\vec{x}$ sobre $M$. En efecto, para todo $\vec{y}\in M$ y para todo $\lambda\in\mathbb{K}$ se tiene
        \begin{equation}
            \begin{split}
                \norm{\vec{x}-(\vec{x_0}+\lambda\vec{y})}^2\geq&\norm{\vec{x}-\vec{x_0}}^2\\
                \Rightarrow \norm{(\vec{x}-\vec{x_0})-\lambda\vec{y}}^2\geq&\norm{\vec{x}-\vec{x_0}}^2\\
                \Rightarrow \norm{\vec{x}-\vec{x_0}}^2+2\Re[\conj{\lambda} \pint{\vec{x}-\vec{x_0}}{\vec{y}}]+\abs{\lambda}^2\norm{\vec{y}}^2\geq&\norm{\vec{x}-\vec{x_0}}^2\\
                \Rightarrow -2\Re[\lambda\pint{\vec{x}-\vec{x_0}}{\vec{y}}]+\abs{\lambda}^2\norm{\vec{y}}^2\geq& 0\\
            \end{split}
        \end{equation}
        en particular, para $\lambda=t\pint{\vec{x}-\vec{x_0}}{\vec{y}}$, con $t\in\mathbb{R}$, la ecuación anterior se transforma en:
        \begin{equation*}
            \begin{split}
                \abs{\pint{\vec{x}-\vec{x_0}}{\vec{y}}}^2\left[-2t+t^2\norm{\vec{y}}\right]\geq0 \\
            \end{split}
        \end{equation*}
        para todo $t\in\mathbb{R}$. Esto exige que $\pint{\vec{x}-\vec{x_0}}{\vec{y}}=0$, o sea que $\vec{x}-\vec{x_0}\perp \vec{y}$.
    \end{proof}

    Dado un subespacio $M$ de un espacio prehilbertiano $H$ un vector $\vec{x}\in H$, puede no existir la proyección ortogonal de $\vec{x}$ sobre $M$. Esto motiva la siguiente definición:

    \begin{mydef}
        Un subespacio $M$ de $H$ se dice que es \textbf{distinguido} si para cada $\vec{x}\in H$ existe la proyección ortogonal de $\vec{x}$ sobre $M$.        
    \end{mydef}

    \begin{exa}
        El subespacio $\phi_0$ de las sucesiones eventualmente constantes de valor cero es un subespacio del espacio hilbretiano $l_2(\mathbb{R})$. Sea $M$ el subespacio de $\phi_0$ dado como sigue:
        \begin{equation*}
            M=\left\{\vec{x}\in\phi_0|x_2=0 \right\}
        \end{equation*}
        Sea $\vec{x}=\left(0,\frac{1}{2^{0/2}},\frac{1}{2^{1/2}},\frac{1}{2},\frac{1}{2^{3/2}},... \right)$. Se tiene que:
        \begin{equation*}
            \begin{split}
                d(\vec{x},M)&=\inf_{\vec{y}\in M}\left\{\norm{\vec{x}-\vec{y}} \right\}\\
                &=\inf_{\vec{y}\in M}\left\{\left[\abs{y_1}+\sum_{i=2}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 \right]^{1/2} \right\}\\
                &=1\\
                &=\inf_{\vec{y}\in M}\left\{\left[\abs{y_1}+1+\sum_{i=3}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 \right]^{1/2} \right\}\\
                &=1\\
            \end{split}
        \end{equation*}
        (pues, $y_2=0$). Pero $\norm{\vec{x}-\vec{y}}>1$, para todo $\vec{y}\in M$. En efecto, sea $\vec{y}\in M$, entonces $\exists m\in\mathbb{N}$ tal que si $k\geq m$ se tiene que $y_k=0=y_2$. Veamos que

        \begin{equation*}
            \begin{split}
                \norm{\vec{x}-\vec{y}}&=\left[\abs{y_1}+1+\sum_{i=3}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 \right]^{1/2}\\
                &\geq \left[1+\sum_{i=3}^{k-1} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 + \sum_{i=k}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 \right]^{1/2}\\
                &= \left[1+\sum_{i=3}^{k-1} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 + \sum_{i=k}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}}^2 \right]^{1/2}\\
                &\geq \left[1+\sum_{i=k}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}}^2 \right]^{1/2}\\
                &> \left[1\right]^{1/2}\\
                &> 1\\
            \end{split}
        \end{equation*}
        
        Luego no existe $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Por lo tanto, no existe la proyección ortogonal de $\vec{x}$ sobre $M$ (es decir, $M$ no es distinguido).

        Sin embargo, si $\vec{x}=(1,1,0,...)\in l_2(\mathbb{R})$, entonces si existe la proyección ortogonal de $\vec{x}$ sobre $M$, pues
        \begin{equation*}
            \begin{split}
                d(\vec{x},M)&=\inf_{\vec{y}\in M}\left\{\norm{\vec{x}-\vec{y}} \right\}\\
                &=\inf_{\vec{y}\in M}\left\{\left[\abs{1-y_1}^2+1^2+\sum_{i=3}^{\infty} \abs{y_i}^2 \right]^{1/2} \right\}\\
                &=1\\
            \end{split}
        \end{equation*}
        y $\norm{\vec{x}-\vec{e_1}}=1$, donde $\vec{e_1}\in M$. Por tanto, $\vec{e_1}$ es la proyección ortogonal de $\vec{x}$ sobre $M$.
    \end{exa}

    \begin{theor}
        Si $M$ es un subespacio completo de un espacio prehilbertiano, entonces $M$ es distinguido. En particular todo subespacio de dimensión finita de un espacio prehilbertiano siempre es distinguido. 
    \end{theor}

    \begin{proof}
        Sea $\vec{x}\in H$. Se debe probar que existe un $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Sea $a=d(\vec{x},M)$. Por propiedades del ínfimo existe una sucesión $\left\{\vec{y_\nu} \right\}_{\nu=1}^{\infty}$ tal que
        \begin{equation}
            \lim_{\nu\rightarrow\infty}\norm{\vec{x}-\vec{y_\nu}}=a
        \end{equation}
        Sean $\nu,\mu\in\mathbb{N}$ arbitrarios. Por la identidad del paralelogramo se tiene que
        \begin{equation*}
            \begin{split}
                2\left(\norm{\vec{x}-\vec{y_\nu}}^2+\norm{\vec{x}-\vec{y_\mu}}^2 \right)
                &=\norm{\vec{y_\nu}-\vec{y_\mu}}^2+\norm{2\vec{x}-(\vec{y_\nu}+\vec{y_\mu})}^2\\
                &=\norm{\vec{y_\nu}-\vec{y_\mu}}^2+4\norm{\vec{x}-\frac{\vec{y_\nu}+\vec{y_\mu}}{2}}^2\\
                &\geq\norm{\vec{y_\nu}-\vec{y_\mu}}^2+4a^2\\
            \end{split}
        \end{equation*}
        de donde
        \begin{equation*}
            \norm{\vec{y_\nu}-\vec{y_\mu}}^2\leq2\left(\norm{\vec{x}-\vec{y_\nu}}^2+\norm{\vec{x}-\vec{y_\mu}}^2 \right)-4a^2
        \end{equation*}
        Tomando límite cuando $\nu,\mu$ tienden a infinito y por (6), se tiene que
        \begin{equation*}
            \lim_{\nu,\mu\rightarrow\infty}\norm{\vec{y_\nu}-\vec{y_\mu}}^2=0
        \end{equation*}
        por tanto, $\left\{\vec{y_\nu} \right\}_{\nu=1}^{\infty}$ es de Cauchy. Por ser $M$ completo, existe $\vec{x_0}\in M$ tal que $\lim_{\nu\rightarrow\infty}\vec{y_\nu}=\vec{x_0}$. Por (6):
        \begin{equation*}
            a=\lim_{\nu\rightarrow\infty}\norm{\vec{x}-\vec{y_\nu}}=\norm{\vec{x}-\vec{x_0}}
        \end{equation*}
    \end{proof}

    \begin{exa}
        ¿Es distinguido el subespacio de $L_2(\mathbb{R},\mathbb{R})$ dado por:
        \begin{equation*}
            M=\left\{f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})| f(x)=0\textup{ c.t.p. en }[1,2] \right\}
        \end{equation*}
        ?
        
        La respuesta es que sí, ya que $M$ es cerrado. En efecto, sea $\left\{f_\nu\right\}_{\nu=1}^{\infty}$ una sucesión en $M$ convergente en promedio cuadrático a una $f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})$, es decir:
        \begin{equation*}
            \lim_{\nu\rightarrow\infty}\mathcal{N}_2(f_\nu-f)=0
        \end{equation*}
        Se sabe que existe una subsucesión de $\left\{f_\nu\right\}_{\nu=1}^{\infty}$, digamos $\left\{f_{\alpha(\nu)}\right\}_{\nu=1}^{\infty}$ que converge c.t.p. a $f$ en $\mathbb{R}$. Como $f_{\alpha(\nu)}=0$ c.t.p. en $[1,2]$, entonces $f=0$ c.t.p. en $[1,2]$, es decir $f\in M$. Por tanto, $M$ es distinguido.

        Ahora, dada $f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})$, ¿Cuál será la proyección ortogonal de $f$ sobre $M$? Es claro que
        \begin{equation*}
            f_0=f\cdot\chi_{\mathbb{R}\backslash[1,2]}\in M
        \end{equation*}
        es la proyección ortogonal de $f$ sobre $M$, y además $f-f_0\perp M$.
    \end{exa}

    \begin{mydef}
        Sea $S\subseteq H$ un conjunto arbitrario. Para este conjunto se define
        \begin{equation*}
            S^{\perp}=\left\{\vec{x}\in H|\vec{x}\perp\vec{s},\forall\vec{s}\in S \right\}
        \end{equation*}
        Es claro que $S^\perp$ es un subespacio cerrado de $H$.
    \end{mydef}

    \begin{sol}
        En efecto, si $\left\{\vec{x_\nu} \right\}$ es una sucesión en $S^\perp$ que converge a $\vec{x}\in H$, entonces
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{s}}=\lim_{\nu\rightarrow\infty}\pint{\vec{x_\nu}}{\vec{y}}=0,\quad\forall\vec{s}\in S
            \end{split}
        \end{equation*}
        por continuidad y para todo $\vec{s}\in S$. Luego $\vec{x}\in S^\perp$. Otra forma es definiendo una función $\cf{T_{\vec{s}}}{H}{\mathbb{K}}$ como
        \begin{equation*}
            T_{\vec{s}}(\vec{x})=\pint{\vec{x}}{\vec{s}},\quad\forall\vec{x}\in H
        \end{equation*}
        Entonces
        \begin{equation*}
            S^\perp=\bigcap_{\vec{s}\in S}\ker T_{\vec{s}}
        \end{equation*}
        Como $T_{\vec{s}}$ es lineal continua para todo $\vec{s}\in S$, entonces se sigue que $S^\perp$ es cerrado.
    \end{sol}

    \begin{propo}
        Un subespacio $M$ de un espacio prehilbertiano $H$ es distinguido si y sólo si
        \begin{equation*}
            H=M\oplus M^\perp
        \end{equation*}
    \end{propo}

    \begin{proof}
        $\Rightarrow$): Suponga que $M$ es distinguido. Como $M\cap M^\perp=\left\{\vec{0} \right\}$, para probar que $H=M\oplus M^\perp$, basta probar que es la suma simplemente, es decir que $H=M+M^\perp$.
        
        Sea $\vec{x}\in H$, como $M$ es distinguido entonces existe $\vec{x_1}\in M$ tal que $\vec{x}-\vec{x_1}\perp M$, tomando $\vec{x_2}=\vec{x}-\vec{x_1}$ se tiene que $\vec{x_2}\in M^\perp$. Además $\vec{x}=\vec{x_1}+\vec{x_2}$, lo que prueba el resultado.

        $\Leftarrow$): Suponga que $H=M\oplus M^\perp$. Hay que probar que $M$ es distinguido. Sea $\vec{x}\in H$ arbitrario. Por hipótesis existen $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ únicos tales que $\vec{x}=\vec{x_1}+\vec{x_2}$. Se afirma que $\vec{x_1}$ es la proyección ortogonal de $\vec{x}$ sobre $M$. 
        
        En efecto,
        \begin{equation*}
            \vec{x}-\vec{x_1}=\vec{x_2}\in M^\perp
        \end{equation*}
        pero $\vec{x_2}\perp M$, por tanto $\vec{x_1}$ es la proyección ortogonal.
    \end{proof}

    \begin{exa}
        Sea $M=\left\{x\in l_2(\mathbb{R})\big| x(2n)=0, \forall n\in\mathbb{N} \right\}$. Afirmamos que $M$ es distinguido, para lo cual basta ver que este subespacio es cerrado (por ser $l_(\mathbb{R})$ completo, es decir por ser un espacio Hilbertiano).

        Sea $\left\{\vec{x_n} \right\}$ una sucesión en $l_2(\mathbb{R})$ que converge a $\vec{x}\in l_2(\mathbb{R})$, es decir
        \begin{equation}
            \begin{split}
                \lim_{n\rightarrow \infty}\mathcal{N}_2(\vec{x}-\vec{x_n})=&0\\
                \lim_{k\rightarrow \infty}(\vec{x}(2k)-\vec{x_n}(2k))=&0,\quad \forall k\in\mathbb{N}\\
                \Rightarrow \vec{x}(2k)=&0\quad\forall x\in\mathbb{N}\\ 
            \end{split}
        \end{equation}
        por lo cual, $\vec{x}\in M$. Luego, $M$ es cerrado. Dado que $M$ es distinguido, si $\vec{x}\in l_2(\mathbb{R})=M\oplus M^\perp$, se tiene
        \begin{equation*}
            \vec{x}=\vec{x_1}+\vec{x_2}
        \end{equation*}
        donde $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ son únciso y están dados por:
        \begin{equation*}
            \begin{split}
                \vec{x_1}=&(\vec{x}(1),0,\vec{x}(3),...)\\
                \vec{x_2}=&(0,\vec{x}(2),0,\vec{x}(4),...)\\
            \end{split}
        \end{equation*}
    \end{exa}

    \begin{cor}
        Si $M$ es un subespacio distinguido de $H$, entonces $M^\perp$ es también un subespacio distinguido.
    \end{cor}

    \begin{proof}
        Se probará que cualquier $\vec{x}\in H$ posee una proyección ortogonal sobre $M^\perp$. Por el teorema anterior:
        \begin{equation*}
            \vec{x}=\vec{x_1}+\vec{x_2}
        \end{equation*}
        con $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ únicos. Luego, $\vec{x}-\vec{x_2}=\vec{x_1}\in M$, por lo que cualquier vector en $M^\perp$ se cumple que $\vec{x_1}\perp \vec{y}$, para todo $\vec{y}\in M$, es decir que $\vec{x_2}$ es la proyecicón ortogonal de $\vec{x}$ sobre $M^\perp$.
    \end{proof}

    \begin{propo}
        Si $M$ es un subespacio distinguido de $H$, entonces $M^{\perp\perp}=M$.
    \end{propo}

    \begin{proof}
        Claramente $M\subseteq M^{\perp\perp}$. Ahora, sea $\vec{x}\in M^{\perp\perp}$, por el teorema $\vec{x}=\vec{x_1}+\vec{x_2}$ donde $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ únicos.

        Se tiene que
        \begin{equation*}
            0=\pint{\vec{x}}{\vec{x_2}}=\pint{\vec{x}}{\vec{x_1}}+\pint{\vec{x_2}}{\vec{x_2}}=\pint{\vec{x_2}}{\vec{x_2}}
        \end{equation*}
        es decir que $\vec{x_2}=\vec{0}$. Por tanto, $\vec{x}\in M$.

        Luego, $M=M^{\perp\perp}$.
    \end{proof}

    \begin{cor}
        En un espacio hilbertiano $H$, un subespacio es distinguido si y sólo si es cerrado.
    \end{cor}

    \begin{proof}
        Si es cerrado es inmediato que es distinguido. Ahora, si es distinguido entonces es cerrado, pues por el corolario anterior $M=M^{\perp\perp}$, donde $M^{\perp\perp}$ es cerrado por ser intersección arbitraria de cerrrados, luego $M$ es cerrado.
    \end{proof}

    \begin{propo}
        Sea $H$ un espacio prehilbertiano y sea $M$ un subespacio distinguido de $H$ (que no se reduce al $\left\{\vec{0} \right\}$). $\forall\vec{x}\in H$ sea $\pi(\vec{x})$ la \textbf{proyección ortogonal de $\vec{x}$ sobre $M$}.

        Entonces $\cf{\pi}{H}{M}$ es lineal continua y tal que $\norm{\pi}=1$. Además, $\pi\circ\pi=\pi$, y $\pint{\pi(\vec{x})}{\vec{y}}=\pint{\vec{x}}{\pi(\vec{y})}$.
    \end{propo}

    \begin{proof}
        Sea $\vec{x}\in H$ y $\alpha\in\mathbb{K}$. Si $\alpha=0$, el resultado es inmediato. Suponga que $\alpha\neq 0$. Se tiene que $\alpha\pi(\vec{x})\in M$ por ser subespacio, y
        \begin{equation*}
            \alpha\vec{x}-\alpha\pi(\vec{x})=\alpha(\vec{x}-\pi(\vec{x}))\perp M
        \end{equation*}
        Luego, $\alpha\pi(\vec{x})$ es una proyección ortogonal de $\alpha\vec{x}$ sobre $M$, pero por unicidad de la proyección ortogonal, se tiene que $\pi(\alpha\vec{x})=\alpha\pi(\vec{x})$.

        Ahora, sean $\vec{x},\vec{y}\in H$. Entonces, $\pi(\vec{x})+\pi(\vec{y})\in M$ y:
        \begin{equation*}
            (\vec{x}+\vec{y})-(\pi(\vec{x})+\pi(\vec{x}))=(\vec{x}-\pi(\vec{x}))+(\vec{y}-\pi(\vec{y}))\perp M
        \end{equation*}
        es decir que $\pi(\vec{x})+\pi(\vec{y})$ es una proyección ortogonal de $\vec{x}+\vec{y}$ sobre $M$. Por unicidad,
        \begin{equation*}
            \pi(\vec{x}+\vec{y})=\pi(\vec{x})+\pi(\vec{y})
        \end{equation*}
        Por tanto, $\pi$ es lineal.


        Ahora, veamos que es continua. Se sabe que:
        \begin{equation*}
            \begin{split}
                d(\vec{x},M)^2&=\norm{\vec{x}-\pi(\vec{x})}^2\\
                &=\norm{\vec{x}}^2-\norm{\pi(\vec{x})}^2\\
                \Rightarrow \norm{\pi(\vec{x})}^2&=\norm{\vec{x}}^2-\norm{\vec{x}-\pi(\vec{x})}^2\\
                &\leq\norm{\vec{x}}^2\\
            \end{split}
        \end{equation*}
        luego, $\pi$ es continua y, $\norm{\pi}\leq 1$.

        Sea ahora $\vec{x}\in M$, $\vec{x}\neq\vec{0}$. Entonces:
        \begin{equation*}
            \norm{\vec{x}}=\norm{\pi(\vec{x})}\leq\norm{\pi}\norm{\vec{x}}
        \end{equation*}
        por tanto, $\norm{\pi}\geq 1$, por lo anterior se sigue que $\norm{\pi}=1$.

        Ya se sabe que $\pi\circ\pi=\pi^2=\pi$ (por la proposición anterior).

        Sean $\vec{x},\vec{y}\in H$ arbitrarios. Entonces, $\pi(\vec{x})\in M$ y $\vec{y}-\pi(\vec{y})\perp M$, por lo cual
        \begin{equation*}
            \begin{split}
                0=&\pint{\pi(\vec{x})}{\vec{y}-\pi(\vec{y})} \\
                =&\pint{\pi(\vec{x})}{\vec{y}}-\pint{\pi(\vec{x})}{\pi(\vec{y})}\\
                \Rightarrow \pint{\pi(\vec{x})}{\vec{y}} =& \pint{\pi(\vec{x})}{\pi(\vec{y})}\\
            \end{split}
        \end{equation*}
        Intercambiando los papeles de $\vec{x}$ y $\vec{y}$ se obtiene que: $\pint{\pi(\vec{y})}{\vec{x}}=\pint{\pi(\vec{y})}{\pi(\vec{x})}$ o sea:
        \begin{equation*}
            \pint{\vec{x}}{\pi(\vec{y})}=\pint{\pi(\vec{x})}{\pi(\vec{y})}
        \end{equation*}
        por lo cual $\pint{\pi(\vec{x})}{\vec{y}}=\pint{\vec{x}}{\pi(\vec{y})}$.
    \end{proof}

    \begin{propo}
        Sea $H$ prehilbertiano. Suponga que $\pi$ es una aplicación lineal de $H$ en $H$ tal que
        \begin{itemize}
            \item $\pi^2=\pi$.
            \item $\pint{\pi(\vec{x})}{\vec{y}}=\pint{\vec{x}}{\pi(\vec{y})},\forall\vec{x},\vec{y}\in H$.
        \end{itemize}
        Entonces existe un único subespacio distinguido $M$ de $H$ tal que $\pi$ es la proyección ortogonal de $H$ sobre $M$.
    \end{propo}

    \begin{proof}
        Claramente, si $M$ existe debe ser $M=\pi(H)$, o sea:
        \begin{equation*}
            M=\pi(H)=\left\{\pi(\vec{x})\big| \vec{x}\in H \right\}
        \end{equation*}
        
        Se debe probar que si $\vec{x}\in H$ es arbitrario $\vec{x}-\pi(\vec{x})\perp M$, o sea
        \begin{equation*}
            \pint{\vec{x}-\pi(\vec{x})}{\pi(\vec{y})}=0,\quad\forall\vec{y}\in H
        \end{equation*}
        Sean $\vec{x},\vec{y}\in H$. Se tiene que:
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}-\pi(\vec{x})}{\pi(\vec{y})}=&\pint{\vec{x}}{\pi(\vec{y})}-\pint{\pi(\vec{x})}{\pi(\vec{y})} \\
                =&\pint{\vec{x}}{\pi(\vec{y})}-\pint{\vec{x}}{\pi(\vec{y})} \\
                =&0\\
            \end{split}
        \end{equation*}
        usando las dos propiedades de $\pi$. Por tanto, $\pi(\vec{x})$ es la proyección ortogonal de $\vec{x}$, es decir que $M$ es distinguido. La unicidad se sigue de la construcción de $M$.
    \end{proof}

    \section{Autodualidad de espacios hilbertianos}

    Si $E$ es un espacio normado, $E^*$ denota su \textbf{dual topológico} formado por todas las aplicaciones lineales continuas de $E$ en $\mathbb{K}$. Si $W\in E^*$, se define la $\norm{W}$ como
    \begin{equation*}
        \norm{W}=\inf\left\{a\in\mathbb{R}\big| \norm{W(\vec{x})}\leq a\norm{\vec{x}},\forall\vec{x} \right\}
    \end{equation*}
    
    Recuerde que $E^*$ es siempre un espaico de Banach aunque $E$ no lo sea.

    \begin{theor}[Teorema de Riesz]
        Sea $H$ un espacio hilbertiano (no reducido a $\left\{\vec{0} \right\}$). Para cada $\vec{y}\in H$ se define una aplicación $\cf{G_{\vec{y}}}{H}{\mathbb{K}}$ como sigue:
        \begin{equation*}
            G_{\vec{y}}(\vec{x})=\pint{\vec{x}}{\vec{y}},\forall\vec{x}\in H
        \end{equation*}
        Entonces, $G_{\vec{y}}$ es un funcional lineal continuo sobre $H$. Además, la aplicación $\cf{G}{H}{H^*}$ dada por:
        \begin{equation*}
            \vec{y}\mapsto G_{\vec{y}}
        \end{equation*}
         es una isometría semilineal de $H$ en $H^*$ que es suprayectiva.
    \end{theor}

    \begin{proof}
        Se probarán varias cosas:
        \begin{enumerate}
            \item Por propiedades del producto escalar, para cada $\vec{y}\in H$ la aplicación $\cf{G_{\vec{y}}}{H}{\mathbb{K}}$ es lineal. Dicha aplicación lineal es continua, pues
            \begin{equation*}
                \abs{G_{\vec{y}}(\vec{x})}=\abs{\pint{\vec{x}}{\vec{y}}}\leq\norm{\vec{x}}\norm{\vec{y}},\quad\forall\vec{x}\in H
            \end{equation*}
            (por Cauchy-Schwartz). Así que $G_{\vec{y}}\in H^*$. Además, $\norm{G_{\vec{y}}}\leq \norm{\vec{y}}$. Por otra, parte, si $\vec{y}\neq\vec{0}$, entonces
            \begin{equation*}
                G_{\vec{y}}(\vec{y})=\pint{\vec{y}}{\vec{y}}=\norm{\vec{y}}^2
            \end{equation*}
            pero, como el operador es continuo, se tiene que $\abs{G_{\vec{y}}(\vec{y})} \leq\norm{G_{\vec{y}}}\norm{\vec{y}}$. Por lo cual, $\norm{\vec{y}}\leq\norm{G_{\vec{y}}}$. Así pues, $\norm{G_{\vec{y}}}=\norm{\vec{y}}$.

            Si $\vec{y}=\vec{0}$, entonces $\norm{G_{\vec{y}}}=0=\norm{\vec{y}}$, pues $G_{\vec{y}}=0$.

            \item La aplicación $\cf{G}{H}{H^*}$, $\vec{y}\mapsto G_{\vec{y}}$ es semilineal, es decir que $G_{\alpha\vec{y}}=\conj{\alpha}G_{\vec{y}}$ y separa sumas. En efecto, sea $\vec{y}\in H$ y $\alpha\in\mathbb{K}$. Entonces:
            \begin{equation*}
                \begin{split}
                    G_{\alpha\vec{y}}(\vec{x})=&\pint{\vec{x}}{\alpha\vec{y}}\\
                    =&\conj{\alpha} \pint{\vec{x}}{\vec{y}}\\
                    =& G_{\vec{y}}(\vec{x}),\quad\forall\vec{x}\in H \\
                \end{split}
            \end{equation*}
            y además, para $\vec{z}\in H$ se tiene que
            \begin{equation*}
                \begin{split}
                    G_{\vec{y}+\vec{z}}(\vec{x})=&\pint{\vec{x}}{\vec{y}+\vec{z}} \\
                    =&\pint{\vec{x}}{\vec{y}}+\pint{\vec{x}}{\vec{z}} \\
                    =& G_{\vec{y}}(\vec{x})+G_{\vec{z}}(\vec{x}),\quad\forall\vec{x}\in H \\
                \end{split}
            \end{equation*}
            por tanto, $G$ es semilineal. Ahora, veamos que es isometría; sean $\vec{y_1},\vec{y_2}\in H$, entonces:
            \begin{equation*}
                \begin{split}
                    \norm{G_{\vec{y_1}}-G_{\vec{y_2}}}=&\norm{G_{\vec{y_1}+\vec{y_2}}} \\
                    =& \norm{\vec{y_1}+\vec{y_2}}\\
                \end{split}
            \end{equation*}
            así, esta función semilineal es isometría. Automáticamente $G$ es inyectiva. Note que $\vec{y}\in\left(\ker G_{\vec{y}} \right)^\perp$ y $G_{\vec{y}}(\vec{y})=\norm{\vec{y}}^2$.

            \item Se probará la suprayectividad. Sea $W\in H^*$ tal que $W\neq 0$ (en caso contrario basta tomar $\vec{y}=\vec{0}$) se debe probar que existe $\vec{y}\in H$ tal que $W=G_{\vec{y}}$. 
            
            Por la parte (2), tal $\vec{y}$ debe cumplir que $\vec{y}\in \left(\ker W\right)^\perp$ y $W(\vec{y})=\norm{\vec{y}}^2$. Como $\ker W $ es un subespacio cerrado de $H$ y $H$ es hilbertiano, entonces $\ker W$ es distinguido. Luego:
            \begin{equation*}
                H=\ker W \oplus\left(\ker W\right)^\perp
            \end{equation*}
            por tanto, la restricción de $W$ a $\left(\ker W\right)^\perp$ es un isomorfismo de $\left(\ker W\right)^\perp$ sobre $\mathbb{K}$. En efecto, como $W\neq 0$ entonces existe $\vec{x}\in H$ tal que $W(\vec{x})\neq 0$, pero podemos escribir de forma única a $\vec{x}=\vec{x_1}+\vec{x_2}$ con $\vec{x_1}\in \ker W$ y $\vec{x_2}\in \left(\ker W\right)^\perp$, entonces:
            \begin{equation*}
                \begin{split}
                    W(\vec{x})=&W(\vec{x_1}+\vec{x_2})\\
                    =&W(\vec{x_1})+W(\vec{x_2})\\
                    =&W(\vec{x_2})\\
                    =&W\big|_{\left(\ker W\right)^\perp} (\vec{x_2})\\
                    \neq&0\\
                \end{split}
            \end{equation*}
            Sea $\beta\in\mathbb{K}$ arbitrario, entonces:
            \begin{equation*}
                \begin{split}
                    W\big|_{\left(\ker W\right)^\perp}\left(\beta\frac{\vec{x_2}}{W(\vec{x_2})} \right)=&\beta\\
                \end{split}
            \end{equation*}
            por tanto la restricción es suprayectiva. Ahora si para algún $\vec{u}\in \left(\ker W\right)^\perp$ se tiene que $W\big|_{\left(\ker W\right)^\perp}\left(\vec{u} \right)=0$, en particular $\vec{u}\in\ker W$, pero:
            \begin{equation*}
                \left(\ker W\right)^\perp\cap \ker W=\left\{\vec{0} \right\}
            \end{equation*}
            por tanto $\vec{u}=\vec{0}$. Así la restricción es inyectiva. Luego es un isomorfismo. En particular la dimensión de $\mathbb{K}$ sobre $\mathbb{K}$ es $1$, así la dimensión de $\left(\ker W\right)^\perp$ es 1.

            Tomemos $\vec{z}$ generador de $\left(\ker W\right)^\perp$. El $\vec{y}$ buscado debe ser de la forma $\vec{y}=\alpha\vec{z}$ donde $\alpha\in\mathbb{K}$. Además,
            \begin{equation*}
                \begin{split}
                    W(\vec{y})=&\norm{\vec{y}}^2\\
                    \Rightarrow \alpha W(\vec{z})=&\alpha^2\norm{\vec{z}}^2\\
                    \Rightarrow \alpha=&\conj{\frac{W(\vec{z})}{\norm{\vec{z}}^2}}\\
                \end{split}
            \end{equation*}
            así, $\vec{y}$ debe ser
            \begin{equation}
                \vec{y}=\conj{\frac{W(\vec{z})}{\norm{\vec{z}}^2}}\vec{z}
            \end{equation}
            Verifiquemos el que vector en (1.8) es el que cumple que $W=G_{\vec{y}}$. Se tiene:
            \begin{equation*}
                \begin{split}
                    G_{\vec{y}}(\vec{x})=&\pint{\vec{x}}{\vec{y}} \\
                \end{split}
            \end{equation*}
            para todo $\vec{x}\in H$, donde este vector se descompone de forma única como $\vec{x}=\vec{x_1}+\vec{x_2}$ con $\vec{x_1}\in\ker W$ y $\vec{x_2}\in\left(\ker W\right)^\perp$. Por tanto:
            \begin{equation*}
                \begin{split}
                    G_{\vec{y}}(\vec{x})=&\pint{\vec{x_1}+\vec{x_2}}{\vec{y}} \\
                    =&\pint{\vec{x_1}}{\vec{y}}+\pint{\vec{x_2}}{\vec{y}} \\
                    =&\pint{\vec{x_2}}{\vec{y}} \\
                \end{split}
            \end{equation*}
            pero los elementos de $\left(\ker W\right)^\perp$ son de la forma $\beta\vec{z}$, por lo cual:\begin{equation*}
                \begin{split}
                    G_{\vec{y}}(\vec{x})=&\pint{\beta\vec{z}}{\vec{y}} \\
                    =&\pint{\beta\vec{z}}{\conj{\frac{W(\vec{z})}{\norm{\vec{z}}^2}}\vec{z}} \\
                    =&\beta\frac{W(\vec{z})}{\norm{\vec{z}}^2}\pint{\vec{z}}{\vec{z}} \\
                    =&\beta W(\vec{z}) \\
                    =&W(\beta\vec{z}) \\
                    =&W(\vec{x_2}) \\
                    =&W(\vec{x}) \\
                \end{split}
            \end{equation*}
            con lo que se tiene el resultado.

        \end{enumerate}
    \end{proof}

    \begin{obs}
        La demostración no cambia en vez de suponer que $H$ es hilbertiano se supone $H$ prehilbertiano tal que todo subespacio cerrado es distinguido (para solventar el problema que puede llegar a haber en la restricción del fucional lineal continuo $W$). Pero la conclusión del teorema afirma que $H$ es (semilinealmente) isométrico al espacio de Banach $H^*$, luego $H$ debe ser de Banach, es decir que es hilbertiano.

        Así pues, un espacio prehilbertiano en el cual todo subespacio cerrado es distinguido es un espacio hilbertiano. 
    \end{obs}

    \begin{propo}[Autodualidad de $L_2$]
        Sea $S$ un conjunto medible en $\mathbb{R}^n$. Para cada $g\in L_2(S,\mathbb{K})$ sea $\varphi_g$ el funcional lineal sobre $L_2(S,\mathbb{K})$ definido como:
        \begin{equation*}
            \varphi_g(f)=\int_Sfg, \quad\forall f\in L_2(S,\mathbb{K})
        \end{equation*}
        entonces, la aplicación lineal $\varphi:g\mapsto \varphi_g$ es una isometría lineal de $L_2(S,\mathbb{K})$ sobre $L_2(S,\mathbb{K})^*$.
    \end{propo}

    \begin{proof}
        Sea $$\psi_g(f)=\int_S f\conj{g}$$ para todo $f\in L_2(S,\mathbb{K})$. Por el teorema de Riesz, $\psi:g\mapsto\psi_g$ es una isometría semilineal de $L_2(S,\mathbb{K})$ sobre $L_2(S,\mathbb{K})^*$.

        Como la función $\eta$, $g\mapsto\conj{g}$ es una isometría semilineal de $L_2(S,\mathbb{K})$ sobre $L_2(S,\mathbb{K})$ y $\varphi$ es la composición de $\eta$ con $\psi$, entonces $\varphi$ es una isometría lineal de $L_2(S,\mathbb{K})$ sobre $L_2(S,\mathbb{K})$. La linealidad es inmediata de las propiedades de la integral de Lebesgue.

    \end{proof}

    ¿Es posible clasificar a los espacios hilbertianos?

    Consideremos las sumas de familisa de elementos en $[0,\infty]$. Se tiene que

    \begin{equation*}
        [0,\infty]=[0,\infty[\cup\left\{\infty \right\}
    \end{equation*}
    todo conjunto $S$ en $[0,\infty]$ posee un supremo, el usual si el conjunto es acotado en $[0,\infty[$ e $\infty$ si $S$ no es acotado.

    Si $\left\{x_n \right\}_{n=1}^\infty$ es una sucesión creciente en $[0,\infty[$, se define:
    \begin{equation*}
        \lim_{n\rightarrow\infty}x_n=\sup\left\{a_n\big|n\in\mathbb{N} \right\}
    \end{equation*}
    este límite coincide con el usual en el caso de que la sucesión sea acotada. De otra forma es igual a $\infty$.

    Se tienen las siguientes propiedades:

    \begin{enumerate}
        \item $a+\infty=\infty+a=\infty$, para todo $a\in[0,\infty[$.
        \item $a\cdot \infty=\infty\cdot a=\infty$, para todo $a\in[0,\infty[$.
        \item $0\cdot\infty=\infty\cdot0=0$.
    \end{enumerate}

    \begin{mydef}
        Sea $\left(a_\alpha\right)_{\alpha\in\Omega}$ una familia arbitraria de elementos de $[0,\infty]$. Se denota por $\mathcal{F}(\Omega)$ a la colección de \textbf{todos los subconjuntos finitos de $\Omega$}. Toda suma:
        \begin{equation*}
            \sum_{\alpha\in J}a_\alpha,\quad\forall J\in\mathcal{F}(\Omega)
        \end{equation*}
        se llama \textbf{suma parcial de la familia $\left(a_\alpha\right)_{\alpha\in\Omega}$}. Al elemento de $[0,\infty]$:
        \begin{equation*}
            \sum_{\alpha\in\Omega}a_\alpha=\sup\left\{ \sum_{\alpha\in J}a_\alpha\big|J\in\mathcal{F}(\Omega) \right\}
        \end{equation*}
        se le llama \textbf{suma de la familia $\left(a_\alpha\right)_{\alpha\in\Omega}$}. Se dice que $\left(a_\alpha\right)_{\alpha\in\Omega}$ es una \textbf{familia sumable} de números no negativos si $\sum_{\alpha\in\Omega}a_\alpha<\infty$.
    \end{mydef}

    \begin{exa}
        Se tiene que:
        \begin{equation*}
            \sum_{t\in[0,1]}t=\infty
        \end{equation*}
    \end{exa}

    \begin{propo}[Conmutatividad general]
        Si $\Omega'$ es otro conjunto de índices para indexar la familia $\left(a_\alpha\right)_{\alpha\in\Omega}$ y $\sigma$ es una biyección de $\Omega$ sobre $\Omega'$, entonces:
        \begin{equation}
            \sum_{\alpha'\in\Omega'}a_{\sigma(\alpha')}=\sum_{\alpha\in\Omega}a_\alpha
        \end{equation}
    \end{propo}

    \begin{proof}
        Es inmediato del hecho de que los conjuntos de las sumas parciales de las dos familias son el mismo, por tanto al tomar el supremo se obtiene el mismo valor.
    \end{proof}

    La ecuación (1.9) se aplica en particular al caso en el que $\Omega=\Omega'$, obteniendo una propiedad de conmutatividad general para sumas de familias en $[0,\infty]$.

    Ahora, ¿se tendrá una propiedad para la asociatividad general? La respuesta es que sí, se tiene un resultado que nos permite obtener esta propiedad para sumar familias.

    \begin{theor}[Sumación por paquetes de familias]
        Sea $\left(a_\alpha\right)_{\alpha\in I}$ una familia en $[0,\infty]$ y $\left(I_\lambda\right)_{\lambda\in L}$ una partición aritraria de subconjuntos de $I$. Si
        \begin{equation*}
            \Delta=\sum_{\alpha\in I}a_\alpha\quad\textup{y}\quad\Delta_\lambda=\sum_{\alpha\in I_\lambda}a_\alpha,\quad\forall\lambda\in L
        \end{equation*}
        entonces,
        \begin{equation*}
            \Delta=\sum_{\lambda\in L}\Delta_\lambda
        \end{equation*}
    \end{theor}

    \begin{proof}
        Sea $J\in\mathcal{F}(I)$ y sea
        \begin{equation*}
            M=\left\{\lambda\in L\big|I_\lambda\cap J\neq\emptyset \right\}
        \end{equation*}
        Entonces $M\in \mathcal{F}(L)$ y
        \begin{equation*}
            \begin{split}
                \sum_{\alpha\in J}a_\alpha=&\sum_{\lambda\in M} \sum_{\alpha\in J\cap I_\lambda}a_\alpha\\
                \leq&\sum_{\lambda\in M}\Delta_\lambda\\
                \leq&\sum_{\lambda\in Ls}\Delta_\lambda\\
            \end{split}
        \end{equation*}
        tomando supremo respecto a $J$ se sigue que:
        \begin{equation}
            \Delta\leq\sum_{\lambda\in L}\Delta_\lambda
        \end{equation}

        Sea $M\in\mathcal{F}(L)$. Fijemos arbitrariamente una $H_\lambda\in\mathcal{F}(I_\lambda)$, para todo $\lambda\in M$. Se tiene
        \begin{equation*}
            \begin{split}
                \sum_{\lambda\in M}\sum_{\alpha\in H_\lambda}a_\alpha=&\sum_{\alpha\in \bigcup_{\lambda\in M}H_\lambda}a_\alpha\\
                \leq&\Delta\\
            \end{split}
        \end{equation*}
        Manteniendo a $M$ fijo y tomando supremo con respecto a $H_\lambda\in\mathcal{F}(I_\lambda)$,resulta:
        \begin{equation*}
            \sum_{\lambda\in M}\Delta_\lambda\leq\Delta
        \end{equation*}
        tomando ahora el supremo con respecto a $M$ se obtiene que:
        \begin{equation}
            \sum_{\lambda\in L}\Delta_\lambda\leq\Delta
        \end{equation}
        de (1.10) y (1.11) se sigue la igualdad.

    \end{proof}

    \begin{exa}
        ¿Es cierto que $\sum_{n\in\mathbb{N}}a_n=\sum_{n=1}^{\infty}a_n$?
    \end{exa}

    La respuesta a esta pregunta la da el siguiente teorema:

    \begin{theor}
        Para toda sucesión $\left\{a_n \right\}_{n=1}^\infty$ en $[0,\infty]$ se cumple:
        \begin{equation*}
            \sum_{n\in\mathbb{N}}a_n=\sum_{n=1}^{\infty}a_n
        \end{equation*}
    \end{theor}

    \begin{proof}
        Sea $\Delta=\sum_{n\in\mathbb{N}}a_n$ y $\Sigma=\sum_{n=1}^{\infty}a_n$. Como la colección de sumas parciales de la serie $\sum_{n=1}^{\infty}a_n$ está contenida en la colección de sumas parciales de $\sum_{n\in J }a_n$ donde $J\in \mathcal{F}(\mathbb{N})$, entonces:
        \begin{equation*}
            \Sigma\leq\Delta
        \end{equation*}
        
        Sea $J\in \mathcal{F}(\mathbb{N})$. Tomando $k=\max_{i\in J} i$ se obtiene que:
        \begin{equation*}
            \sum_{n\in J }a_n\leq\sum_{n=1 }^{k}a_n
        \end{equation*}
        tomando supremos se sigue que $\Delta\leq\Sigma$. Finalmente, se obtiene que $\Delta=\Sigma$.

    \end{proof}

    \begin{cor}[Propiedad de conmutatividad para series]
        Sea $\left\{a_n \right\}_{n=1}^\infty$ en $[0,\infty]$, y sea $\sigma$ una biyección de $\mathbb{N}$ sobre $\mathbb{N}$. Entonces:
        \begin{equation*}
            \sum_{n=1}^{\infty}a_n=\sum_{n=1}^{\infty}a_{\sigma(n) }
        \end{equation*}
    \end{cor}

    \begin{proof}
        Es inmediata del teorema anterior y de la propiedad de conmutatividad general.
    \end{proof}

    \begin{cor}
        Sea $\left\{a_{i,j} \right\}{(i,j)\in\mathbb{N}\times\mathbb{N}}$ una sucesión doble en $[0,\infty]$ y, $\sigma$ una biyección de $\mathbb{N}\times\mathbb{N}$ sobre $\mathbb{N}$.

        Tomemos $a_{i,j }=b_{\sigma(i,j)}$ para todo $(i,j)\in\mathbb{N}\times\mathbb{N}$. Entonces:
        \begin{equation*}
            \sum_{(i,j)\in\mathbb{N}\times\mathbb{N}}a_{i,j }=\sum_{k=1 }^{\infty}b_k
        \end{equation*}

        Además, sumando por paquetes, se tiene en particular que:
        \begin{equation*}
            \sum_{(i,j)\in\mathbb{N}\times\mathbb{N}}a_{i,j }=\sum_{i=1 }^{\infty} \sum_{j=1 }^{\infty}a_{ i,j}=\sum_{j=1 }^{\infty} \sum_{is=1 }^{\infty}a_{ i,j}
        \end{equation*}
    \end{cor}

    \begin{proof}
        Es inmediata del teorema de sumación por paquetes de familias.
    \end{proof}

    \begin{theor}
        Para que una familia $\left(a_\alpha \right)_{\alpha\in\Omega }$ de elementos de $[0,\infty]$ sea sumable, son necesarias y suficientes las condiciones siguientes:
        \begin{enumerate}
            \item El conjunto:
            \begin{equation*}
                \Omega_0=\left\{\alpha\in\Omega\Big|a_\alpha\neq0 \right\}
            \end{equation*}
            sea a lo sumo numerable.
            \item En el caso de que $\Omega_0$ sea numerable, si tenemos una numeración $n\mapsto\alpha(n)$ es una biyección de $\mathbb{N}$ sobre $\Omega_0$ se tenga que:
            \begin{equation*}
                \sum_{n=1 }^{\infty}a_{\alpha(n)}<\infty
            \end{equation*}
        \end{enumerate}
        En este caso:
        \begin{equation*}
            \sum_{\alpha\in\Omega }a_\alpha=\sum_{n=1 }^{\infty}a_{\alpha(n)}
        \end{equation*}

    \end{theor}
    
    \begin{proof}
        La suficiencia es clara. (Ejercicio) %TODO%

        Veamos la necesidad. Supona que la familia $(a_\alpha)_{\alpha\in\Omega}$ de números no negativos es sumable de suma digamos $\Delta$. Sea:
        \begin{equation*}
            A_\nu=\left\{\alpha\Big|a_\alpha\geq\frac{1}{\nu} \right\},\quad\forall\nu\in\mathbb{N}
        \end{equation*}
        probaremos que los $A_\nu$ son finitos. Sea $\left\{\alpha_1,...,\alpha_k\right\}$ una familia finita de índices en $A_\nu$ con $\nu\in\mathbb{N}$. Entonces:
        \begin{equation*}
            \frac{k}{\nu}\leq a_{\alpha_1}+...+a_{\alpha_k}\leq\Delta
        \end{equation*}
        por tanto, $k\leq \nu\Delta$. Esto prueba que para cada $\nu\in\mathbb{N}$, $A_\nu$ es finito.

        Como $\Omega_0=\bigcup_{\nu=1 }^\infty A_\nu$, entonces $\Omega_0$ es a lo sumo numerable.

        El resto se deja como ejercicio al lector.
    \end{proof}

    \section{Familias sumables de números complejos}

    \begin{mydef}
        Sea $\left(u_\alpha \right)_{\alpha\in\Omega}$ una familia arbitraria de números complejos. Se dice que dicha familia es \textbf{sumable} si la familia de los módulos $\left(\abs{u_\alpha } \right)_{\alpha\in\Omega}$ es una familia sumable de números no negativos.
    \end{mydef}

    \begin{propo}
        Sea $\left(u_\alpha \right)_{\alpha\in\Omega}$ una familia sumable de números complejos. Defina:
        \begin{equation*}
            \Omega_0=\left\{\alpha\in\Omega\Big|u_\alpha\neq0 \right\}
        \end{equation*}
        entonces $\Omega_0$ es a lo sumo numerable. Además, si $i\mapsto\alpha(i)$ es una biyección de $\mathbb{N}$ sobre $\Omega_0$, la serie
        \begin{equation*}
            \sum_{i=1 }^{\infty}a_{\alpha(i)}
        \end{equation*}
        es absolutamente convergente, y la suma de dicha serie es independiente la biyección $\alpha$ elegida, la cual se denomina \textbf{suma de la familia sumable $\left(u_\alpha \right)_{\alpha\in\Omega}$}, y se escribe
        \begin{equation*}
            \sum_{\alpha\in\Omega }a_\alpha=\sum_{i=1 }^{\infty}a_{\alpha(i)}
        \end{equation*}
        Si $\Omega'$ es otro conjunto numerable tal que $\Omega_0\subseteq\Omega'\subseteq\Omega$ y $i\mapsto\alpha(i)$ es una biyección de $\mathbb{N}$ sobre $\Omega'$, entonces:
        \begin{equation*}
            \sum_{\alpha\in\Omega }a_\alpha=\sum_{i=1 }^{\infty}a_{\alpha(i)}
        \end{equation*}
    \end{propo}

    \begin{proof}
        Para la primera parte. Como $\left(u_\alpha \right)_{\alpha\in\Omega}$ es sumable, entonces la familia de módulos $\left(\abs{u_\alpha} \right)_{\alpha\in\Omega}$ es sumable. Por la proposición anterior, el conjunto:
        \begin{equation*}
            \begin{split}
                \Omega_0&=\left\{\alpha\in\Omega\Big|\abs{u_\alpha}>0 \right\}\\
                &=\left\{\alpha\in\Omega\Big|u_\alpha\neq0 \right\}\\
            \end{split}
        \end{equation*}
        es a lo sumo numerable. Claramente se tiene que la serie $\sum_{i=1 }^{\infty}u_{\alpha(i)}$ es absolutamente convergente (nuevamente, pues la familia de los módulos es sumable).

        Ahora, sea $i\mapsto\beta(i)$ otra biyección de $\mathbb{N}$ sobre $\Omega_0$. Hay que probar que:
        \begin{equation*}
            s=\sum_{i=1 }^{\infty}u_{\alpha(i)}=\sum_{i=1 }^{\infty}u_{\beta(i)}=t
        \end{equation*}
        Dado $\varepsilon>0$ existe $n_0\in\mathbb{N}$ tal que:
        \begin{equation*}
            \sum_{i>n_0 }\abs{u_{\alpha(i)} }<\frac{\varepsilon}{3}\quad\textup{y}\quad \sum_{i>n_0 }\abs{u_{\beta(i)} }<\frac{\varepsilon}{3}
        \end{equation*}
        también existe $n_1\in\mathbb{N}$ tal que $n_1>n_0$ y $\left\{\alpha(1),...,\alpha(n_0)\right\}\subseteq\left\{\beta(1),...,\beta(n_1) \right\}$ (básicamente podemos cubrir todos los índices de $\alpha$ con los $\beta$ eventualmente). Se tiene:
        \begin{equation*}
            \begin{split}
                \abs{s-t}\leq&\abs{s-\sum_{i=1 }^{n_0}u_{\alpha(i)}}+\abs{\sum_{i=1 }^{n_0 }u_{\alpha(i)}-\sum_{i=1 }^{n_1 }u_{\beta(i)}}+\abs{\sum_{i=1 }^{n_1}u_{\beta(i)}-t} \\
                < &\frac{2\varepsilon}{3}+\abs{\sum_{i=1 }^{n_0 }u_{\alpha(i)}-\sum_{i=1 }^{n_1 }u_{\beta(i)}}\\
            \end{split}
        \end{equation*}
        donde la primera desigualdad se da por la convergencia de la suma de los módulos. El último término, después de la reducción, se convierte en la suma de unos cuantos $u_\alpha(i)$ con $i\geq n_0$, los cuales al ser mayorizados con sus módulos suman algo menor que $\frac{\varepsilon}{3}$. Por tanto:
        \begin{equation*}
            \abs{s-t}<\varepsilon
        \end{equation*}
        luego, $s=t$.
    \end{proof}

    \begin{mydef}
        Sea $\Omega$ un conjunto arbitrario.
        \begin{enumerate}
            \item $l_1(\Omega,\mathbb{K})$ denota al conjunto de funciones $\cf{f}{\Omega}{\mathbb{K}}$ tales que $\left(f(\alpha) \right)_{\alpha\in\Omega}$ es una familia sumable en $\mathbb{K}$. Si $f\in l_1(\Omega,\mathbb{K})$, se escribe:
            \begin{equation*}
                \mathcal{N}_1(f)=\sum_{\alpha\in\Omega }\abs{f(\alpha)}
            \end{equation*}
            \item $l_2(\Omega,\mathbb{K})$ denota al conjunto de funciones $\cf{f}{\Omega}{\mathbb{K}}$ tales que $\left(f(\alpha)^2 \right)_{\alpha\in\Omega}$ es una familia sumable en $\mathbb{K}$. Si $f\in l_2(\Omega,\mathbb{K})$, se escribe:
            \begin{equation*}
                \mathcal{N}_2(f)=\left[\sum_{\alpha\in\Omega }\abs{f(\alpha)}^2\right]^{1/2}
            \end{equation*}
        \end{enumerate}
    \end{mydef}

    \begin{propo}
        $l_1(\Omega,\mathbb{K})$ es un espacio vectorial sobre el campo $\mathbb{K}$, y $\mathcal{N}_1$ es una norma sobre $l_1(\Omega,\mathbb{K})$. Además, si $f,g\in l_1(\Omega,\mathbb{K})$ entonces,
        \begin{equation*}
            \sum_{\alpha\in\Omega }(f+g)(\alpha)=\sum_{\alpha\in\Omega }f(\alpha)+\sum_{\alpha\in\Omega }g(\alpha)
        \end{equation*}
    \end{propo}

    \begin{proof}
        Sea $f\in l_1(\Omega,\mathbb{K})$ y $\lambda\in\mathbb{K}$. Sea $J\in\mathcal{F}(\Omega)$, se tiene que:
        \begin{equation*}
            \sum_{\alpha\in J }\abs{\lambda f(\alpha)}=\abs{\lambda} \sum_{\alpha\in J }\abs{f(\alpha)}\leq\abs{\lambda}\mathcal{N}_1(f)
        \end{equation*}
        tomando supremos se sigue que $\lambda f\in l_1(\Omega,\mathbb{K})$, pues la familia de sus módulos es sumable.

        Sean ahora $f,g\in l_1(\Omega,\mathbb{K})$ y $J\in\mathcal{F}(\Omega)$. Se sabe que:
        \begin{equation*}
            \sum_{\alpha\in J }\abs{(f+g)(\alpha)}\leq \sum_{\alpha\in J }\abs{f(\alpha)}+\sum_{\alpha\in J }\abs{g(\alpha)}=\mathcal{N}_1(f)+\mathcal{N}_1(g)
        \end{equation*}
        por tanto, $f+g\in l_1(\Omega,\mathbb{K})$ y $\mathcal{N}_1(f+g)\leq\mathcal{N}_1(f)+\mathcal{N}_1(g)$.

        Finalmente, se tiene que $\mathcal{N}_1(f)=0$ si y sólo si $f(\alpha)=0$ para todo $\alpha\in\Omega$, si y sólo si $f=0$.

        Por tanto, $\mathcal{N}_1$ es una norma sobre $l_1(\Omega,\mathbb{K})$.

        Sean ahora $f,g\in l_1(\Omega,\mathbb{K})$. Tomemos:
        \begin{equation*}
            \Omega_1=\left\{\alpha\in\Omega\Big|f(\alpha)\neq0 \right\}\quad\textup{y}\quad\Omega_2=\left\{\alpha\in\Omega\Big|g(\alpha)\neq0 \right\}
        \end{equation*}
        Defina $\Omega_0=\Omega_1\cup\Omega_2$. $\Omega_0,\Omega_1$ y $\Omega_2$ son a lo sumo numerables. Sea $i\mapsto\alpha(i)$ una biyección de $\mathbb{N}$ sobre $\Omega_0$. Entonces:
        \begin{equation*}
            \begin{split}
                \sum_{\alpha\in\Omega }(f+g)(\alpha)=&\sum_{i=1 }^\infty(f+g)(\alpha(i))\\
                =&\sum_{i=1 }^\infty f(\alpha(i))+\sum_{i=1 }^\infty g(\alpha(i))\\
                =&\sum_{\alpha\in\Omega }f(\alpha)+\sum_{\alpha\in\Omega }g(\alpha)\\
            \end{split}
        \end{equation*}
    \end{proof}

    \begin{obs}
        En la sumatoria con $\Omega_0$ se usó el último resultado de la proposición 1.3.1, ya que puede que la familia $\Omega_0$ no coincida con aquella en la que $\alpha\in\Omega$ es tal que $(f+g)(\alpha)=0$, sin embargo este conjunto $\Omega_0$ contiene a este conjunto que se especificó.
    \end{obs}

    \begin{theor}
        El espacio normado $l_1(\Omega,\mathbb{K})$ con la norma $\mathcal{N}_1$ es un espacio de Banach.
    \end{theor}

    \begin{proof}
        Sea $\left\{f_\nu \right\}_{\nu=1}^\infty$ una sucesión de Cauchy en $l_1(\Omega,\mathbb{K})$, y sea $\varepsilon>0$. Existe entonces $n_0\geq 0$ tal que:
        \begin{equation}
            \mathcal{N}_2(f_p-f_q)<\varepsilon,\quad\forall p,q\geq n_0
        \end{equation}
        en particular, para cada $\alpha\in\Omega$:
        \begin{equation*}
            \abs{f_p(\alpha)-f_q(\alpha)}\leq\mathcal{N}_2(f_p-f_1)<\varepsilon,\quad\forall p,q\geq n_0
        \end{equation*}
        entonces, la sucesión $\left\{f_\nu(\alpha) \right\}_{\nu=1}^\infty$ es de Cauchy en $\mathbb{K}$. Por tanto, al ser $\mathbb{K}$ completo, entonces para cada $\alpha\in\Omega$ existe $f(\alpha)\in\mathbb{K}$ tal que:
        \begin{equation*}
            \lim_{\nu\rightarrow\infty}f_\nu(\alpha)=f(\alpha)
        \end{equation*}
        defina $\cf{f}{\Omega}{\mathbb{K}}$ la función tal que $\alpha\mapsto f(\alpha)$. Veamos que la sucesión $\left\{f_\nu \right\}_{\nu=1}^\infty$ converge a $f$. En efecto, se tiene por (1.12) que si $J\in\mathcal{F}(\Omega)$:
        \begin{equation*}
            \sum_{ \alpha\in J}\abs{f_p(\alpha)-f_q(\alpha)}<\varepsilon,\quad\forall p,q\geq n_0
        \end{equation*}
        tomemos $p\geq n_0$ fijo y tomemos el límite cuando $q\rightarrow\infty$, se tiene que:
        \begin{equation*}
            \sum_{ \alpha\in J}\abs{f_p(\alpha)-f(\alpha)}\leq\varepsilon
        \end{equation*}
        por ser $J$ finito arbitrario, se sigue que $f_p-f\in l_1(\Omega,\mathbb{K})$, de donde se sigue que $f\in l_1(\Omega,\mathbb{K})$ y:
        \begin{equation*}
            \mathcal{N}_2(f_p-f)\leq\varepsilon,\quad\forall p\geq n_0
        \end{equation*}
        luego, $l_1(\Omega,\mathbb{K})$ es completo, es decir que es de Banach.
    \end{proof}

    \begin{theor}
        Sean $f,g\in l_2(\Omega,\mathbb{K})$. Entonces, $fg \in l_1(\Omega,\mathbb{K})$ y:
        \begin{equation*}
            \mathcal{N}_1(fg)\leq\mathcal{N}_2(f)\mathcal{N}_2(g)
        \end{equation*}
        Además, $l_2(\Omega,\mathbb{K})$ es un espacio vectorial sobre el campo $\mathbb{K}$. Se define $\forall f,g\in l_2(\Omega,\mathbb{K})$
        \begin{equation*}
            \pint{f}{g}=\sum_{\alpha\in\Omega}f(\alpha)\conj{g(\alpha)}
        \end{equation*}
        La aplicación $(f,g)\mapsto\pint{f}{g}$ hace de $l_2(\Omega,\mathbb{K})$ un espacio hilbertiano en el cual la norma inducida por este producto escalar es $\mathcal{N}_2$.
    \end{theor}

    \begin{proof}
        Sea $J\in\mathcal{F}(\Omega)$. Por Cauchy-Schwartz para sumas finitas se tiene que:
        \begin{equation*}
            \begin{split}
                \sum_{\alpha\in J }\abs{f(\alpha)g(\alpha)}\leq&\left( \sum_{\alpha\in J }\abs{f(\alpha)}^2 \right)^{1/2}+\left( \sum_{\alpha\in J }\abs{g(\alpha)}^2 \right)^{1/2}\\
                \leq&\mathcal{N}_2(f)\mathcal{N}_2(g) \\
            \end{split}
        \end{equation*}
        tomando supremo respecto a $J$ se obtiene que $\mathcal{N}_1(fg)\leq\mathcal{N}_2(f)\mathcal{N}_2(g)$.

        Sean $f\in l_2(\Omega,\mathbb{K})$ y $\lambda\in\mathbb{K}$. Para todo $J\in\mathcal{F}(\Omega)$ se tiene que:
        \begin{equation*}
            \sum_{\alpha\in J}\abs{\lambda f(\alpha)}^2\leq\abs{\lambda}^2\sum_{\alpha\in J}\abs{f(\alpha)}^2\leq\abs{\lambda}^2\mathcal{N}_2(f)^2
        \end{equation*}
        tomando supremos se sigue que $\lambda f\in l_2(\Omega,\mathbb{K})$, y que $\mathcal{N}_2(\lambda f)\leq\abs{\lambda}\mathcal{N}_2(f)$ (para la igualdad hay que fijarse en la desigualdad conversa, partiendo de $\mathcal{N}_2(f)$).

        Sean $f,g\in l_2(\Omega,\mathbb{K})$. Para todo $J\in\mathcal{F}(\Omega)$ se tiene que:
        \begin{equation*}
            \begin{split}
                \sum_{\alpha\in J }\abs{f(\alpha)+g(\alpha)}^2=&\sum_{\alpha\in J }\abs{f(\alpha)}^2+\sum_{\alpha\in J }\abs{g(\alpha)}^2+\sum_{\alpha\in J }2\Re(f(\alpha)\conj{g(\alpha)})\\
                \leq&\sum_{\alpha\in J }\abs{f(\alpha)}^2+\sum_{\alpha\in J }\abs{g(\alpha)}^2+\sum_{\alpha\in J }2\abs{f(\alpha)+g(\alpha)}\\
                \leq& \mathcal{N}_2(f)+\mathcal{N}_2(g)+2\mathcal{N}_1(fg)
            \end{split}
        \end{equation*}
        tomando supremo respecto a $J$ se sigue que $f+g\in l_2(\Omega,\mathbb{K})$.

        La definición $\pint{f}{g}=\sum_{\alpha\in\Omega}f(\alpha)\conj{g(\alpha)}$ tiene sentido pues $f,g\in l_2(\Omega,\mathbb{K})$ implica que $fg\in l_1(\Omega,\mathbb{K})$. Se verifica de inmediato que $\pint{f}{g}$ es un producto escalar el cual induce $\mathcal{N}_2$.

        Ahora probaremos que es completo. Sea $\left\{f_\nu \right\}_{\nu=1}^\infty$ una sucesión de Cauchy en $l_2(\Omega,\mathbb{K})$. Sea $\varepsilon>0$. Existe $n_0\in\mathbb{N}$ tal que:
        \begin{equation*}
            \mathcal{N}_2(f_p-f_q)<\varepsilon,\quad\forall p,q\geq n_0
        \end{equation*}
        ya que para cada $\alpha\in\Omega$:
        \begin{equation*}
            \abs{f_p(\alpha)-f_q(\alpha)}\leq\mathcal{N}_2(f_p-f_q)<\varepsilon ,\quad\forall p,q\geq n_0
        \end{equation*}
        Como $\mathbb{K}$ es completo, existe $f(\alpha)\in\mathbb{K}$ tal que:
        \begin{equation*}
            \lim_{\nu\rightarrow\infty}f_\nu(\alpha)=f(\alpha)
        \end{equation*}
        Sea $J\in\mathcal{F}(\Omega)$. Se tiene entonces que:
        \begin{equation*}
            \sum_{\alpha\in J}\abs{f_p(\alpha)-f_q(\alpha)}^2\leq\varepsilon^2,\quad\forall p,q\geq n_0
        \end{equation*}
        Manteniendo a $p\geq n_0$ fijo y tomando límite cuando $q\rightarrow\infty$ y siendo $J$ finito,
        \begin{equation*}
            \sum_{\alpha\in J}\abs{f_p(\alpha)-f(\alpha)}^2\leq\varepsilon^2
        \end{equation*}
        Esto prueba que $f_p-f\in l_2(\Omega,\mathbb{K})$, de donde $f\in l_2(\Omega,\mathbb{K})$ y
        \begin{equation*}
            \mathcal{N}_2(f_p-f)\leq \varepsilon,\quad\forall p,q\geq n_0
        \end{equation*}
        luego, $l_2(\Omega,\mathbb{K})$ es completo.
    \end{proof}

    \section{Familias Ortonormales (O.N.)}

    \begin{mydef}
        Una familia de vectores, digamos $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ de vectores en un espacio prehilbertiano $H$ es \textbf{ortonormal} si:
        \begin{equation*}
                \pint{\vec{u_\alpha}}{\vec{u_\beta}}
                =\left\{\begin{array}{lcr}
                    0 & \textup{ si } \alpha\neq\beta\\
                    1 & \textup{ si } \alpha=\beta\\
                \end{array}
                \right.\quad\forall\alpha,\beta\in\Omega
        \end{equation*}
    \end{mydef}
    
    Recuerde que una familia $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ en $H$ es \textbf{linealmente independiente} si cualquier subcolección finita es linealmente independiente. Se tiene que si $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ es una familia O.N., entonces dicha familia es l.l. (linealmente independiente). En efecto, si $(\vec{u_1},...,\vec{u_n})$ son O.N., entonces
    \begin{eqnarray}
        \begin{split}
            \norm{\sum_{i=1 }^{n}\alpha_i\vec{u_i}}^2=& \sum_{i=1 }^{n}\abs{\alpha_i}^2\norm{\vec{u_i}}^2 \\
            =& \sum_{i=1 }^{n}\abs{\alpha_i}^2 \\
        \end{split}
    \end{eqnarray}
    (esto por Pitágoras), la cual es 0 si todos los $\alpha_i$ son cero, es decir si los vectores son l.i.

    ¿Cuando un espacio hilbertiano o prehilbertiano posee una base O.N.?

    \begin{propo}
        Se cumple lo siguiente:
        \begin{enumerate}
            \item Todo espacio hilbertiano $H$ de dimensión finita posee una base O.N.
            \item Sea $M$ un subespacio de dimensión finita de un espacio prehilbertiano $H$. Sea $(\vec{e_1},...,\vec{e_n})$ una base O.N. de $M$. Dado $\vec{x}\in H$. La proyección ortogonal de $\vec{x}$ sobre $M$ es:
            \begin{equation*}
                \vec{x_0}=\sum_{ i=1}^{n}\pint{\vec{x}}{\vec{e_i}}\vec{e_i}
            \end{equation*}
        \end{enumerate}
    \end{propo}

    \begin{proof}
        De (1): La prueba se hará por inducción sobre la dimensión de $H$.
        \begin{itemize}
            \item Suponga que la dimensión es 1. Existe $\vec{u}\in H$ tal que $\vec{u}\neq\vec{0}$. Una base O.N. de $H$ es $(\frac{\vec{u}}{\norm{\vec{u}}})$.
            \item Suponga que el resultado es cierto para dimensión $n-1$. Sea $H$ de dimensión $n$, y $\vec{u}\in H$ diferente de $\vec{0}$, defina:
            \begin{equation*}
                \vec{e_1}=\frac{\vec{u}}{\norm{\vec{u}}}
            \end{equation*}
            Sea $M=\mathcal{L}(\vec{e_1})$. Ya que $H=M\oplus M^\perp$ (ya que $M$ es distinguido por ser de dimensión finita), necesariamente $\dim M^\perp = n-1$. Por hipótesis inductiva $M^\perp$ posee una base O.N. digamos $(\vec{e_2},...,\vec{e_n})$.
            
            Entonces, es claro que $(\vec{e_1},...,\vec{e_n})$ es base O.N. de $H$.
        \end{itemize}
        aplicando inducción se sigue el resultado.

        De (2): Sea $\vec{x_0}=\sum_{ i=1}^{n}\pint{\vec{x}}{\vec{e_i}}\vec{e_i}$. Se tiene
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}-\vec{x_0}}{\vec{e_i}}=&\pint{\vec{x}}{\vec{e_i}}-\pint{\vec{x_0}}{\vec{e_i}} \\
                =&\pint{\vec{x}}{\vec{e_i}}-\pint{\sum_{ j=1}^{n}\pint{\vec{x}}{\vec{e_j}}\vec{e_j}}{\vec{e_i}} \\
                =&\pint{\vec{x}}{\vec{e_i}}-\pint{\pint{\vec{x}}{\vec{e_j}}\vec{e_j}}{\vec{e_i}} \\
                =&\pint{\vec{x}}{\vec{e_i}}-\pint{\vec{x}}{\vec{e_j}}\pint{\vec{e_j}}{\vec{e_i}} \\
                =&\pint{\vec{x}}{\vec{e_i}}-\pint{\vec{x}}{\vec{e_j}}\\
                =&0\\
            \end{split}
        \end{equation*}
        Siendo $M=\mathcal{L}(\vec{e_1},...,\vec{e_n})$, necesariamente $\pint{\vec{x}-\vec{x_0}}{\vec{y}}=0$, para todo $\vec{y}\in M$. Por tanto, $\vec{x_0}$ es efectivamente la proyección ortogonal de $\vec{x}$ sobre $M$.
    \end{proof}

    \begin{mydef}
        Sea $H$ prehilbertiano y sea $(\vec{u}_\alpha)_{\alpha\in\Omega}$ una familia O.N. en $H$. Para cada $\vec{x}\in H$ se define una función $\cf{\hat{x}}{\Omega}{\mathbb{K}}$ dada por:
        \begin{equation*}
            \hat{x}(\alpha)=\pint{\vec{x}}{\vec{u_\alpha}},\quad\forall\alpha\in\Omega
        \end{equation*}
        los escalares $\hat{x}(\alpha)$ se llaman \textbf{los coeficientes de Fourier de $\vec{x}$ con respecto a la familia ortonormal $(\vec{u}_\alpha)_{\alpha\in\Omega}$}.
    \end{mydef}

    \begin{theor}
        Con las hipótesis y notaciones de la definición anterior, $\forall\vec{x}\in H$ se tiene que $\hat{x}\in l_2(\Omega,\mathbb{K})$, y se cumple
        \begin{equation*}
            \mathcal{N}_2(\hat{x})\leq\norm{\vec{x}}
        \end{equation*}
        es decir:
        \begin{equation*}
            \sum_{\alpha\in\Omega}\abs{\hat{x}(\alpha)}^2\leq\norm{\vec{x}}^2
        \end{equation*}
        la desigualdad anterior es llamada \textbf{desigualdad de Bessel}.
    \end{theor}

    \begin{proof}
        Sea $J\in\mathcal{F}(\Omega)$ y defina $M_J=\mathcal{L}((\vec{u_\alpha})_{\alpha\in J})$. Entonces $M_J$ es un subespacio de dimensión finita de $H$ provisto de la base O.N. $(\vec{u_\alpha})_{\alpha\in J}$. Entonces, la proyección ortogonal de $\vec{x}$ sobre $M_J$ debe ser:
        \begin{equation*}
            \vec{x_0}=\sum_{\alpha\in J }\pint{\vec{x}}{\vec{u_\alpha}}\vec{u_\alpha}=\sum_{\alpha\in J }\hat{x}(\alpha)\vec{u_\alpha}
        \end{equation*}
        por la proposición anterior. Por Pitágoras:
        \begin{equation*}
            \sum_{\alpha\in J}\abs{\hat{x}(\alpha)}^2=\norm{\sum_{\alpha\in J}\hat{x}(\alpha)\vec{u_\alpha}}^2=\norm{\vec{x_0}}^2=\norm{\vec{x}}^2-d(\vec{x},M)^2\leq\norm{\vec{x}}^2
        \end{equation*}
        tomando supremos respecto a $J$ se tiene que:
        \begin{equation*}
            \begin{split}
                \mathcal{N}_2(\hat{x})^2&\leq\norm{\vec{x}}^2\\
                \Rightarrow \mathcal{N}_2(\hat{x})&\leq\norm{\vec{x}}\\
            \end{split}
        \end{equation*}
        lo cual prueba el resultado.
    \end{proof}

    \begin{cor}
        La aplicación $\vec{x}\mapsto\hat{x}$ de $H$ en $l_2(\Omega,\mathbb{K})$ es una aplicación lineal continua de norma menor o igual a 1.
    \end{cor}

    \begin{proof}
        Veamos que es lineal. Sean $\vec{x},\vec{x_1},\vec{x_2}\in H$ y $a\in\mathbb{K}$. Se tiene que:
        \begin{equation*}
            \begin{split}
                \widehat{x_1+x_2}(\alpha)&=\pint{\vec{x_1}+\vec{x_2}}{\vec{u_\alpha}}\\
                &=\pint{\vec{x_1}}{\vec{u_\alpha}}+\pint{\vec{x_1}+\vec{x_2}}{\vec{u_\alpha}}\\
                &=\hat{x_1}(\alpha)+\hat{x_2}(\alpha)
            \end{split}
        \end{equation*}
        para todo $\alpha\in\Omega$. Por tanto, $\widehat{x_1+x_2}=\hat{x_1}+\hat{x_2}$.

        Además,
        \begin{equation*}
            \begin{split}
                \widehat{a x}(\alpha)&=\pint{a\vec{x}}{\vec{u_\alpha}}\\
                &=a\pint{\vec{x}}{\vec{u_\alpha}}\\
                &=a\hat{x}(\alpha) \\
            \end{split}
        \end{equation*}
        para todo $\alpha\in\Omega$. Por tanto, $\widehat{a x}=a\hat{x}$. Luego, $\vec{x}\mapsto\hat{x}$ es lineal y continua por la desigualdad de Bessel, donde se deduce de forma inmediata que la norma de esta aplicación lineal es menor o igual que uno.
    \end{proof}
        
    \begin{cor}
        Si $(\vec{u}_\alpha)_{\alpha\in\Omega}$ es un sistema O.N. de $H$ y $\vec{x}\in H$, entonces:
        \begin{equation*}
            \hat{x}(\alpha)=\pint{\vec{x}}{\vec{u_\alpha}}\neq0
        \end{equation*}
        para una cantidad a lo sumo numerable de índices $\alpha\in\Omega$.
    \end{cor}

    \begin{proof}
        Sea
        \begin{equation*}
            \begin{split}
                \Omega_0
                &=\left\{\alpha\in\Omega\Big| \pint{\vec{x}}{\vec{u_\alpha}} \neq0 \right\}\\
                &=\left\{\alpha\in\Omega\Big| \hat{x}(\alpha)\neq0 \right\}\\
            \end{split}
        \end{equation*}
        como la familia $\left\{\hat{x}(\alpha) \right\}_{\alpha\in\Omega}$ es sumable, el conjunto $\Omega_0$ es a lo sumo numerable, es decir que $\pint{\vec{x}}{\vec{u_\alpha}}\neq0$ para una cantidad a lo sumo numerable de $\alpha\in\Omega$.
    \end{proof}

    \begin{theor}[Teorema de Riesz-Fischer]
        Sea $H$ prehilbertiano y sea $(\vec{u_\alpha})_{\alpha\in\Omega}$ una familia O.N. en $H$. Se supone que el subespacio cerrado
        \begin{equation*}
            M=\overline{\mathcal{L}((\vec{u_\alpha})_{\alpha\in\Omega})}
        \end{equation*}
        es completo (lo cual se cumple en particular si $H$ es hilbertiano). Entonces la aplicación $\vec{x}\mapsto\hat{x}$ es suprayectiva de $H$ sobre $l_2(\Omega,\mathbb{K})$. Más precisamente, dado $\varphi\in l_2(\Omega,\mathbb{K})$ existe un único $\vec{x_0}\in M$ tal que sus coeficientes de Fourier coinciden con $\varphi$, i.e. $\hat{x_0}=\varphi$.
        
        Además, para cualquier $\vec{x}\in H$ se cumple que $\hat{x}=\varphi$ si y sólo si $\vec{x}-\vec{x_0}\perp M$. 
    \end{theor}

    \begin{proof}
        Se harán varias cosas:
        \begin{enumerate}
            \item Sea $\varphi\in l_2(\Omega,\mathbb{K})$ y sea $\Omega_0=\left\{\alpha\in\Omega\Big|\varphi(\alpha)\neq0 \right\}$ el cual es a lo sumo numerable. Sea $i\mapsto\alpha(i)$ una biyección de $\mathbb{N}$ sobre $\Omega_0$.
            
            Considere la serie
            \begin{equation*}
                \sum_{i=1} ^{\infty}\varphi(\alpha(i))\vec{u_{\alpha(i)}}
            \end{equation*}
            en $M$. Como
            \begin{equation*}
                \sum_{i=1}^{\infty}\abs{\varphi(\alpha(i))}^2=\mathcal{N}_2(\varphi)<\infty
            \end{equation*}
            dado $\varepsilon>0$ existe $n_0\in\mathbb{N}$ tal que si $q>p\geq n_0$:
            \begin{equation*}
                \begin{split}
                    \norm{\sum_{i=p }^{q}\varphi(\alpha(i))\vec{u_{\alpha(i)}}}^2
                    =&\sum_{i=p }^{q}\abs{\varphi(\alpha(i))^2}\norm{\vec{u_{\alpha(i)}}}^2 \\
                    =&\sum_{i=p }^{q}\abs{\varphi(\alpha(i))^2}\\
                    \leq&\varepsilon^2 \\
                \end{split}
            \end{equation*}
            se cumple pues la condición de Cauchy para la convergencia de la serie en $M$. Como $M$ es completo, existe $\vec{x_0}\in M$ único tal que
            \begin{equation*}
                \vec{x_0}=\sum_{i=1} ^{\infty}\varphi(\alpha(i))\vec{u_{\alpha(i)}}
            \end{equation*}
            Se tiene
            \begin{equation*}
                \begin{split}
                    \pint{\sum_{i=1 }^{m}\varphi(\alpha)\vec{u_{\alpha(i)}}}{\vec{u_{\alpha(k)}}}=\varphi(\alpha(k)),\quad\forall m\geq k \\
                \end{split}
            \end{equation*}
            tomando límite cuando $m\rightarrow\infty$ y usando la continuidad de $\pint{\cdot}{\cdot}$:
            \begin{equation*}
                \Rightarrow \hat{x_0}(\alpha(k))=\pint{\vec{x_0}}{\vec{u_{\alpha(k)}}}=\varphi(\alpha(k)),\quad\forall k\in\mathbb{N}
            \end{equation*}
            Sea $\alpha\in\Omega\backslash\Omega_0$. Se tiene:
            \begin{equation*}
                \begin{split}
                    \pint{\sum_{i=1 }^{m}\varphi(\alpha)\vec{u_{\alpha(i)}}}{\vec{u_{\alpha}}}=0=\varphi(\alpha(k)),\quad\forall m\in\mathbb{N} \\
                \end{split}
            \end{equation*}
            tomando límite cuando $m\rightarrow\infty$ y usando la continuidad de $\pint{\cdot}{\cdot}$ se obtiene que:
            \begin{equation*}
                \Rightarrow \hat{x_0}(\alpha)=\pint{\vec{x_0}}{\vec{u_{\alpha}}}=\varphi(\alpha)
            \end{equation*}
            por tanto, $\hat{x_0}=\varphi$.

            \item Sea $\vec{x}\in H$ tal que $\hat{x}=\varphi$. Entonces,
            \begin{equation*}
                \widehat{x-x_0}=\hat{x}-\hat{x_0}=\varphi-\varphi=0
            \end{equation*}
            luego
            \begin{equation*}
                \pint{\vec{x}-\vec{x_0}}{\vec{u_\alpha}}=0\quad\forall\alpha\in\Omega
            \end{equation*}
            Así pues, $\vec{x}-\vec{x_0}\perp\mathcal{L}((\vec{u_\alpha})_{\alpha\in\Omega})$. Siendo los elementos de $M$ límites de sucesiones en $\mathcal{L}((\vec{u_\alpha})_{\alpha\in\Omega})$, por la continuidad de $\pint{\cdot}{\cdot}$ se tiene que $\vec{x}-\vec{x_0}\perp M$.

            Recíprocamente, si $\vec{x}-\vec{x_0}\perp M$, es claro que $\hat{0}=\widehat{x-x_0}=\hat{x}-\hat{x_0}=\hat{x}-\varphi$. Por tanto, $\hat{x}=\varphi$. En particular, si $\vec{x}\in M$ y $\hat{x}=\varphi$, resulta que $\vec{x}-\vec{x_0}\in M$ y $\vec{x}-\vec{x_0}\perp M$, luego $\vec{x}-\vec{x_0}\perp\vec{x}-\vec{x_0}$, o sea $\vec{x}-\vec{x_0}=\vec{0}\Rightarrow \vec{x}=\vec{x_0}$, es decir que el $\vec{x_0}$ es único.
        \end{enumerate}
    \end{proof}
 
    \begin{obs}
        La unicidad de $\hat{x_0}\in M$ implica que $\sum_{i=1}^{\infty}\varphi(\alpha(i))\vec{u_{\alpha(i)}}$ es independiente de la biyección elegida $i\mapsto \alpha(i)$ de $\mathbb{N}$ sobre $\Omega_0$. Abusando de la notación se escribe
        \begin{equation*}
            \hat{x_0}=\sum_{\alpha\in\Omega}\varphi(\alpha)\vec{u_\alpha}
        \end{equation*}
    \end{obs}

    ¿Cuándo el subespacio $M$ coincide con $H$?

    \begin{mydef}
        Una familia O.N. de vectores $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ se dice \textbf{maximal} si no existe una familia O.N. que la contenga propiamente. Es decir, $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ es O.N. maximal si y sólo si
        \begin{equation*}
            \vec{x}\perp\vec{u_\alpha},\forall\alpha\in\Omega\Rightarrow\vec{x}=\vec{0}
        \end{equation*}
    \end{mydef}

    \begin{theor}[\textbf{Teorema de Parseval}]
        Sea $H$ hilbertiano, y sea $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ una familia O.N. de vectores en $H$. Son equivalentes las siguientes afirmaciones:
        \begin{enumerate}
            \item $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ es maximal.
            \item $\overline{\mathcal{L}\left(\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}\right)}=H$.
            \item Para todo $\vec{x}\in H$, $\N{2}{\hat{x}}=\norm{\vec{x}}$, o sea que $\sum_{\alpha\in\Omega}\abs{\hat{x}(\alpha)}^2=\norm{\vec{x}}^2$.
            \item Para todo $\vec{x},\vec{y}\in H$,
            \begin{equation*}
                \pint{\hat{x}}{\hat{y}}=\pint{\vec{x}}{\vec{y}}
            \end{equation*}
            o sea
            \begin{equation*}
                \sum_{\alpha\in\Omega}x(\alpha)\conj{y(\alpha)}=\pint{\vec{x}}{\vec{y}}
            \end{equation*}
        \end{enumerate}

    \end{theor}

    \begin{proof}
        $1)\Rightarrow 2)$: Suponga que 2. es falso, entonces,
        \begin{equation*}
            M=\overline{\mathcal{L}\left(\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}\right)}\neq H
        \end{equation*}
        es decir que existe $\vec{x}\in H$ tal que $\vec{x}\notin M$. Como $M$ es un subesapcio cerrado de $H$ hilbertiano, entonces es ditinguido, luego existe la proyección ortogonal de $\vec{x}$ sobre $M$, digamos $\vec{x_0}$, entonces $\vec{x}-\vec{x_0}\neq\vec{0}$, pues $\vec{x}\notin M$ y, en particular
        \begin{equation*}
            \vec{x}-\vec{x_0}\perp M
        \end{equation*}
        luego,
        \begin{equation*}
            \vec{x}-\vec{x_0}\perp\vec{u_\alpha}\quad\forall\alpha\in\Omega
        \end{equation*}
        es decir que la familia $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ no es maximal.

        $2)\Rightarrow 3)$: Ya se sabe que $\N{2}{\hat{x}}\leq\norm{\vec{x}}$ por la desigualdad de Bessel.
        Sea $\vec{x}\in H$ y $\varepsilon>0$. Como $\overline{\mathcal{L}\left(\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}\right)}= H$, existen $J\subseteq \mathcal{F}(\Omega)$ y $\left(\lambda_\alpha \right)_{\alpha\in J }$ en $\mathbb{K}$ tales que:
        \begin{equation}
            \norm{\vec{x}-\sum_{\alpha\in J }\lambda_\alpha\vec{u_\alpha}}<\sqrt{\varepsilon}
        \end{equation}
        Sea $M_J=\mathcal{L}\left(\left(\vec{u_\alpha} \right)_{ \alpha\in J} \right)$, el cual es de dimensión finita, luego cerrado. Por tanto, de la ecuación anterior se sigue que:
        \begin{equation*}
            d(\hat{x},M_J)\leq\sqrt{\varepsilon}
        \end{equation*}
        Si $\hat{x_0}=\sum_{\alpha\in J}\hat{x}(\alpha)\vec{u_\alpha}$, entonces $\vec{x_0}$ es la proyección ortogonal de $\vec{x}$ sobre $M_J$, luego:
        \begin{equation*}
            \begin{split}
                d(\vec{x},M_J)^2&=\norm{\vec{x}}^2-\norm{\vec{x_0}}^2\\
                &\leq\varepsilon\\
            \end{split}
        \end{equation*}
        así pues:
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}}^2-\varepsilon&=\norm{\vec{x_0}}^2 \\
                &=\sum_{\alpha\in J}\abs{\hat{x}(\alpha)}^2\norm{\vec{u_\alpha}}^2\\
                &=\sum_{\alpha\in J}\abs{\hat{x}(\alpha)}^2\\
                &\leq\N{2}{\hat{x}}^2
            \end{split}
        \end{equation*}
        Como $\varepsilon>0$ es arbitrario, se sigue que
        \begin{equation*}
            \norm{\vec{x}}\leq\N{2}{\hat{x}}
        \end{equation*}
        por tanto, $\norm{\vec{x}}=\N{2}{\hat{x}}$.

        $3)\Rightarrow 4)$: La identidad $\norm{\vec{x}}=\N{2}{\hat{x}}$ puede ser reescrita como:
        \begin{equation*}
            \pint{\hat{x}}{\hat{x}}=\pint{\vec{x}}{\vec{x}},\quad\forall\vec{x}\in H
        \end{equation*}
        entonces, para todo $\vec{x},\vec{y}\in H$ y para todo $\lambda\in\mathbb{K}$ se cumple:
        \begin{equation*}
            \begin{split}
                \pint{\hat{x}+\lambda\hat{y}}{\hat{x}+\lambda\hat{y}}&=\pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}\\
                \Rightarrow \conj{\lambda}\pint{\hat{x}}{\hat{y}}+\lambda\pint{\hat{y}}{\hat{x}} &=\conj{\lambda}\pint{\vec{x}}{\vec{y}}+\lambda\pint{\vec{y}}{\vec{x}} \\
            \end{split}
        \end{equation*}
        tomando $\lambda=1$ y $\lambda=i$ tenemos las siguientes igualdades:
        \begin{equation*}
            \begin{split}
                \pint{\hat{x}}{\hat{y}}+\pint{\hat{y}}{\hat{x}} &=\pint{\vec{x}}{\vec{y}}+\pint{\vec{y}}{\vec{x}}\\
                \Rightarrow \pint{\hat{x}}{\hat{y}}+\conj{\pint{\hat{y}}{\hat{x}}}&=\pint{\vec{x}}{\vec{y}}+\conj{\pint{\vec{x}}{\vec{y}}} \\
                \Rightarrow 2\Re\pint{\hat{x}}{\hat{y}}&=2\Re\pint{\vec{x}}{\vec{y}}\\
                \Rightarrow \Re\pint{\hat{x}}{\hat{y}}&=\Re\pint{\vec{x}}{\vec{y}}\\
            \end{split}
        \end{equation*}
        y
        \begin{equation*}
            \begin{split}
                i\pint{\hat{x}}{\hat{y}}-i\pint{\hat{y}}{\hat{x}}&=i\pint{\vec{x}}{\vec{y}}-i\pint{\vec{y}}{\vec{x}}\\
                \Rightarrow i\pint{\hat{x}}{\hat{y}}+\conj{i\pint{\hat{x}}{\hat{y}}}&=i\pint{\vec{x}}{\vec{y}}+\conj{i\pint{\vec{y}}{\vec{x}}}\\
                \Rightarrow 2\Re i\pint{\hat{x}}{\hat{y}} &=2\Re i\pint{\vec{x}}{\vec{y}} \\
                \Rightarrow -\Im \pint{\hat{x}}{\hat{y}} &=-\Im \pint{\vec{x}}{\vec{y}} \\
                \Rightarrow \Im \pint{\hat{x}}{\hat{y}} &=\Im \pint{\vec{x}}{\vec{y}} \\
            \end{split}
        \end{equation*}
        lo anterior muestra que $\pint{\vec{x}}{\vec{y}}$ y $\pint{\hat{x}}{\hat{y}}$ tienen las mismas parte real e imaginaria, por tanto son iguales.

        $4)\Rightarrow 1)$: Supongamos que 1. es falso. Entonces, existe $\vec{x}\in H\backslash\left\{\vec{0} \right\}$ tal que $\vec{x}\perp\vec{u_\alpha}$ para todo $\alpha\in\Omega$, es decir que:
        \begin{equation*}
            \hat{x}(\alpha)=\pint{\vec{x}}{\vec{u_\alpha}},\quad\forall\alpha\in\Omega
        \end{equation*}
        al tomar $\vec{x}=\vec{y}$, resulta que:
        \begin{equation*}
            \pint{\hat{x}}{\hat{x}}=0
        \end{equation*}
        siendo que $\pint{\vec{x}}{\vec{x}}\neq0$. Luego, se tiene el resultado.
    \end{proof}

    \begin{obs}
        Las identidades en 3. y 4. son llamadas \textbf{identidades de Parseval}.
    \end{obs}

    \begin{obs}
        Si $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ es un sistema ortonormal maximal en un espacio hilbertiano $H$, entonces $\vec{x}\mapsto\hat{x}$ es una isometría lineal de $H$ en $l_2(\Omega,\mathbb{K})$. Pero, por el teorema de Riesz-Fischer esta aplicación es suprayectiva, luego $H$ y $l_2(\Omega,\mathbb{K})$ son linealmente isométricos (en particular, son isomorfos). La isometría inversa está dada por: para todo $\varphi\in l_2(\Omega,\mathbb{K})$,
        \begin{equation*}
            \varphi\mapsto\sum_{\alpha\in\Omega}\varphi(\alpha)\vec{u_\alpha}
        \end{equation*}
    \end{obs}

    \begin{theor}
        En todo espacio prehilbertiano $H$ existe una familia O.N. maximal.
    \end{theor}

    \begin{proof}
        Sea $\mathcal{O}$ la colección de todas las familias ortonormales de vectores en $H$. La relación inclusión $\subseteq$ hace de $\mathcal{O}$ un conjunto ordenado (no totalmente ordenado). Se verá que $\mathcal{O}$ es inductivo. Sea $\left(\mathcal{F}_i \right)_{i\in J}$ una cadena en $\mathcal{O}$, es decir, $\left(\mathcal{F}_i \right)_{i\in J}$ es una familia de sistemas O.N. tales que:
        \begin{equation*}
            \mathcal{F}_i\subseteq\mathcal{F}_j\quad\textup{o}\quad\mathcal{F}_j\subseteq\mathcal{F}_i
        \end{equation*}
        para todo $i,j\in J$. Sea $\mathcal{F}=\bigcup_{ i\in J}\mathcal{F}_i$. Si $\vec{u_\alpha},\vec{u_\beta}\in\mathcal{F}$, entonces existe $i\in J$ tal que $\vec{u_\alpha},\vec{u_\beta}\in\mathcal{F}_i$, luego $\pint{\vec{u_\alpha}}{\vec{u_\beta}}=\delta_{\alpha\beta}$, por tanto $\mathcal{F}$ es un sistema O.N, el cual es un mayorante de la cadena $\left(\mathcal{F}_i \right)_{i\in J}$ (es decir que tiene un elemento máximo).
        
        Así pues, $\mathcal{O}$ es un conjunto ordenado inductivo. Por el lema de Zorn, $\mathcal{O}$ contiene un elemento maximal. Este elemento es un sistema ortonormal maximal.
    \end{proof}

    \begin{theor}
        En un espacio hilbertiano $H$, dos familias O.N. maximales tienen la misma cardinalidad.
    \end{theor}

    \begin{proof}
        Sean $\mathcal{A},\mathcal{B}$ dos familias ortonormales maximales en $H$. Se tienen dos casos:
        \begin{enumerate}
            \item $\mathcal{A}$ es finito, digamos que $\mathcal{A}=\left(\vec{u_1},...,\vec{u_n} \right)$. Por Parseval:
            \begin{equation*}
                H=\overline{\mathcal{L}(\vec{u_1},...,\vec{u_n})}=\mathcal{L}(\vec{u_1},...,\vec{u_n})
            \end{equation*}
            pues la dimensión del generado es finita por tanto, el generado es cerrado. Así pues, $H$ es de dimensión finita.
            
            Como los elementos de $\mathcal{B}$ son l.i. entonces $\mathcal{B}$ también debe ser finito y cumplen que si $\mathcal{B}=\left\{\vec{v_1},...,\vec{v_m} \right\}$:
            \begin{equation*}
                H=\overline{\mathcal{L}\left\{\vec{v_1},...,\vec{v_m} \right\}}=\mathcal{L}\left\{\vec{v_1},...,\vec{v_m} \right\}
            \end{equation*}
            es decir que $m=n$ por ser bases de $H$. Luego $\abs{\mathcal{A}}=\abs{\mathcal{B}}$.
            \item Suponga que $\mathcal{A}$ es infinito. Por 1. también debe suceder que $\mathcal{B}$ sea infinito. Para todo $\vec{a}\in\mathcal{A}$ se define:
            \begin{equation*}
                \mathcal{B}_{\vec{a}}=\left\{\vec{b}\in\mathcal{B}\Big|\hat{a}\left(\vec{b}\right) =\pint{\vec{a}}{\vec{b}}\neq0 \right\}
            \end{equation*}
            se sabe que $\mathcal{B}_{\vec{a}}$ es a lo sumo numerable. Por otra parte, por la identidad de Parseval:
            \begin{equation*}
                \begin{split}
                    1&=\norm{\vec{b}}^2\\
                    &=\N{2}{\hat{b}}^2\\
                    &=\sum_{\vec{a}\in\mathcal{A}}\pint{\vec{b}}{\vec{a}}^2\\
                    &=\sum_{\vec{a}\in\mathcal{A}}\abs{\hat{b}(\vec{a})}^2\\
                \end{split}
            \end{equation*}
            así que, debe existir $\vec{a}\in\mathcal{A}$ tal que $\pint{\vec{b}}{\vec{a}}\neq0$, es decir que
            \begin{equation*}
                \mathcal{B}=\bigcup_{\alpha\in\mathcal{A}}\mathcal{B}_\alpha
            \end{equation*}
            Entonces,
            \begin{equation*}
                \begin{split}
                    \abs{\mathcal{B}}&=\abs{\bigcup_{\alpha\in\mathcal{A}}\mathcal{B}_\alpha}\\
                    &\leq\abs{\mathcal{A}}\aleph_0\\
                    &\leq\abs{\mathcal{A}}\\
                \end{split}
            \end{equation*}
            pues, $\abs{\mathcal{B}_{\vec{a}}}\leq\aleph_0$, para todo $\vec{a}\in\mathcal{A}$. Por simetría se obtiene también que $\abs{\mathcal{A}}\leq\abs{\mathcal{B}}$. Por tanto, por Cantor-Bernstein se sigue que $\abs{\mathcal{A}}=\abs{\mathcal{B}}$.
        \end{enumerate}
    \end{proof}

    \begin{theor}[\textbf{Teorema de Clasificación de espacios Hilbertianos}]
        Todo espacio hilbertiano es linealmente isométrico a algún $l_2(\Omega,\mathbb{K})$. Además, $l_2(\Omega,\mathbb{K})$ y $l_2(\Omega',\mathbb{K})$ son linealmente isométricos si y sólo si $\abs{\Omega}=\abs{\Omega'}$.
    \end{theor}

    \begin{proof}
        Para la primera parte, sea $H$ hilbertiano. Por un teorema anterior, $H$ posee una familia O.N. maximal, digamos $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$, entonces, por otro teorema $H$ debe ser linealmente isométrico a $l_2(\Omega,\mathbb{K})$.

        Para la segunda parte, sea $\Omega$ arbitrario. Para cada $\alpha\in\Omega$ se define $\varphi_\alpha\in l_2(\Omega,\mathbb{K})$ como:
        \begin{equation*}
            \varphi_{\alpha}(\beta)=\delta_{\alpha\beta},\quad\forall\alpha,\beta\in\Omega
        \end{equation*}
        Claramente $\left(\varphi_\alpha \right)_{\alpha\in\Omega}$ es un sistema O.N. en $l_2(\Omega,\mathbb{K})$.

        Sea $\Omega'$ otro conjunto tal que $\abs{\Omega'}\neq\abs{\Omega}$. Si $l_2(\Omega,\mathbb{K})$ y $l_2(\Omega',\mathbb{K})$ fuesen linealmente isométricos, se tendrían dos sistemas O.N. maximales en $l_2(\Omega,\mathbb{K})$ con misma cardinalidad, a saber $\left(\varphi_\alpha \right)_{\alpha\in\Omega}$ y el inducido por el de $l_2(\Omega',\mathbb{K})$ a través de la isometría, lo cual no es posible por un teorema anterior.

        Si $\abs{\Omega}=\abs{\Omega'}$ entonces, existe existe $\cf{h}{\Omega}{\Omega'}$ biyección. En este caso, la aplicación $f\mapsto f\circ h$ sería una isometría lineal de $l_2(\Omega',\mathbb{K})$ en $l_2(\Omega,\mathbb{K})$.
    \end{proof}

    \section{Espacios Separables}

    \begin{mydef}
        Un número complejo $\lambda$ se llama \textbf{complejo racional} si su parte real e imaginaria son racionales.
    \end{mydef}

    \begin{theor}
        Un espacio hilbertiano $H$ es separable si y sólo si $H$ posee una familia ortonormal maximal a lo sumo numerable.
    \end{theor}

    \begin{proof}
        $\Rightarrow)$: Suponga que $H$ es separable (de dimensión infinita, si es dimensión finita el resultado es inmediato). Sea $\left(\vec{x_i} \right)_{i\in\mathbb{N}}$ una sucesión densa en $H$ (la cual existe por ser separable). Sea ahora $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ una familia O.N. maximal. Para obtener el resultado, se debe probar que $\Omega$ es numerable.

        Sea
        \begin{equation*}
            A_i=\left\{\alpha\in\Omega\Big|\pint{\vec{x_i}}{\vec{u_\alpha}}\neq0 \right\}
        \end{equation*}
        Se sabe que $A_i$ es a lo sumo numerable (ya que coincide con los coeficientes de Fourier de $\vec{x_i}$). Para que $\Omega$ sea numerable, basta probar con que:
        \begin{equation*}
            \Omega=\bigcup_{ i=1}^\infty A_i
        \end{equation*}
        Una contención se tiene por definición de los $A_i$. Sea $\alpha\in\Omega$ arbitrario. Como $\left(\vec{x_i} \right)_{i\in\mathbb{N}}$ es densa, existe $l\in\mathbb{N}$ tal que:
        \begin{equation*}
            \norm{\vec{x_l}-\vec{u_\alpha}}<1
        \end{equation*}
        Si fuera que $\pint{\vec{\vec{x_l}}}{u_\alpha}=0$, entonces
        \begin{equation*}
            \begin{split}
                \norm{\vec{x_l}-\vec{u_\alpha}}^2&=\norm{\vec{u_\alpha}}^2+\norm{\vec{x_\alpha}}^2\\
                &\geq 1\\
                \Rightarrow \norm{\vec{x_l}-\vec{u_\alpha}}&\geq1
            \end{split}
        \end{equation*}
        lo cual no puede suceder por la elección del $l$. Por tanto, $\pint{\vec{x_i}}{\vec{u_\alpha}}\neq0$, es decir que $\alpha\in A_l$.

        $\Leftarrow)$: Suponga que $\left(\vec{u_n} \right)_{n\in\mathbb{N} }$ es O.N. maximal. Sea $S$ la colección de todas las sumas finitas de la forma 
        \begin{equation*}
            \sum_{ i\in J}\alpha_i\vec{u_i}
        \end{equation*}
        con $\alpha_i$ complejo racional (o racional en caso de que el campo sea $\mathbb{R}$) y $J\in\mathcal{F}(\mathbb{N})$.Se sabe que $S$ es numerable. Como $H=\overline{\mathcal{L}(\left(\vec{u_n} \right)_{n\in\mathbb{N} })}$, dado $\vec{x}\in H$ y $\varepsilon>0$ existe $\sum_{ i=1}^{n}\beta_i\vec{u_i}$ con $\beta_i\in\mathbb{K}$ tal que
        \begin{equation*}
            \norm{\vec{x}-\sum_{i=1 }^{n}\beta_i\vec{u_i}}<\frac{\varepsilon}{2}
        \end{equation*}
        Ahora, para cada $i\in\left[|1,n|\right]$ existe $\alpha_i$ racional compljeto (o racional) tal que:
        \begin{equation*}
            \abs{\beta_i-\alpha_i}<\frac{\varepsilon}{2n}
        \end{equation*}
        Entonces:
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}-\sum_{i=1 }^{n}\alpha_i\vec{u_i}}&\leq\norm{\vec{x}-\sum_{i=1 }^{n}\beta_i\vec{u_i}}+\norm{\sum_{ i=1}^{n}(\beta_i-\alpha_i)\vec{u_i}}\\
                &<\frac{\varepsilon}{2}+\sum_{ i=1}^{n}\abs{\beta_i-\alpha_i}\norm{\vec{u_i}}\\
                &=\frac{\varepsilon}{2}+\sum_{ i=1}^{n}\abs{\beta_i-\alpha_i}\\
                &<\frac{\varepsilon}{2}+\frac{\varepsilon}{2} \\
                &=\varepsilon\\
            \end{split}
        \end{equation*}
    \end{proof}

    \begin{obs}
        La prueba de la ida de la prueba anterior la hizo un alumno de la ESFM. Su nombre era \textit{Nunez Esquer}.
    \end{obs}

    \begin{cor}
        Un espacio hilbertiano de dimensión infinita es separable si y sólo si es linealmente isométrico a $l_2(\mathbb{N},\mathbb{K})$:
        \begin{equation*}
            l_2(\mathbb{N},\mathbb{K})=\left\{\left\{a_n \right\}_{n=1 }^\infty\Big|a_n\in\mathbb{K},\forall n\in\mathbb{N}\textup{ tal que }\sum_{ n=1}^{\infty}\abs{a_n}<\infty \right\}
        \end{equation*}
        provisto del producto escalar:
        \begin{equation*}
            \pint{\vec{a}}{\vec{b}}=\sum_{ n=1}^{\infty}a_n\conj{b_n}
        \end{equation*}
    \end{cor}

    \begin{proof}
        Es inmediato del teorema anterior.
    \end{proof}

    \begin{exa}
        En particular con el corolario anterior, como $L_2(S,\mathbb{K})$ es separable (siendo $S\subseteq\mathbb{R}^n$ medible), entonces
        \begin{equation*}
            L_2(S,\mathbb{K}) \equiv l_2(\mathbb{N},\mathbb{K})
        \end{equation*}
        Notemos que para tener la isometría lineal, necesitamos a la familia O.N. maximal $(f_n)_{ n\in\mathbb{N}}$ de tal suerte que $f\in\mathcal{L}_2(S,\mathbb{K})\mapsto \hat{f}=(\pint{f}{f_n})_{n\in\mathbb{N} }$ y la función inversa es $\varphi\in l_2(\mathbb{N},\mathbb{K})\mapsto f=\sum_{ n=1}^{\infty}\varphi(n)f_n=f$.
    \end{exa}

    \section{$L_\infty$ como dual de $L_1$}

    \begin{lema}[\textbf{Lema de los promedios}]
        Sea $S\subseteq\mathbb{R}^n$ medible, y sea $\cf{f}{S}{\mathbb{K}}$ medible e integrable en todo conjunto medible con medida finita dentro de $S$. Sea $F$ un conjunto cerrado en $\mathbb{K}$. Se supone que para todo $A\subseteq S$ medible tal que $0<m(A)<\infty$ se cumple lo siguiente:
        \begin{equation*}
            \frac{1}{m(A)}\int_A f\in F
        \end{equation*}
        Entonces, $f(x)\in F$ c.t.p. en $S$.
    \end{lema}

    \begin{proof}
        \begin{enumerate}
            \item Sea $C$ una bola cerrada en $\mathbb{K}\backslash F$ de centro $u$ y radio $r>0$ tal que $C\cap F=\emptyset$. Se afirma que $f^{-1}(C)$ es despreciable en $S$. En efecto, escriba
            \begin{equation*}
                f^{-1}(C)=\bigcup_{ k=1}^\infty\left[f^{-1}(C)\cap P_k \right]
            \end{equation*}
            donde $P_k=\left[-k,k\right]^n$, para todo $k\in\mathbb{N}$. Basta probar que
            \begin{equation*}
                A_k=f^{-1}(C)\cap P_k
            \end{equation*}
            es despreciable para todo $k\in\mathbb{N}$. Se tiene que $m(A_k)$ es finita. Si $m(A_k)>0$, entonces por la hipótesis se tiene que
            \begin{equation*}
                \frac{1}{m(A_k)}\int_{A_k}f\in F
            \end{equation*}
            Pero,
            \begin{equation*}
                \begin{split}
                    \abs{\frac{1}{m(A_k)}\int_{A_k}f-u}&=\frac{1}{m(A_k)}\abs{\int_{A_k}(f(x)-u)dx}\\
                    &\leq \frac{1}{m(A_k)}\int_{A_k}\abs{f(x)-u}dx\\
                    &\leq r m(A_k)\frac{1}{m(A_k)}\\
                    &=r\\
                \end{split}
            \end{equation*}
            por tanto, $\frac{1}{m(A_k)}\int_{A_k}f\in C$\contradiction, ya que $F\cap C=\emptyset$. Luego, $A_k$ tiene medida cero, así $f^{-1}(C)$ es despreciable.

            \item Resta probar que $f^{-1}(\mathbb{K}\backslash F)$ es despreciable. Como $F$ es cerrado, entonces para todo $x\in\mathbb{K}\backslash F$ existe una bola cerrada de centro $x$ y radio $r_x>0$ tal que
            \begin{equation*}
                F\cap C_x=\emptyset
            \end{equation*}
            Note que $\left(\mathring{C_x} \right)_{ x\in\mathbb{K}\backslash F}$ forman un recubrimiento abierto de $\mathbb{K}\backslash F$. Ya que $\mathbb{K}$, y por tanto $\mathbb{K}\backslash F$ es separable, existe un recubrimiento numerable $\left(\mathring{C_n} \right)_{ n\in\mathbb{K}\backslash F}$ de $\mathbb{K}\backslash F$ (por el T. de Lindelöf). Entonces:
            \begin{equation*}
                f^{-1}(\mathbb{K}\backslash F)=\bigcup_{ n=1}^\infty f^{-1}(\mathring{C_n})
            \end{equation*}
            es despreciable por (1).
        \end{enumerate}
        Por ambos incisos, se sigue que $f(x)\in F$ para casi todo $x\in S$.
    \end{proof}

    \begin{cor}
        Sea $S\subseteq\mathbb{R}^n$ medible y $\cf{f}{S}{\mathbb{K}}$ medible e integrable en todo subconjunto de $S$ con medida finita. Se supone que $\forall A\subseteq S$ con medida finita:
        \begin{equation*}
            \int_A f=0
        \end{equation*}
        entonces $f(x)=0$ para casi toda $x\in S$.
    \end{cor}

    \begin{proof}
        
    \end{proof}

    \begin{cor}
        Sean $S$ y $f$ como en el corolario anterior. Se supone que existe $b\geq 0$ tal que:
        \begin{equation*}
            \abs{\int_A f}\leq b m(A)
        \end{equation*}
        para todo $A\subseteq S$ con medida finita. Entonces, $\abs{f(x)}$ para casi toda $x\in S$.
    \end{cor}

    \begin{proof}
        
    \end{proof}

    \begin{theor}
        Sea $S\subseteq\mathbb{R}^n$ medible. Para cada $g\in\mathcal{L}_\infty(S,\mathbb{K})$ se define $\phi_g$ como la función de $\mathcal{L}_1(S,\mathbb{K})$ en $\mathbb{K}$ dada como sigue:
        \begin{equation*}
            \phi_g(f)=\int_S fg,\quad\forall f\in\mathcal{L}_1(S,\mathbb{K})
        \end{equation*}
        Entonces, $\phi_g$ es un funcional lineal continuo sobre $L_1(S,\mathbb{K})$ tal que $\norm{\phi_g}=\N{\infty}{g}$. Además, la aplicación $\phi:g\mapsto \phi_g$ es una isometría lineal de $L_\infty(S,\mathbb{K})$ sobre $L_1(S,\mathbb{K})^*$.
    \end{theor}

    \begin{proof}
        Note que nada cambia si se reemplazan $f$ o $g$ por funciones equivalentes (es decir que la norma de sus diferencias esa 0), luego es indistinto usar las notaciones $\mathcal{L}_1$ o $L_1$
        y $\mathcal{L}_\infty$ o $L_\infty$.

        Sea $g\in\mathcal{L}_\infty(S,\mathbb{K})$. Claramente $\phi_g$ es un funcional lineal sobre $L_1(S,\mathbb{K})$. Además, 
        \begin{equation*}
            \abs{\phi_g(f)}\leq\abs{\int_S fg}\leq\int_S\abs{f}\abs{g}\leq \N{\infty}{g}\N{1}{f}
        \end{equation*}
        por tanto, $\phi_g\in L_1(S,\mathbb{K})^*$, y $\norm{\phi_g}\leq\N{\infty}{g}$.

        Si $A\subseteq S$ es medible de medida finita, entonces
        \begin{equation*}
            \begin{split}
                \abs{\int_A g}=&\abs{\int_S g\chi_A}\\
                =&\abs{\phi_g(\chi_A)} \\
                \leq&\norm{\phi_g}\N{1}{\chi_A} \\
                =&\norm{\phi_g}m(A)\\
            \end{split}
        \end{equation*}
        por el corolario anterior se sigue que
        \begin{equation*}
            \abs{g(x)}\leq\norm{\phi_g},\textup{ para casi toda }x\in S
        \end{equation*}
        por ende, $\N{\infty}{g}\leq\norm{\phi_g}$. Finalmente, se sigue que $\norm{\phi_g}=\N{\infty}{g}$ (es decir que es isometría) y, claramente es lineal.

        \begin{enumerate}
            \item Probaremos que $\phi_g$ es suprayectiva bajo la hipótesis adicional de que $m(S)<\infty$. Sea $\varphi\in L_1(S,\mathbb{K})^*$. Como $m(S)<\infty$, entonces $L_2(S,\mathbb{K})\subseteq L_1(S,\mathbb{K})$ luego por reestricción $\varphi$ es un funcional lineal sobre $L_2(S,\mathbb{K})$.
            
            Recordemos que:
            \begin{equation*}
                \frac{\N{1}{f}}{m(S)}\leq\frac{\N{2}{f}}{m(S)^{1/2}},\quad\forall f\in\mathcal{L}_2(S,\mathbb{K})
            \end{equation*}
            Entonces:
            \begin{equation*}
                \begin{split}
                    \abs{\varphi(f)}&\leq\norm{\varphi}\N{1}{f}\\
                    &\leq\norm{\varphi}m(S)^{1/2}\N{2}{f},\quad\forall f\in\mathcal{L}_2(S,\mathbb{K})\\
                \end{split}
            \end{equation*}
            Así pues, la reestricción de $\varphi$ a $L_2(S,\mathbb{K})$ es lineal continua, es decir, $\varphi\in L_2(S,\mathbb{K})^*$. Por la autodualidad de $L_2(S,\mathbb{K})$, existe $g\in\mathcal{L}_2(S,\mathbb{K})$ tal que:
            \begin{equation}
                \varphi(f)=\int_Sfg,\quad\forall f\in\mathcal{L}_2(S,\mathbb{K})
            \end{equation}
            se afirma que $g\in \mathcal{L}_\infty(S,\mathbb{K})$. En efecto, sea $A\subseteq S$ medible. Entonces:
            \begin{equation*}
                \begin{split}
                    \abs{\int_Ag}&=\abs{\varphi(\chi_A)}\\
                    &\leq \norm{\varphi}\N{1}{\chi_A}\\
                    &=\norm{\varphi}m(A)\\
                \end{split}
            \end{equation*}
            luego, por el lema de los promedios:
            \begin{equation*}
                \abs{g(x)}\leq\norm{\varphi}
            \end{equation*}
            para casi todo $x\in S$. Por ende, $g\in \mathcal{L}_\infty(S,\mathbb{K})$.

            Para esta $g$ se tiene definido el correspondiente funcional lineal $\phi_g$ sobre $L_1(S,\mathbb{K})$. De acuerdo a la ecuación anterior se tiene que:
            \begin{equation*}
                \varphi(f)=\int_Sfg=\phi_g(f),\quad\forall f\in\mathcal{L}_2(S,\mathbb{K})
            \end{equation*}
            En particular, ambos operadores lineales continuos coinciden en el conjunto de las funciones escalonadas, denso en $L_1(S,\mathbb{K})$. Por el principio de ampliación de identidades, $\varphi$ y $\phi_g$ coinciden en todo $L_1(S,\mathbb{K})$.

            \item $\phi_g$ es suprayectiva en el caso general. Escriba $S$ en la forma:
            \begin{equation*}
                S=\bigcup_{ k=1}^\infty S_k
            \end{equation*}
            donde los $S_k$ son disjuntos a pares de medida finita. Sea $\varphi\in L_1(S,\mathbb{K})^*$ y $f\in\mathcal{L}_1(S,\mathbb{K})$. Para todo $k\in\mathbb{N}$ se define $f_k=f\chi_{S_k}$. Entonces:
            \begin{equation*}
                f=\sum_{ k=1}^{\infty}f_k
            \end{equation*}
            en todo punto de $S$ (por ser disjuntos los $S_k$). Además:
            \begin{equation*}
                \abs{\sum_{ k=1}^{m}f_k}\leq \abs{f},\quad\forall m\in\mathbb{N}
            \end{equation*}
            donde la última función es integrable e independiente de $m$.
            Por tanto, usando Lebesgue:
            \begin{equation*}
                \lim_{ m\rightarrow\infty}\N{1}{f-\sum_{ k=1}^{m}f_k}=0
            \end{equation*}
            como $\varphi$ es continua sobre $L_1(S,\mathbb{K})$,
            \begin{equation}
                \varphi(f)=\sum_{ k=1}^{\infty}\varphi(f_k)
            \end{equation}
            para cada $k\in\mathbb{N}$, sea $\varphi_k$ la reestricción de $\varphi$ a las funciones integrables en $L_1(S,\mathbb{K})$ nulas fuera de $S_k$. $\varphi_k$ se puede considerar entonces como un funcional lineal continuo sobre $L_1(S_k,\mathbb{K})$. Por (1) existe una función $g_k\in\mathcal{L}_\infty(S_k,\mathbb{K})$, que se identifica de manera natural con una función de $S$ en $\mathbb{K}$ nula fuera de $S_k$ tal que:
            \begin{equation*}
                \varphi_k=\phi_{g_k}
            \end{equation*}
            o sea:
            \begin{equation*}
                \varphi(f_k)=\phi_{ g_k}(f_k)=\int_{S_k}f_kg_k=\int_Sfg_k
            \end{equation*}
            (recuerde que $g_k$ es nula fuera de $S_k$). Sustituyendo en la ecuación anterior se tiene que:
            \begin{equation*}
                \varphi(f)=\sum_{ k=1}^{\infty}\int_Sfg_k
            \end{equation*}
            Defina $g=\sum_{ k=1}^{\infty}g_k$. Esta es una función de $S$ en $\mathbb{K}$ en todo punto de $S$. Es claro que esta función está bien definida ya que los $S_k$ son disjuntos a pares. Por (1):
            \begin{equation*}
                \N{\infty}{g_k}=\norm{\phi_{g_k}}=\norm{\varphi_k}\leq\norm{\varphi}
            \end{equation*}
            es decir, para casi toda $x\in S_k$, $\abs{g_k(x)}\leq\norm{\varphi}$. Luego, para casi toda $x\in S$,
            \begin{equation*}
                \abs{g(x)}\leq \norm{\varphi}
            \end{equation*}
            es decir que $g\in\mathcal{L}_\infty(S,\mathbb{K})$ (por ser los $S_k$ disjuntos). Observe que:
            \begin{equation*}
                fg=\sum_{ k=1}^{\infty}fg_k
            \end{equation*}
            en todo punto de $S$, Además, para todo $m\in\mathbb{N}$,
            \begin{equation*}
                \abs{\sum_{ k=1}^{m}fg_k}\leq\sum_{ k=1}^{m}\abs{f}\abs{g_k}\leq \abs{f}\sum_{ k=1}^{\infty}\abs{g_k}=\abs{f}\abs{g}
            \end{equation*}
            (otra vez, por ser los $S_k$ disjuntos), dónde la última función es integrable e independiente de $m$. Por el teorema de Lebesgue se sigue que:
            \begin{equation*}
                \varphi(f)=\sum_{ k=1}^{\infty}\int_Sfg_k=\int_Sf\sum_{ k=1}^{\infty}g_k=\int_Sfg
            \end{equation*}
            para esta $g\in\mathcal{L}_\infty(S,\mathbb{K})$ ya está definido el funcional lineal continuo $\phi_g$ como:
            \begin{equation*}
                \phi_g(f)=\int_Sfg
            \end{equation*}
            por tanto:
            \begin{equation*}
                \varphi(f)=\phi_g(f),\quad\forall f\in L_1(S,\mathbb{K})
            \end{equation*}
        \end{enumerate}
    \end{proof}

    \chapter{Convolución}
    
    Se sabe que el producto puntual de dos funciones integrables no necesariamente es una función integrable (por ejemplo, $f(x)=g(x)=\frac{1}{\sqrt{x}}\chi_{]0,1[}$). Sin embargo, es posible definir un auténtico producto en $L_1(\mathbb{R}^n,\mathbb{K})$ que sea compatible con la adición y el producto por escalares, con el cual $L_1(\mathbb{R}^n,\mathbb{K})$ sea un \textbf{álgebra de Banach conmutativa sin elemento identidad}. Tal operación se llama la \textbf{convolución}.

    \section{Preliminares}

    \begin{lema}
        Si $M$ es un subconjunto despreciable de $\mathbb{R}^n$, entonces $M\times\mathbb{R}^m$ es despreciable en $\mathbb{R}^{n+m}$.
    \end{lema}

    \begin{proof}
        Escriba a $\mathbb{R}^m$ como unión numerable de rectángulos acotados disjuntos. Basta probar que si $Q$ es un rectángulo acotado en $\mathbb{R}^m$, entonces $M\times Q$ es despreciable en $\mathbb{R}^{ n+m}$.

        Sea $\varepsilon>0$. Por definición de medida exterior, existe $\left\{P_\nu \right\}_{\nu=1}^\infty$ sucesión de rectángulos acotados tales que $M\subseteq \bigcup_{ \nu=1}^\infty P_\nu$ y:
        \begin{equation*}
            \sum_{ \nu=1}^{\infty}\textup{Vol}(P_\nu)<\varepsilon
        \end{equation*}
        Entonces, $\left\{P_\nu\times Q \right\}_{\nu=1}^\infty$ es una sucesión de rectángulos acotados en $\mathbb{R}^{n+m}$ tales que $M\times Q\subseteq \bigcup_{ \nu=1}^\infty P_\nu\times Q$, y
        \begin{equation*}
            \begin{split}
                \sum_{ \nu=1}^{\infty}\textup{Vol}(P_\nu\times Q)&=\textup{Vol}(Q)\cdot\sum_{ \nu=1}^{\infty}\textup{Vol}(P_\nu)\\
                &<\textup{Vol}(Q)\varepsilon\\
            \end{split}
        \end{equation*}
        (en caso de que $\textup{Vol}(Q)>0$), luego, el conjunto $M\times Q$ es despreciable, con lo cual el conjunto $M\times\mathbb{R}^m$ también lo es.
    \end{proof}

    \begin{mydef}
        Si $\cf{f}{\mathbb{R}^p}{\mathbb{K}}$ y $\cf{g}{\mathbb{R}^q}{\mathbb{K}}$, se define el \textbf{producto tensorial de $f$ y $g$} como la función: $\cf{f\otimes g}{\mathbb{R}^{ p+q}}{\mathbb{K}}$, dada por:
        \begin{equation*}
            f\otimes g(x,y)=f(x)g(y),\quad\forall (x,y)\in\mathbb{R}^{ p+q}
        \end{equation*}
    \end{mydef}

    \begin{propo}
        Si $\cf{f}{\mathbb{R}^p}{\mathbb{K}}$ y $\cf{g}{\mathbb{R}^q}{\mathbb{K}}$ son funciones medibles, entonces $\cf{f\otimes g}{\mathbb{R}^{ p+q}}{\mathbb{K}}$ es medible.
    \end{propo}

    \begin{proof}
        \begin{enumerate}
            \item Afirmamos que el resultado es cierto para funciones escalonadas $\cf{\varphi}{\mathbb{R}^p}{\mathbb{K}}$ y $\cf{\psi}{\mathbb{R}^q}{\mathbb{K}}$ escritas canónicamente como:
            \begin{equation*}
                \varphi=\sum_{ i=1}^{r}c_i\chi_{ P_i}\quad\textup{y}\quad\psi=\sum_{ j=1}^{s}d_j\chi_{ Q_j}
            \end{equation*}
            donde los $P_i$ y $Q_j$ son rectángulos acotados disjuntos. En este caso:
            \begin{equation*}
                \begin{split}
                    \varphi\otimes \psi(x,y)&=\sum_{ i=1}^{r}\sum_{ j=1}^{s}c_id_j\chi_{ P_i}(x)\chi_{ Q_j}(y)\\
                    &=\sum_{ i=1}^{r}\sum_{ j=1}^{s}c_id_j\chi_{ P_i\times Q_j}(x,y)\\
                \end{split}
            \end{equation*}
            la cual es una función escalonada en $\mathbb{R}^{ p+q}$, luego medible.
            \item En el caso general, se sabe que existen $\left\{ \varphi_\nu\right\}_{ \nu=1}^\infty$ en $\mathcal{E}(\mathbb{R}^p,\mathbb{K})$ y $\left\{\psi_\nu \right\}_{ \nu=1}^\infty$ en $\mathcal{E}(\mathbb{R}^q,\mathbb{K})$ y conjuntos despreciables $M\subseteq \mathbb{R}^p$, $N\subseteq \mathbb{R}^q$ tales que:
            \begin{equation*}
                \lim_{ \nu\rightarrow\infty}\varphi_\nu(x)=f(x),\quad\forall x\in \mathbb{R}^p\backslash M
            \end{equation*}
            y,
            \begin{equation*}
                \lim_{ \nu\rightarrow\infty}\psi_\nu(x)=g(x),\quad\forall x\in \mathbb{R}^q\backslash N
            \end{equation*}
            luego, se tiene que:
            \begin{equation*}
                \begin{split}
                    \lim_{ \nu\rightarrow\infty}\varphi_\nu\otimes \psi_\nu(x,y)&=\lim_{ \nu\rightarrow\infty}\varphi_\nu(x)\psi_\nu(y)\\
                    &=f(x)g(y)\\
                \end{split}
            \end{equation*}
            para todo $(x,y)\in \mathbb{R}^{ p+q}\backslash\left[M\times\mathbb{R}^q\cup\mathbb{R}^p\times N \right]$. Por el lema anterior se tine que $M\times\mathbb{R}^q\cup\mathbb{R}^p\times N$ es despreciable en $\mathbb{R}^{ p+q}$. Como $\varphi_\nu\otimes \psi_\nu$ son medibles para todo $\nu\in\mathbb{N}$, entonces $f\otimes g$ es medible.
        \end{enumerate}
    \end{proof}

    \begin{cor}
        Si $\cf{f}{\mathbb{R}^p}{\mathbb{K}}$ es medible, entonces $\cf{F}{\mathbb{R}^{ p+q}}{\mathbb{K}}$ dada como:
        \begin{equation*}
            F(x,y)=f(x),\quad\forall (x,y)\in\mathbb{R}^{ p+q}
        \end{equation*}
        es medible.
    \end{cor}

    \begin{proof}
        Es inmediata de la proposición anterior tomando a $f$ y $g=\chi_{\mathbb{R}^q}$.
        
    \end{proof}

    \begin{cor}
        Si $f\in\mathcal{L}_1(\mathbb{R}^p,\mathbb{K})$, $g\in\mathcal{L}_1(\mathbb{R}^q,\mathbb{K})$, entonces $f\otimes g\in \mathcal{L}_1(\mathbb{R}^{p+q},\mathbb{K})$ y:
        \begin{equation*}
            \int_{\mathbb{R}^{p}+q }f\otimes g=\int_{\mathbb{R}^p}f\cdot\int_{\mathbb{R}^q}g
        \end{equation*}
    \end{cor}

    \begin{proof}
        Es inmediato del teorema de Tonelli.
    \end{proof}

    \section{Convolución}

    \begin{mydef}
        Sean $\cf{f,g}{\mathbb{R}^n}{\mathbb{K}}$ funciones medibles. La \textbf{convolución de $f$ por $g$} se define como la función de $\mathbb{R}^n$ en $\mathbb{K}$ tal que:
        \begin{equation*}
            f*g(x)=\int_{\mathbb{R}^n}f(y)g(x-y)dx
        \end{equation*}
        para toda $x\in\mathbb{R}^n$ tal que la integral exista.
    \end{mydef}

\end{document}