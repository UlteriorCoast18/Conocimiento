\documentclass[12pt]{report}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{array}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[a4paper, margin = 1.5cm]{geometry}

%En esta parte se hacen redefiniciones de algunos comandos para que resulte agradable el verlos%

\renewcommand{\theenumii}{\roman{enumii}}

\def\proof{\paragraph{Demostración:\\}}
\def\endproof{\hfill$\blacksquare$}

\def\sol{\paragraph{Solución:\\}}
\def\endsol{\hfill$\square$}

%En esta parte se definen los comandos a usar dentro del documento para enlistar%

\newtheoremstyle{largebreak}
  {}% use the default space above
  {}% use the default space below
  {\normalfont}% body font
  {}% indent (0pt)
  {\bfseries}% header font
  {}% punctuation
  {\newline}% break after header
  {}% header spec

\theoremstyle{largebreak}

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!60!red!30
]{exa}{Ejemplo}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!50!blue!30
]{obs}{Observación}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{theor}{Teorema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{propo}{Proposición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{cor}{Corolario}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{lema}{Lema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    roundcorner=5pt,
    backgroundcolor = gray!30,
    hidealllines = true
]{mydef}{Definición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    roundcorner=5pt
]{excer}{Ejercicio}[section]

%En esta parte se colocan comandos que definen la forma en la que se van a escribir ciertas funciones%

\newcommand\divides{\ensuremath{\bigm|}}
\newcommand\cf[3]{\ensuremath{#1:#2\rightarrow#3}}
\newcommand\contradiction{\ensuremath{\#_c}}
\newcommand\abs[1]{\ensuremath{\big|#1\big|}}
\newcommand\norm[1]{\ensuremath{\|#1\|}}
\newcommand\ora[1]{\ensuremath{\vec{#1}}}
\newcommand\pint[2]{\ensuremath{\left(#1\big| #2\right)}}
\newcommand\conj[1]{\ensuremath{\overline{#1}}}

%recuerda usar \clearpage para hacer un salto de página

\begin{document}
    \setlength{\parskip}{5pt} % Añade 5 puntos de espacio entre párrafos
    \setlength{\parindent}{12pt} % Pone la sangría como me gusta
    \title{Espacios Hilbertianos}
    \author{Cristo Daniel Alvarado}
    \maketitle

    \tableofcontents %Con este comando se genera el índice general del libro%

    \chapter{Espacios Hilbertianos}
    
    %apostol de análisis matemático, lang de análisis real

    \section{Conceptos básicos. Proyecciones ortogonales}

    \begin{mydef}
        Sea $H$ un espacio vectorial sobre el campo $\mathbb{K}$. Decimos que $H$ es un \textbf{espacio prehilbertiano} si está dotado de una aplicación $(\ora{x},\ora{y})\mapsto \pint{\ora{x}}{\ora{y}}$ con las propiedades siguientes:
        \begin{enumerate}
            \item $\forall \ora{y}\in H$ fijo, $\ora{x}\mapsto\pint{\ora{x}}{\ora{y}}$ es una aplicación lineal de $H$ en $\mathbb{K}$, o sea
            \begin{equation*}
                \begin{split}
                    \pint{\ora{x_1}+\ora{x_2}}{\ora{y}}=&\pint{\ora{x_1}}{\ora{y}}+\pint{\ora{x_2}}{\ora{y}}\\
                    \pint{\alpha\ora{x}}{\ora{y}}=&\alpha\cdot\pint{\ora{x}}{\ora{y}}\\
                \end{split}
            \end{equation*}
            para todo $\ora{x},\ora{x_1},\ora{x_2}\in H$ y $\alpha\in\mathbb{K}$.
            \item $(\ora{y}\big| \ora{x})=\conj{\pint{\ora{x}}{\ora{y}}}$, para todo $\ora{x}\in H$.
            \item $\pint{\ora{x}}{\ora{x}}\geq0$, para todo $\ora{x}\in H$.
            \item $\pint{\ora{x}}{\ora{x}}=0$ si y sólo si $\ora{x}=0$.
        \end{enumerate}
    \end{mydef}

    \begin{obs}
        Si $\mathbb{K}=\mathbb{R}$, entonces 1) y 2) implican que $\forall \ora{x}\in H$ fijo, la aplicación $\ora{y}\mapsto\pint{\ora{x}}{\ora{y}}$ de $H$ en $\mathbb{R}$ es lineal. En este caso se dice que $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ es una \textbf{forma bilineal sobre $H$}.

        Si $\mathbb{K}=\mathbb{C}$, entonces
        \begin{equation*}
            \begin{split}
                \pint{\ora{x}}{\ora{y_1}+\ora{y_2}}=&\pint{\ora{x}}{\ora{y_1}}+\pint{\ora{x}}{\ora{y_2}}\\
                \pint{\ora{x}}{\alpha\ora{y}}=&\conj{\alpha}\pint{\ora{x}}{\ora{y}}\\
            \end{split}
        \end{equation*}
        Se dice que $\ora{y}\mapsto\pint{\ora{x}}{\ora{y}}$ es entonces \textbf{semilineal} y que $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ es \textbf{sesquilineal} ($1\frac{1}{2}$-lineal).

        La aplicación $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ se llama \textbf{producto escalar sobre $H$}.
    \end{obs}

    \begin{mydef}
        Para todo $\ora{x}\in H$ se define la \textbf{norma de $\ora{x}$} como: $\norm{\ora{x}}=\sqrt{\pint{\ora{x}}{\ora{x}}}$.
    \end{mydef}

    \begin{exa}
        Sea $H=\mathbb{K}^n$
        %El producto interior usual en ese espacio ya conocido y es prehilbertiano
    \end{exa}

    \begin{exa}
        Sea $S\subseteq\mathbb{R}^n$ medible y sea $H=L_2(S,\mathbb{K})$. Para todo $f,g\in H$ se define
        \begin{equation*}
            \pint{f}{g}=\int_Sf\conj{g}
        \end{equation*}
        La integral existe por Hölder con $p=p^*=2$. Este es un producto escalar sobre $H$ y, en este caso:
        \begin{equation*}
            \norm{f}=\left[\int_S\big|f\big|^2 \right]^{\frac{1}{2}}=\mathcal{N}_2(f),\quad \forall f\in H
        \end{equation*}
    \end{exa}

    \begin{exa}
        Sea $H=l_2(\mathbb{K})$ el espacio de sucesiones en $\mathbb{K}$ que son cuadrado sumables. Se sabe que $\ora{x}=(x_1,x_2,...)\in l_2(\mathbb{K})$ si y sólo si
        \begin{equation*}
            \sum_{i=1}^{\infty}|x_i|^2<\infty
        \end{equation*}
        $l_2(\mathbb{K})$ es un espacio prehilbertiano con el producto escalar:
        \begin{equation*}
            \pint{\ora{x}}{\ora{y}}=\sum_{i=1}^{\infty}x_i\conj{y_i}
        \end{equation*}
        donde la serie es convergente por Hölder. En este caso:
        \begin{equation*}
            \norm{\ora{x}}=\left[\sum_{i=1}^{\infty}\abs{x_i}^2\right]^{\frac{1}{2}}=\mathcal{N}_2(\ora{x}),\quad\forall\ora{x}\in l_2(\mathbb{K})
        \end{equation*}
    \end{exa}

    \begin{theor}[Desigualdad de Cauchy-Schwartz]
        Sea $H$ un espacio prehilbertiano. Entonces:
        \begin{enumerate}
            \item Se cumple la desigualdad de Cauchy-Schwartz:
            \begin{equation*}
                \abs{\pint{\vec{x}}{\vec{y}}}\leq \norm{\vec{x}}\norm{\vec{y}}, \quad\forall\vec{x},\vec{y}\in H
            \end{equation*}
            y, la igualdad se da si y sólo si los vectores son linealmente dependientes.
            \item Se cumple la desigualdad triangular:
            \begin{equation*}
                \norm{\vec{x}+\vec{y}}\leq\norm{\vec{x}}+\norm{\vec{y}},\quad\forall\vec{x},\vec{y}\in H
            \end{equation*}
            y la igualdad se da si y sólo si uno de los vectores es múltiplo no negativo del otro.
        \end{enumerate}
    \end{theor}

    \begin{proof}
        De 1): Se supondrá que $\mathbb{K}=\mathbb{C}$ (el caso en que sea $\mathbb{R}$ es similar y se deja como ejercicio).

        Sean $\vec{x},\vec{y}\in H$. En el caso de que alguno de los vectores sea $\vec{0}$, el resultado es inmediato (ambos miembros de la desigualdad son cero). Por lo cual, supongamos que ambos son no cero. Se tiene para todo $\lambda\in\mathbb{K}$ que
        \begin{equation}
            \begin{split}
                0\leq& \pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}\\
                =& \pint{\vec{x}}{\vec{x}}+\conj{\lambda}\pint{\vec{x}}{\vec{y}}+\lambda\pint{\vec{y}}{\vec{x}}+\lambda\conj{\lambda}\pint{\vec{y}}{\vec{y}}\\
                =& \pint{\vec{x}}{\vec{x}}+\conj{\lambda\pint{\vec{y}}{\vec{x}}}+\lambda\pint{\vec{y}}{\vec{x}}+\lambda\conj{\lambda}\pint{\vec{y}}{\vec{y}}\\
                &= \norm{\vec{x}}^2+2\Re{\lambda\pint{\vec{y}}{\vec{x}}}+\abs{\lambda}^2\norm{\vec{y}}^2\\
            \end{split}
        \end{equation}
        En particular, para
        \begin{equation*}
            \lambda(t)=\left\{\begin{array}{lcr}
                    t\frac{\pint{\vec{x}}{\vec{y}}}{\abs{\pint{\vec{x}}{\vec{y}}}} & \textup{si} & \pint{\vec{x}}{\vec{y}}\neq0\\
                    t & \textup{si} & \pint{\vec{x}}{\vec{y}}=0
                \end{array}
            \right.
        \end{equation*}
        con $t\in\mathbb{R}$, la desigualdad (1) se convierte en
        \begin{equation}
            0\leq\norm{\vec{x}}^2+2t\abs{\pint{\vec{y}}{\vec{x}}}+t^2\norm{\vec{y}}^2
        \end{equation}
        El trinomio anterior es mayor o igual a cero si y sólo si su discriminante:
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}^2-\norm{\vec{x}}^2\norm{\vec{y}}^2\leq0
        \end{equation*}
        es decir
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}\leq\norm{\vec{x}}\norm{\vec{y}}
        \end{equation*}
        Si $\abs{\pint{\vec{x}}{\vec{y}}}=\norm{\vec{x}}^2\norm{\vec{y}}^2$, entonces el trinomio en (2) tiene una raíz doble. Luego, existe $\lambda\in\mathbb{C}$ tal que
        \begin{equation*}
            \pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}=0
        \end{equation*}
        pero lo anterior solo sucede si y sólo si $\vec{x}+\lambda\vec{y}=0$, es decir si $\vec{x}$ y $\vec{y}$ son linealmente dependientes.

        De 2): Se tiene lo siguiente:
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}+\vec{y}}^2=&\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}\\
                =&\norm{\vec{x}}+2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}}^2\\
                \leq&\norm{\vec{x}}+2\abs{\pint{\vec{y}}{\vec{x}}}+\norm{\vec{y}}^2\\
                \leq &\norm{\vec{x}}+2\norm{\vec{x}}\norm{\vec{y}}+\norm{\vec{y}}^2\\
                =& (\norm{\vec{x}}+\norm{\vec{y}})^2\\
            \end{split}
        \end{equation*}
        lo cual implica la desigualdad que se quiere probar. Ahora, la igualdad se cumple si y sólo si
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}=\Re\pint{\vec{x}}{\vec{y}}\textup{ y }\abs{\pint{\vec{x}}{\vec{y}}}=\norm{\vec{x}}\norm{\vec{y}}
        \end{equation*}
        la primera igualdad implica que $\pint{\vec{x}}{\vec{y}}$ es real (en particular, $\geq0$ por el valor absoluto) y la segunda implica que $\vec{x}$ y $\vec{y}$ son linealmente dependientes. Es decir, si y sólo si un vector es multiplo no negativo del otro.
    \end{proof}

    Se concluye del teorema anterior que $\norm{\cdot}$ es una norma sobre $H$. En lo sucesivo se consdierará a $H$ como espacio normado dotado de esta norma.

    \begin{propo}
        La aplicación $(\vec{x},\vec{y})\mapsto\pint{\vec{x}}{\vec{y}}$
        es una función continua del espacio normado producto $H\times H$ en $\mathbb{K}$.
    \end{propo}

    \begin{proof}
        Sean $\vec{x},\vec{y}\in H$ y, $\left\{\vec{x_n} \right\}_{n=1}^\infty$ y $\left\{\vec{y_n} \right\}_{n=1}^\infty$ dos sucesiones que convergen a $\vec{x}$ y $\vec{y}$, respectivamente. Se probará que $\left\{\pint{\vec{x_n}}{\vec{y_n}} \right\}_{n=1}^\infty$ converge a $\pint{\vec{x}}{\vec{y}}$ en $\mathbb{K}$.
        Se tiene que
        \begin{equation}
            \begin{split}
                \abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}
                \leq&\abs{\pint{\vec{x}-\vec{x_n}}{\vec{y}}}+\abs{\pint{\vec{x_n}}{\vec{y}-\vec{y_n}}}\\
                \leq&\norm{\vec{x}-\vec{x_n}}\norm{\vec{y}}+\norm{\vec{x_n}}\norm{\vec{y}-\vec{y_n}}\\
            \end{split}
        \end{equation}
        para todo $n\in\mathbb{N}$. Como $\left\{\vec{x_n} \right\}$ es convergente, es acotada. Luego existe $M>0$ tal que
        \begin{equation*}
            \norm{\vec{x_n}}\leq M,\quad\forall n\in\mathbb{N}
        \end{equation*}
        Se sigue de (3) que
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}
            \leq\norm{\vec{x}-\vec{x_n}}\norm{\vec{y}}+M\norm{\vec{y}-\vec{y_n}}
        \end{equation*}
        y, por ende
        \begin{equation*}
            \lim_{n\rightarrow\infty}\abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}=0
        \end{equation*}
        con lo que se tiene el resultado.
    \end{proof}

    \begin{mydef}
        Decimos que un espacio prehilbertiano se llama \textbf{Hilbertiano}, si la norma $\norm{\cdot}$ hace de él un espacio normado completo (o sea, un espacio normado de Banach).
    \end{mydef}

    \begin{exa}
        Los espacios $L_2(S,\mathbb{K})$, $l_2(\mathbb{K})$ y todo espacio prehilbertiano de dimensión finita ($\mathbb{K}^n$) son hilbretianos (ya que, todo espacio prehilbertiano de dimensión finita es isomorfo a $\mathbb{R}^k$, para algún $k\in\mathbb{N}$).
    \end{exa}

    De ahora en adelante, $H$ denotará siempre a un espacio prehilbertiano (a menos que se indique lo contrario).

    \begin{mydef}
        Sean $\vec{x},\vec{y}\in H$. Se dice que \textbf{$\vec{x}$ y $\vec{y}$ son ortogonales} y se escribe $\vec{x}\perp\vec{y}$, si $\pint{\vec{x}}{\vec{y}}=0$.
    \end{mydef}

    \begin{obs}
        La condición $\vec{x}\perp\vec{y}$ para todo $\vec{x}\in H$ implica que $\vec{y}=\vec{0}$, pues en particular $\pint{\vec{y}}{\vec{y}}=0\Rightarrow\vec{y}=\vec{0}$.
    \end{obs}

    \begin{theor}[Teorema de Pitágoras]
        Si $\left(\vec{x_1},\dots,\vec{x_n} \right)$ es un sistema de vectores ortogonales (a pares), entonces
        \begin{equation*}
            \norm{\vec{x_1}+\dots+\vec{x_n}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2
        \end{equation*}
    \end{theor}

    \begin{proof}
        Se procederá por inducción sobre $n$. Veamos el caso $n=2$. En este caso, veamos que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x_1}+\vec{x_2}}^2=&\pint{\vec{x_1}+\vec{x_2}}{\vec{x_1}+\vec{x_2}}\\
                =&\norm{\vec{x_1}}^2+\pint{\vec{x_1}}{\vec{x_2}}+\pint{\vec{x_2}}{\vec{x_1}}+\norm{\vec{x_2}}^2\\
                =&\norm{\vec{x_1}}^2+\norm{\vec{x_2}}^2\\
            \end{split}
        \end{equation*}
        Suponga que el resultado se cumple para $n\geq2$. Sea $\vec{x_1},...\vec{x_{n+1}}\in H$ un sistema de vectores ortogonales. Observemos que
        \begin{equation*}
            \begin{split}
                \pint{\vec{x_1}+\cdots+\vec{x_n}}{\vec{x_{n+1}}}&=\pint{\vec{x_1}}{\vec{x_{n+1}}}+\cdots+\pint{\vec{x_n}}{\vec{x_{n+1}}}\\
                &=0+\cdots+0\\
                &=0\\
            \end{split}
        \end{equation*}
        por lo cual, $x_{n+1}\perp\vec{x_1}+\cdots+\vec{x_n}$. Por el caso $n=2$ se sigue que:
        \begin{equation*}
            \norm{\vec{x_1}+\cdots+\vec{x_{n+1}}}^2=\norm{\vec{x_1}+\cdots+\vec{x_n}}^2+\norm{\vec{x_{n+1}}}^2
        \end{equation*}
        Pero, por hipótesis de inducción:
        \begin{equation*}
            \norm{\vec{x_1}+\dots+\vec{x_n}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2
        \end{equation*}
        Por lo cual:
        \begin{equation*}
            \norm{\vec{x_1}+\cdots+\vec{x_{n+1}}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2+\norm{\vec{x_{n+1}}}^2
        \end{equation*}
        Aplicando inducción se sigue el resultado.
    \end{proof}

    \begin{propo}[Identidad del paralelogramo]
        Para todo $\vec{x},\vec{y}\in H$ se cumple la identidad del paralelogramo:
        \begin{equation*}
            \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2=2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)
        \end{equation*}
    \end{propo}

    \begin{proof}
        Sean $\vec{x},\vec{y}\in H$. Veamos que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2
                =&\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}+\pint{\vec{x}-\vec{y}}{\vec{x}-\vec{y}}\\
                =&\norm{\vec{x}}^2+2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}^2}+\norm{\vec{x}}^2-2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}}^2\\
                =&2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)\\
            \end{split}
        \end{equation*}
    \end{proof}

    Este resultado anterior es importante, pues en espacios donde la norma no venga de un producto escalar, no necesariamente se cumple la igualdad.

    \begin{exa}
        Los vectores $\chi_{[0,1]}$ y $\chi_{[1,2]}$ son ortogonales en $L_2(\mathbb{R},\mathbb{R})$ (es inmediato del producto escalar en $L_2(\mathbb{R},\mathbb{R})$).
    \end{exa}

    \begin{exa}
        Los vectores $\sen$ y $\cos$ son ortogonales en $L_2([-\pi,\pi[,\mathbb{R})$. En efecto, veamos que
        \begin{equation*}
            \pint{\sen}{\cos}=\int_{-\pi}^{\pi}\sen x\cos xdx=\frac{1}{2}\int_{-\pi}^{\pi}\sen 2xdx=0
        \end{equation*}
        En particular, por Pitágoras se tiene que
        \begin{equation*}
            \int_{-\pi}^{\pi}\abs{\sen x+\cos x}^2dx=\int_{-\pi}^{\pi}\abs{\sen x}^2dx+\int_{-\pi}^{\pi}\abs{\cos x}^2dx
        \end{equation*}
    \end{exa}

    \begin{exa}
        Si $\vec{x}=(1,1,\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{3}{3},...)$ y $\vec{x}=(1,-1,\frac{1}{2},-\frac{1}{2},\frac{1}{2},-\frac{3}{3},)$ son elementos de $l_2(\mathbb{R})$, se tiene que $\vec{x}\perp\vec{y}$. En efecto, veamos que
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{y}}=&\sum_{n=1}^{\infty}x_n\conj{y_n}\\
                =&\lim_{n\rightarrow\infty}s_n
            \end{split}
        \end{equation*}
        donde $\left\{s_n\right\}_{n=1}^\infty$ es la sucesión de sumas parciales, siendo $s_{2m}=0$ y $s_{2m-1}=\frac{1}{m}$. Por lo cual
        \begin{equation*}
            \pint{\vec{x}}{\vec{y}}=\lim_{n\rightarrow\infty}s_n=0
        \end{equation*}
    \end{exa}

    \begin{theor}
        Sea $M$ un subespacio de un espaco prehilbertiano $H$ y sea $\vec{x}\in H$.
        \begin{enumerate}
            \item Suponiendo que existe $\vec{x_0}\in M$ tal que $\vec{x}-\vec{x_0}\perp M$, es decir que $\vec{x}-\vec{x_0}\perp\vec{y}$, para todo $\vec{y}\in M$, se tiene
            \begin{equation*}
                \norm{\vec{x}-\vec{x_0}}<\norm{\vec{x}-\vec{y}},\quad\forall\vec{y}\in M,\vec{y}\neq\vec{x_0}
            \end{equation*}
            Así pues, si existe $\vec{x_0}$, tal vector es único y es llamado \textbf{la proyección ortogonal de $\vec{x}$ sobre $M$}. Además
            \begin{equation*}
                d(\vec{x},M)^2=\norm{\vec{x}-\vec{x_0}}^2=\norm{\vec{x}}^2-\norm{\vec{x_0}}^2
            \end{equation*}
            \item Recíprocamente, si existe un $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$, entonces $\vec{x_0}$ es la proyección ortogonal de $\vec{x}$ sobre $M$. En particular, si $\vec{x}\in M$ entonces $\vec{x}=\vec{x_0}$, es decir que $\vec{x}$ es su propia proyección ortogonal sobre $M$.
        \end{enumerate}
    \end{theor}

    \begin{proof}
        De 1): Suponga que existe $\vec{x_0}\in M$ con la condición especificada. Sea $\vec{y}\in M$ distinto de $\vec{x_0}$. Como $\vec{x_0}-\vec{x}\perp\vec{x_0}-\vec{y}$, por el Teorema de Pitágoras se tiene que
        \begin{equation}
            \norm{\vec{x}-\vec{y}}^2=\norm{\vec{x}-\vec{x_0}}^2+\vec{x_0}-\vec{y}^2>\norm{\vec{x}-\vec{x_0}}^2
        \end{equation}
        pues $\vec{x_0}\neq \vec{y}$. Así pues, $\vec{x_0}$ es único. Además $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Aplicando la ecuación 4) con $\vec{y}=\vec{0}$ se tiene que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}}^2=&\norm{\vec{x}-\vec{x_0}}^2+\norm{\vec{x_0}}^2\\
                \Rightarrow d(\vec{x},M)^2=\norm{\vec{x}-\vec{x_0}}^2=&\norm{\vec{x}}^2-\norm{\vec{x_0}}^2\\
            \end{split}
        \end{equation*}
        
        De 2) Si existe $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$, entonces $\vec{x_0}$ debe ser la proyección ortogonal de $\vec{x}$ sobre $M$. En efecto, para todo $\vec{y}\in M$ y para todo $\lambda\in\mathbb{K}$ se tiene
        \begin{equation}
            \begin{split}
                \norm{\vec{x}-(\vec{x_0}+\lambda\vec{y})}^2\geq&\norm{\vec{x}-\vec{x_0}}^2\\
                \Rightarrow \norm{(\vec{x}-\vec{x_0})-\lambda\vec{y}}^2\geq&\norm{\vec{x}-\vec{x_0}}^2\\
                \Rightarrow \norm{\vec{x}-\vec{x_0}}^2+2\Re[\conj{\lambda} \pint{\vec{x}-\vec{x_0}}{\vec{y}}]+\abs{\lambda}^2\norm{\vec{y}}^2\geq&\norm{\vec{x}-\vec{x_0}}^2\\
                \Rightarrow -2\Re[\lambda\pint{\vec{x}-\vec{x_0}}{\vec{y}}]+\abs{\lambda}^2\norm{\vec{y}}^2\geq& 0\\
            \end{split}
        \end{equation}
        en particular, para $\lambda=t\pint{\vec{x}-\vec{x_0}}{\vec{y}}$, con $t\in\mathbb{R}$, la ecuación anterior se transforma en:
        \begin{equation*}
            \begin{split}
                \abs{\pint{\vec{x}-\vec{x_0}}{\vec{y}}}^2\left[-2t+t^2\norm{\vec{y}}\right]\geq0 \\
            \end{split}
        \end{equation*}
        para todo $t\in\mathbb{R}$. Esto exige que $\pint{\vec{x}-\vec{x_0}}{\vec{y}}=0$, o sea que $\vec{x}-\vec{x_0}\perp \vec{y}$.
    \end{proof}

    Dado un subespacio $M$ de un espacio prehilbertiano $H$ un vector $\vec{x}\in H$, puede no existir la proyección ortogonal de $\vec{x}$ sobre $M$. Esto motiva la siguiente definición:

    \begin{mydef}
        Un subespacio $M$ de $H$ se dice que es \textbf{distinguido} si para cada $\vec{x}\in H$ existe la proyección ortogonal de $\vec{x}$ sobre $M$.        
    \end{mydef}

    \begin{exa}
        El subespacio $\phi_0$ de las sucesiones eventualmente constantes de valor cero es un subespacio del espacio hilbretiano $l_2(\mathbb{R})$. Sea $M$ el subespacio de $\phi_0$ dado como sigue:
        \begin{equation*}
            M=\left\{\vec{x}\in\phi_0|x_2=0 \right\}
        \end{equation*}
        Sea $\vec{x}=\left(0,\frac{1}{2^{0/2}},\frac{1}{2^{1/2}},\frac{1}{2},\frac{1}{2^{3/2}},... \right)$. Se tiene que:
        \begin{equation*}
            \begin{split}
                d(\vec{x},M)&=\inf_{\vec{y}\in M}\left\{\norm{\vec{x}-\vec{y}} \right\}\\
                &=\inf_{\vec{y}\in M}\left\{\left[\abs{y_1}+\sum_{i=2}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 \right]^{1/2} \right\}\\
                &=1\\
                &=\inf_{\vec{y}\in M}\left\{\left[\abs{y_1}+1+\sum_{i=3}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 \right]^{1/2} \right\}\\
                &=1\\
            \end{split}
        \end{equation*}
        (pues, $y_2=0$). Pero $\norm{\vec{x}-\vec{y}}>1$, para todo $\vec{y}\in M$. En efecto, sea $\vec{y}\in M$, entonces $\exists m\in\mathbb{N}$ tal que si $k\geq m$ se tiene que $y_k=0=y_2$. Veamos que

        \begin{equation*}
            \begin{split}
                \norm{\vec{x}-\vec{y}}&=\left[\abs{y_1}+1+\sum_{i=3}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 \right]^{1/2}\\
                &\geq \left[1+\sum_{i=3}^{k-1} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 + \sum_{i=k}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 \right]^{1/2}\\
                &= \left[1+\sum_{i=3}^{k-1} \abs{\frac{1}{2^{(i-2)/2}}-y_i}^2 + \sum_{i=k}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}}^2 \right]^{1/2}\\
                &\geq \left[1+\sum_{i=k}^{\infty} \abs{\frac{1}{2^{(i-2)/2}}}^2 \right]^{1/2}\\
                &> \left[1\right]^{1/2}\\
                &> 1\\
            \end{split}
        \end{equation*}
        
        Luego no existe $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Por lo tanto, no existe la proyección ortogonal de $\vec{x}$ sobre $M$ (es decir, $M$ no es distinguido).

        Sin embargo, si $\vec{x}=(1,1,0,...)\in l_2(\mathbb{R})$, entonces si existe la proyección ortogonal de $\vec{x}$ sobre $M$, pues
        \begin{equation*}
            \begin{split}
                d(\vec{x},M)&=\inf_{\vec{y}\in M}\left\{\norm{\vec{x}-\vec{y}} \right\}\\
                &=\inf_{\vec{y}\in M}\left\{\left[\abs{1-y_1}^2+1^2+\sum_{i=3}^{\infty} \abs{y_i}^2 \right]^{1/2} \right\}\\
                &=1\\
            \end{split}
        \end{equation*}
        y $\norm{\vec{x}-\vec{e_1}}=1$, donde $\vec{e_1}\in M$. Por tanto, $\vec{e_1}$ es la proyección ortogonal de $\vec{x}$ sobre $M$.
    \end{exa}

    \begin{theor}
        Si $M$ es un subespacio completo de un espacio prehilbertiano, entonces $M$ es distinguido. En particular todo subespacio de dimensión finita de un espacio prehilbertiano siempre es distinguido. 
    \end{theor}

    \begin{proof}
        Sea $\vec{x}\in H$. Se debe probar que existe un $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Sea $a=d(\vec{x},M)$. Por propiedades del ínfimo existe una sucesión $\left\{\vec{y_\nu} \right\}_{\nu=1}^{\infty}$ tal que
        \begin{equation}
            \lim_{\nu\rightarrow\infty}\norm{\vec{x}-\vec{y_\nu}}=a
        \end{equation}
        Sean $\nu,\mu\in\mathbb{N}$ arbitrarios. Por la identidad del paralelogramo se tiene que
        \begin{equation*}
            \begin{split}
                2\left(\norm{\vec{x}-\vec{y_\nu}}^2+\norm{\vec{x}-\vec{y_\mu}}^2 \right)
                &=\norm{\vec{y_\nu}-\vec{y_\mu}}^2+\norm{2\vec{x}-(\vec{y_\nu}+\vec{y_\mu})}^2\\
                &=\norm{\vec{y_\nu}-\vec{y_\mu}}^2+4\norm{\vec{x}-\frac{\vec{y_\nu}+\vec{y_\mu}}{2}}^2\\
                &\geq\norm{\vec{y_\nu}-\vec{y_\mu}}^2+4a^2\\
            \end{split}
        \end{equation*}
        de donde
        \begin{equation*}
            \norm{\vec{y_\nu}-\vec{y_\mu}}^2\leq2\left(\norm{\vec{x}-\vec{y_\nu}}^2+\norm{\vec{x}-\vec{y_\mu}}^2 \right)-4a^2
        \end{equation*}
        Tomando límite cuando $\nu,\mu$ tienden a infinito y por (6), se tiene que
        \begin{equation*}
            \lim_{\nu,\mu\rightarrow\infty}\norm{\vec{y_\nu}-\vec{y_\mu}}^2=0
        \end{equation*}
        por tanto, $\left\{\vec{y_\nu} \right\}_{\nu=1}^{\infty}$ es de Cauchy. Por ser $M$ completo, existe $\vec{x_0}\in M$ tal que $\lim_{\nu\rightarrow\infty}\vec{y_\nu}=\vec{x_0}$. Por (6):
        \begin{equation*}
            a=\lim_{\nu\rightarrow\infty}\norm{\vec{x}-\vec{y_\nu}}=\norm{\vec{x}-\vec{x_0}}
        \end{equation*}
    \end{proof}

    \begin{exa}
        ¿Es distinguido el subespacio de $L_2(\mathbb{R},\mathbb{R})$ dado por:
        \begin{equation*}
            M=\left\{f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})| f(x)=0\textup{ c.t.p. en }[1,2] \right\}
        \end{equation*}
        ?
        
        La respuesta es que sí, ya que $M$ es cerrado. En efecto, sea $\left\{f_\nu\right\}_{\nu=1}^{\infty}$ una sucesión en $M$ convergente en promedio cuadrático a una $f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})$, es decir:
        \begin{equation*}
            \lim_{\nu\rightarrow\infty}\mathcal{N}_2(f_\nu-f)=0
        \end{equation*}
        Se sabe que existe una subsucesión de $\left\{f_\nu\right\}_{\nu=1}^{\infty}$, digamos $\left\{f_{\alpha(\nu)}\right\}_{\nu=1}^{\infty}$ que converge c.t.p. a $f$ en $\mathbb{R}$. Como $f_{\alpha(\nu)}=0$ c.t.p. en $[1,2]$, entonces $f=0$ c.t.p. en $[1,2]$, es decir $f\in M$. Por tanto, $M$ es distinguido.

        Ahora, dada $f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})$, ¿Cuál será la proyección ortogonal de $f$ sobre $M$? Es claro que
        \begin{equation*}
            f_0=f\cdot\chi_{\mathbb{R}\backslash[1,2]}\in M
        \end{equation*}
        es la proyección ortogonal de $f$ sobre $M$, y además $f-f_0\perp M$.
    \end{exa}

    \begin{mydef}
        Sea $S\subseteq H$ un conjunto arbitrario. Para este conjunto se define
        \begin{equation*}
            S^{\perp}=\left\{\vec{x}\in H|\vec{x}\perp\vec{s},\forall\vec{s}\in S \right\}
        \end{equation*}
        Es claro que $S^\perp$ es un subespacio cerrado de $H$.
    \end{mydef}

    \begin{sol}
        En efecto, si $\left\{\vec{x_\nu} \right\}$ es una sucesión en $S^\perp$ que converge a $\vec{x}\in H$, entonces
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{s}}=\lim_{\nu\rightarrow\infty}\pint{\vec{x_\nu}}{\vec{y}}=0,\quad\forall\vec{s}\in S
            \end{split}
        \end{equation*}
        por continuidad y para todo $\vec{s}\in S$. Luego $\vec{x}\in S^\perp$. Otra forma es definiendo una función $\cf{T_{\vec{s}}}{H}{\mathbb{K}}$ como
        \begin{equation*}
            T_{\vec{s}}(\vec{x})=\pint{\vec{x}}{\vec{s}},\quad\forall\vec{x}\in H
        \end{equation*}
        Entonces
        \begin{equation*}
            S^\perp=\bigcap_{\vec{s}\in S}\ker T_{\vec{s}}
        \end{equation*}
        Como $T_{\vec{s}}$ es lineal continua para todo $\vec{s}\in S$, entonces se sigue que $S^\perp$ es cerrado.
    \end{sol}

    \begin{propo}
        Un subespacio $M$ de un espacio prehilbertiano $H$ es distinguido si y sólo si
        \begin{equation*}
            H=M\oplus M^\perp
        \end{equation*}
    \end{propo}

    \begin{proof}
        $\Rightarrow$): Suponga que $M$ es distinguido. Como $M\cap M^\perp=\left\{\vec{0} \right\}$, para probar que $H=M\oplus M^\perp$, basta probar que es la suma simplemente, es decir que $H=M+M^\perp$.
        
        Sea $\vec{x}\in H$, como $M$ es distinguido entonces existe $\vec{x_1}\in M$ tal que $\vec{x}-\vec{x_1}\perp M$, tomando $\vec{x_2}=\vec{x}-\vec{x_1}$ se tiene que $\vec{x_2}\in M^\perp$. Además $\vec{x}=\vec{x_1}+\vec{x_2}$, lo que prueba el resultado.

        $\Leftarrow$): Suponga que $H=M\oplus M^\perp$. Hay que probar que $M$ es distinguido. Sea $\vec{x}\in H$ arbitrario. Por hipótesis existen $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ únicos tales que $\vec{x}=\vec{x_1}+\vec{x_2}$. Se afirma que $\vec{x_1}$ es la proyección ortogonal de $\vec{x}$ sobre $M$. 
        
        En efecto,
        \begin{equation*}
            \vec{x}-\vec{x_1}=\vec{x_2}\in M^\perp
        \end{equation*}
        pero $\vec{x_2}\perp M$, por tanto $\vec{x_1}$ es la proyección ortogonal.
    \end{proof}

    \begin{exa}
        Sea $M=\left\{x\in l_2(\mathbb{R})\big| x(2n)=0, \forall n\in\mathbb{N} \right\}$. Afirmamos que $M$ es distinguido, para lo cual basta ver que este subespacio es cerrado (por ser $l_(\mathbb{R})$ completo, es decir por ser un espacio Hilbertiano).

        Sea $\left\{\vec{x_n} \right\}$ una sucesión en $l_2(\mathbb{R})$ que converge a $\vec{x}\in l_2(\mathbb{R})$, es decir
        \begin{equation}
            \begin{split}
                \lim_{n\rightarrow \infty}\mathcal{N}_2(\vec{x}-\vec{x_n})=&0\\
                \lim_{k\rightarrow \infty}(\vec{x}(2k)-\vec{x_n}(2k))=&0,\quad \forall k\in\mathbb{N}\\
                \Rightarrow \vec{x}(2k)=&0\quad\forall x\in\mathbb{N}\\ 
            \end{split}
        \end{equation}
        por lo cual, $\vec{x}\in M$. Luego, $M$ es cerrado. Dado que $M$ es distinguido, si $\vec{x}\in l_2(\mathbb{R})=M\oplus M^\perp$, se tiene
        \begin{equation*}
            \vec{x}=\vec{x_1}+\vec{x_2}
        \end{equation*}
        donde $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ son únciso y están dados por:
        \begin{equation*}
            \begin{split}
                \vec{x_1}=&(\vec{x}(1),0,\vec{x}(3),...)\\
                \vec{x_2}=&(0,\vec{x}(2),0,\vec{x}(4),...)\\
            \end{split}
        \end{equation*}
    \end{exa}

    \begin{cor}
        Si $M$ es un subespacio distinguido de $H$, entonces $M^\perp$ es también un subespacio distinguido.
    \end{cor}

    \begin{proof}
        Se probará que cualquier $\vec{x}\in H$ posee una proyección ortogonal sobre $M^\perp$. Por el teorema anterior:
        \begin{equation*}
            \vec{x}=\vec{x_1}+\vec{x_2}
        \end{equation*}
        con $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ únicos. Luego, $\vec{x}-\vec{x_2}=\vec{x_1}\in M$, por lo que cualquier vector en $M^\perp$ se cumple que $\vec{x_1}\perp \vec{y}$, para todo $\vec{y}\in M$, es decir que $\vec{x_2}$ es la proyecicón ortogonal de $\vec{x}$ sobre $M^\perp$.
    \end{proof}

    \begin{propo}
        Si $M$ es un subespacio distinguido de $H$, entonces $M^{\perp\perp}=M$.
    \end{propo}

    \begin{proof}
        Claramente $M\subseteq M^{\perp\perp}$. Ahora, sea $\vec{x}\in M^{\perp\perp}$, por el teorema $\vec{x}=\vec{x_1}+\vec{x_2}$ donde $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ únicos.

        Se tiene que
        \begin{equation*}
            0=\pint{\vec{x}}{\vec{x_2}}=\pint{\vec{x}}{\vec{x_1}}+\pint{\vec{x_2}}{\vec{x_2}}=\pint{\vec{x_2}}{\vec{x_2}}
        \end{equation*}
        es decir que $\vec{x_2}=\vec{0}$. Por tanto, $\vec{x}\in M$.

        Luego, $M=M^{\perp\perp}$.
    \end{proof}

    \begin{cor}
        En un espacio hilbertiano $H$, un subespacio es distinguido si y sólo si es cerrado.
    \end{cor}

    \begin{proof}
        Si es cerrado es inmediato que es distinguido. Ahora, si es distinguido entonces es cerrado, pues por el corolario anterior $M=M^{\perp\perp}$, donde $M^{\perp\perp}$ es cerrado por ser intersección arbitraria de cerrrados, luego $M$ es cerrado.
    \end{proof}

    \begin{propo}
        Sea $H$ un espacio prehilbertiano y sea $M$ un subespacio distinguido de $H$ (que no se reduce al $\left\{\vec{0} \right\}$). $\forall\vec{x}\in H$ sea $\pi(\vec{x})$ la \textbf{proyección ortogonal de $\vec{x}$ sobre $M$}.

        Entonces $\cf{\pi}{H}{M}$ es lineal continua y tal que $\norm{\pi}=1$. Además, $\pi\circ\pi=\pi$, y $\pint{\pi(\vec{x})}{\vec{y}}=\pint{\vec{x}}{\pi(\vec{y})}$.
    \end{propo}

    \begin{proof}
        Sea $\vec{x}\in H$ y $\alpha\in\mathbb{K}$. Si $\alpha=0$, el resultado es inmediato. Suponga que $\alpha\neq 0$. Se tiene que $\alpha\pi(\vec{x})\in M$ por ser subespacio, y
        \begin{equation*}
            \alpha\vec{x}-\alpha\pi(\vec{x})=\alpha(\vec{x}-\pi(\vec{x}))\perp M
        \end{equation*}
        Luego, $\alpha\pi(\vec{x})$ es una proyección ortogonal de $\alpha\vec{x}$ sobre $M$, pero por unicidad de la proyección ortogonal, se tiene que $\pi(\alpha\vec{x})=\alpha\pi(\vec{x})$.

        Ahora, sean $\vec{x},\vec{y}\in H$. Entonces, $\pi(\vec{x})+\pi(\vec{y})\in M$ y:
        \begin{equation*}
            (\vec{x}+\vec{y})-(\pi(\vec{x})+\pi(\vec{x}))=(\vec{x}-\pi(\vec{x}))+(\vec{y}-\pi(\vec{y}))\perp M
        \end{equation*}
        es decir que $\pi(\vec{x})+\pi(\vec{y})$ es una proyección ortogonal de $\vec{x}+\vec{y}$ sobre $M$. Por unicidad,
        \begin{equation*}
            \pi(\vec{x}+\vec{y})=\pi(\vec{x})+\pi(\vec{y})
        \end{equation*}
        Por tanto, $\pi$ es lineal.


        Ahora, veamos que es continua. Se sabe que:
        \begin{equation*}
            \begin{split}
                d(\vec{x},M)^2&=\norm{\vec{x}-\pi(\vec{x})}^2\\
                &=\norm{\vec{x}}^2-\norm{\pi(\vec{x})}^2\\
                \Rightarrow \norm{\pi(\vec{x})}^2&=\norm{\vec{x}}^2-\norm{\vec{x}-\pi(\vec{x})}^2\\
                &\leq\norm{\vec{x}}^2\\
            \end{split}
        \end{equation*}
        luego, $\pi$ es continua y, $\norm{\pi}\leq 1$.

        Sea ahora $\vec{x}\in M$, $\vec{x}\neq\vec{0}$. Entonces:
        \begin{equation*}
            \norm{\vec{x}}=\norm{\pi(\vec{x})}\leq\norm{\pi}\norm{\vec{x}}
        \end{equation*}
        por tanto, $\norm{\pi}\geq 1$, por lo anterior se sigue que $\norm{\pi}=1$.

        Ya se sabe que $\pi\circ\pi=\pi^2=\pi$ (por la proposición anterior).

        Sean $\vec{x},\vec{y}\in H$ arbitrarios. Entonces, $\pi(\vec{x})\in M$ y $\vec{y}-\pi(\vec{y})\perp M$, por lo cual
        \begin{equation*}
            \begin{split}
                0=&\pint{\pi(\vec{x})}{\vec{y}-\pi(\vec{y})} \\
                =&\pint{\pi(\vec{x})}{\vec{y}}-\pint{\pi(\vec{x})}{\pi(\vec{y})}\\
                \Rightarrow \pint{\pi(\vec{x})}{\vec{y}} =& \pint{\pi(\vec{x})}{\pi(\vec{y})}\\
            \end{split}
        \end{equation*}
        Intercambiando los papeles de $\vec{x}$ y $\vec{y}$ se obtiene que: $\pint{\pi(\vec{y})}{\vec{x}}=\pint{\pi(\vec{y})}{\pi(\vec{x})}$ o sea:
        \begin{equation*}
            \pint{\vec{x}}{\pi(\vec{y})}=\pint{\pi(\vec{x})}{\pi(\vec{y})}
        \end{equation*}
        por lo cual $\pint{\pi(\vec{x})}{\vec{y}}=\pint{\vec{x}}{\pi(\vec{y})}$.
    \end{proof}

    \begin{propo}
        Sea $H$ prehilbertiano. Suponga que $\pi$ es una aplicación lineal de $H$ en $H$ tal que
        \begin{itemize}
            \item $\pi^2=\pi$.
            \item $\pint{\pi(\vec{x})}{\vec{y}}=\pint{\vec{x}}{\pi(\vec{y})},\forall\vec{x},\vec{y}\in H$.
        \end{itemize}
        Entonces existe un único subespacio distinguido $M$ de $H$ tal que $\pi$ es la proyección ortogonal de $H$ sobre $M$.
    \end{propo}

    \begin{proof}
        Claramente, si $M$ existe debe ser $M=\pi(H)$, o sea:
        \begin{equation*}
            M=\pi(H)=\left\{\pi(\vec{x})\big| \vec{x}\in H \right\}
        \end{equation*}
        
        Se debe probar que si $\vec{x}\in H$ es arbitrario $\vec{x}-\pi(\vec{x})\perp M$, o sea
        \begin{equation*}
            \pint{\vec{x}-\pi(\vec{x})}{\pi(\vec{y})}=0,\quad\forall\vec{y}\in H
        \end{equation*}
        Sean $\vec{x},\vec{y}\in H$. Se tiene que:
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}-\pi(\vec{x})}{\pi(\vec{y})}=&\pint{\vec{x}}{\pi(\vec{y})}-\pint{\pi(\vec{x})}{\pi(\vec{y})} \\
                =&\pint{\vec{x}}{\pi(\vec{y})}-\pint{\vec{x}}{\pi(\vec{y})} \\
                =&0\\
            \end{split}
        \end{equation*}
        usando las dos propiedades de $\pi$. Por tanto, $\pi(\vec{x})$ es la proyección ortogonal de $\vec{x}$, es decir que $M$ es distinguido. La unicidad se sigue de la construcción de $M$.
    \end{proof}

    \section{Autodualidad de espacios hilbertianos}

    Si $E$ es un espacio normado, $E^*$ denota su \textbf{dual topológico} formado por todas las aplicaciones lineales continuas de $E$ en $\mathbb{K}$. Si $W\in E^*$, se define la $\norm{W}$ como
    \begin{equation*}
        \norm{W}=\inf\left\{a\in\mathbb{R}\big| \norm{W(\vec{x})}\leq a\norm{\vec{x}},\forall\vec{x} \right\}
    \end{equation*}
    
    Recuerde que $E^*$ es siempre un espaico de Banach aunque $E$ no lo sea.

    \begin{theor}[Teorema de Riesz]
        Sea $H$ un espacio hilbertiano (no reducido a $\left\{\vec{0} \right\}$). Para cada $\vec{y}\in H$ se define una aplicación $\cf{G_{\vec{y}}}{H}{\mathbb{K}}$ como sigue:
        \begin{equation*}
            G_{\vec{y}}(\vec{x})=\pint{\vec{x}}{\vec{y}},\forall\vec{x}\in H
        \end{equation*}
        Entonces, $G_{\vec{y}}$ es un funcional lineal continuo sobre $H$. Además, la aplicación $\cf{G}{H}{H^*}$ dada por:
        \begin{equation*}
            \vec{y}\mapsto G_{\vec{y}}
        \end{equation*}
         es una isometría semilineal de $H$ en $H^*$ que es suprayectiva.
    \end{theor}

    \begin{proof}
        Se probarán varias cosas:
        \begin{enumerate}
            \item Por propiedades del producto escalar, para cada $\vec{y}\in H$ la aplicación $\cf{G_{\vec{y}}}{H}{\mathbb{K}}$ es lineal. Dicha aplicación lineal es continua, pues
            \begin{equation*}
                \abs{G_{\vec{y}}(\vec{x})}=\abs{\pint{\vec{x}}{\vec{y}}}\leq\norm{\vec{x}}\norm{\vec{y}},\quad\forall\vec{x}\in H
            \end{equation*}
            (por Cauchy-Schwartz). Así que $G_{\vec{y}}\in H^*$. Además, $\norm{G_{\vec{y}}}\leq \norm{\vec{y}}$. Por otra, parte, si $\vec{y}\neq\vec{0}$, entonces
            \begin{equation*}
                G_{\vec{y}}(\vec{y})=\pint{\vec{y}}{\vec{y}}=\norm{\vec{y}}^2
            \end{equation*}
            pero, como el operador es continuo, se tiene que $\abs{G_{\vec{y}}(\vec{y})} \leq\norm{G_{\vec{y}}}\norm{\vec{y}}$. Por lo cual, $\norm{\vec{y}}\leq\norm{G_{\vec{y}}}$. Así pues, $\norm{G_{\vec{y}}}=\norm{\vec{y}}$.

            Si $\vec{y}=\vec{0}$, entonces $\norm{G_{\vec{y}}}=0=\norm{\vec{y}}$, pues $G_{\vec{y}}=0$.

            \item La aplicación $\cf{G}{H}{H^*}$, $\vec{y}\mapsto G_{\vec{y}}$ es semilineal, es decir que $G_{\alpha\vec{y}}=\conj{\alpha}G_{\vec{y}}$ y separa sumas. En efecto, sea $\vec{y}\in H$ y $\alpha\in\mathbb{K}$. Entonces:
            \begin{equation*}
                \begin{split}
                    G_{\alpha\vec{y}}(\vec{x})=&\pint{\vec{x}}{\alpha\vec{y}}\\
                    =&\conj{\alpha} \pint{\vec{x}}{\vec{y}}\\
                    =& G_{\vec{y}}(\vec{x}),\quad\forall\vec{x}\in H \\
                \end{split}
            \end{equation*}
            y además, para $\vec{z}\in H$ se tiene que
            \begin{equation*}
                \begin{split}
                    G_{\vec{y}+\vec{z}}(\vec{x})=&\pint{\vec{x}}{\vec{y}+\vec{z}} \\
                    =&\pint{\vec{x}}{\vec{y}}+\pint{\vec{x}}{\vec{z}} \\
                    =& G_{\vec{y}}(\vec{x})+G_{\vec{z}}(\vec{x}),\quad\forall\vec{x}\in H \\
                \end{split}
            \end{equation*}
            por tanto, $G$ es semilineal. Ahora, veamos que es isometría; sean $\vec{y_1},\vec{y_2}\in H$, entonces:
            \begin{equation*}
                \begin{split}
                    \norm{G_{\vec{y_1}}-G_{\vec{y_2}}}=&\norm{G_{\vec{y_1}+\vec{y_2}}} \\
                    =& \norm{\vec{y_1}+\vec{y_2}}\\
                \end{split}
            \end{equation*}
            así, esta función semilineal es isometría. Automáticamente $G$ es inyectiva. Note que $\vec{y}\in\left(\ker G_{\vec{y}} \right)^\perp$ y $G_{\vec{y}}(\vec{y})=\norm{\vec{y}}^2$.

            \item Se probará la suprayectividad. Sea $W\in H^*$ tal que $W\neq 0$ (en caso contrario basta tomar $\vec{y}=\vec{0}$) se debe probar que existe $\vec{y}\in H$ tal que $W=G_{\vec{y}}$. 
            
            Por la parte (2), tal $\vec{y}$ debe cumplir que $\vec{y}\in \left(\ker W\right)^\perp$ y $W(\vec{y})=\norm{\vec{y}}^2$. Como $\ker W $ es un subespacio cerrado de $H$ y $H$ es hilbertiano, entonces $\ker W$ es distinguido. Luego:
            \begin{equation*}
                H=\ker W \oplus\left(\ker W\right)^\perp
            \end{equation*}
            por tanto, la restricción de $W$ a $\left(\ker W\right)^\perp$ es un isomorfismo de $\left(\ker W\right)^\perp$ sobre $\mathbb{K}$. En efecto, como $W\neq 0$ entonces existe $\vec{x}\in H$ tal que $W(\vec{x})\neq 0$, pero podemos escribir de forma única a $\vec{x}=\vec{x_1}+\vec{x_2}$ con $\vec{x_1}\in \ker W$ y $\vec{x_2}\in \left(\ker W\right)^\perp$, entonces:
            \begin{equation*}
                \begin{split}
                    W(\vec{x})=&W(\vec{x_1}+\vec{x_2})\\
                    =&W(\vec{x_1})+W(\vec{x_2})\\
                    =&W(\vec{x_2})\\
                    =&W\big|_{\left(\ker W\right)^\perp} (\vec{x_2})\\
                    \neq&0\\
                \end{split}
            \end{equation*}
            Sea $\beta\in\mathbb{K}$ arbitrario, entonces:
            \begin{equation*}
                \begin{split}
                    W\big|_{\left(\ker W\right)^\perp}\left(\beta\frac{\vec{x_2}}{W(\vec{x_2})} \right)=&\beta\\
                \end{split}
            \end{equation*}
            por tanto la restricción es suprayectiva. Ahora si para algún $\vec{u}\in \left(\ker W\right)^\perp$ se tiene que $W\big|_{\left(\ker W\right)^\perp}\left(\vec{u} \right)=0$, en particular $\vec{u}\in\ker W$, pero:
            \begin{equation*}
                \left(\ker W\right)^\perp\cap \ker W=\left\{\vec{0} \right\}
            \end{equation*}
            por tanto $\vec{u}=\vec{0}$. Así la restricción es inyectiva. Luego es un isomorfismo. En particular la dimensión de $\mathbb{K}$ sobre $\mathbb{K}$ es $1$, así la dimensión de $\left(\ker W\right)^\perp$ es 1.

            Tomemos $\vec{z}$ generador de $\left(\ker W\right)^\perp$. El $\vec{y}$ buscado debe ser de la forma $\vec{y}=\alpha\vec{z}$ donde $\alpha\in\mathbb{K}$. Además,
            \begin{equation*}
                \begin{split}
                    W(\vec{y})=&\norm{\vec{y}}^2\\
                    \Rightarrow \alpha W(\vec{z})=&\alpha^2\norm{\vec{z}}^2\\
                    \Rightarrow \alpha=&\conj{\frac{W(\vec{z})}{\norm{\vec{z}}^2}}\\
                \end{split}
            \end{equation*}
            así, $\vec{y}$ debe ser
            \begin{equation}
                \vec{y}=\conj{\frac{W(\vec{z})}{\norm{\vec{z}}^2}}\vec{z}
            \end{equation}
            Verifiquemos el que vector en (1.8) es el que cumple que $W=G_{\vec{y}}$. Se tiene:
            \begin{equation*}
                \begin{split}
                    G_{\vec{y}}(\vec{x})=&\pint{\vec{x}}{\vec{y}} \\
                \end{split}
            \end{equation*}
            para todo $\vec{x}\in H$, donde este vector se descompone de forma única como $\vec{x}=\vec{x_1}+\vec{x_2}$ con $\vec{x_1}\in\ker W$ y $\vec{x_2}\in\left(\ker W\right)^\perp$. Por tanto:
            \begin{equation*}
                \begin{split}
                    G_{\vec{y}}(\vec{x})=&\pint{\vec{x_1}+\vec{x_2}}{\vec{y}} \\
                    =&\pint{\vec{x_1}}{\vec{y}}+\pint{\vec{x_2}}{\vec{y}} \\
                    =&\pint{\vec{x_2}}{\vec{y}} \\
                \end{split}
            \end{equation*}
            pero los elementos de $\left(\ker W\right)^\perp$ son de la forma $\beta\vec{z}$, por lo cual:\begin{equation*}
                \begin{split}
                    G_{\vec{y}}(\vec{x})=&\pint{\beta\vec{z}}{\vec{y}} \\
                    =&\pint{\beta\vec{z}}{\conj{\frac{W(\vec{z})}{\norm{\vec{z}}^2}}\vec{z}} \\
                    =&\beta\frac{W(\vec{z})}{\norm{\vec{z}}^2}\pint{\vec{z}}{\vec{z}} \\
                    =&\beta W(\vec{z}) \\
                    =&W(\beta\vec{z}) \\
                    =&W(\vec{x_2}) \\
                    =&W(\vec{x}) \\
                \end{split}
            \end{equation*}
            con lo que se tiene el resultado.

        \end{enumerate}
    \end{proof}

    \begin{obs}
        La demostración no cambia en vez de suponer que $H$ es hilbertiano se supone $H$ prehilbertiano tal que todo subespacio cerrado es distinguido (para solventar el problema que puede llegar a haber en la restricción del fucional lineal continuo $W$). Pero la conclusión del teorema afirma que $H$ es (semilinealmente) isométrico al espacio de Banach $H^*$, luego $H$ debe ser de Banach, es decir que es hilbertiano.

        Así pues, un espacio prehilbertiano en el cual todo subespacio cerrado es distinguido es un espacio hilbertiano. 
    \end{obs}

    \begin{propo}[Autodualidad de $L_2$]
        Sea $S$ un conjunto medible en $\mathbb{R}^n$. Para cada $g\in L_2(S,\mathbb{K})$ sea $\varphi_g$ el funcional lineal sobre $L_2(S,\mathbb{K})$ definido como:
        \begin{equation*}
            \varphi_g(f)=\int_Sfg, \quad\forall f\in L_2(S,\mathbb{K})
        \end{equation*}
        entonces, la aplicación lineal $\varphi:g\mapsto \varphi_g$ es una isometría lineal de $L_2(S,\mathbb{K})$ sobre $L_2(S,\mathbb{K})^*$.
    \end{propo}

    \begin{proof}
        Sea $$\psi_g(f)=\int_S f\conj{g}$$ para todo $f\in L_2(S,\mathbb{K})$. Por el teorema de Riesz, $\psi:g\mapsto\psi_g$ es una isometría semilineal de $L_2(S,\mathbb{K})$ sobre $L_2(S,\mathbb{K})^*$.

        Como la función $\eta$, $g\mapsto\conj{g}$ es una isometría semilineal de $L_2(S,\mathbb{K})$ sobre $L_2(S,\mathbb{K})$ y $\varphi$ es la composición de $\eta$ con $\psi$, entonces $\varphi$ es una isometría lineal de $L_2(S,\mathbb{K})$ sobre $L_2(S,\mathbb{K})$. La linealidad es inmediata de las propiedades de la integral de Lebesgue.

    \end{proof}

    ¿Es posible clasificar a los espacios hilbertianos?

    Consideremos las sumas de familisa de elementos en $[0,\infty]$. Se tiene que

    \begin{equation*}
        [0,\infty]=[0,\infty[\cup\left\{\infty \right\}
    \end{equation*}
    todo conjunto $S$ en $[0,\infty]$ posee un supremo, el usual si el conjunto es acotado en $[0,\infty[$ e $\infty$ si $S$ no es acotado.

    Si $\left\{x_n \right\}_{n=1}^\infty$ es una sucesión creciente en $[0,\infty[$, se define:
    \begin{equation*}
        \lim_{n\rightarrow\infty}x_n=\sup\left\{a_n\big|n\in\mathbb{N} \right\}
    \end{equation*}
    este límite coincide con el usual en el caso de que la sucesión sea acotada. De otra forma es igual a $\infty$.

    Se tienen las siguientes propiedades:

    \begin{enumerate}
        \item $a+\infty=\infty+a=\infty$, para todo $a\in[0,\infty[$.
        \item $a\cdot \infty=\infty\cdot a=\infty$, para todo $a\in[0,\infty[$.
        \item $0\cdot\infty=\infty\cdot0=0$.
    \end{enumerate}

    \begin{mydef}
        Sea $\left(a_\alpha\right)_{\alpha\in\Omega}$ una familia arbitraria de elementos de $[0,\infty]$. Se denota por $\mathcal{F}(\Omega)$ a la colección de \textbf{todos los subconjuntos finitos de $\Omega$}. Toda suma:
        \begin{equation*}
            \sum_{\alpha\in J}a_\alpha,\quad\forall J\in\mathcal{F}(\Omega)
        \end{equation*}
        se llama \textbf{suma parcial de la familia $\left(a_\alpha\right)_{\alpha\in\Omega}$}. Al elemento de $[0,\infty]$:
        \begin{equation*}
            \sum_{\alpha\in\Omega}a_\alpha=\sup\left\{ \sum_{\alpha\in J}a_\alpha\big|J\in\mathcal{F}(\Omega) \right\}
        \end{equation*}
        se le llama \textbf{suma de la familia $\left(a_\alpha\right)_{\alpha\in\Omega}$}. Se dice que $\left(a_\alpha\right)_{\alpha\in\Omega}$ es una \textbf{familia sumable} de números no negativos si $\sum_{\alpha\in\Omega}a_\alpha<\infty$.
    \end{mydef}

    \begin{exa}
        Se tiene que:
        \begin{equation*}
            \sum_{t\in[0,1]}t=\infty
        \end{equation*}
    \end{exa}

    \begin{propo}[Conmutatividad general]
        Si $\Omega'$ es otro conjunto de índices para indexar la familia $\left(a_\alpha\right)_{\alpha\in\Omega}$ y $\sigma$ es una biyección de $\Omega$ sobre $\Omega'$, entonces:
        \begin{equation}
            \sum_{\alpha'\in\Omega'}a_{\sigma(\alpha')}=\sum_{\alpha\in\Omega}a_\alpha
        \end{equation}
    \end{propo}

    \begin{proof}
        Es inmediato del hecho de que los conjuntos de las sumas parciales de las dos familias son el mismo, por tanto al tomar el supremo se obtiene el mismo valor.
    \end{proof}

    La ecuación (1.9) se aplica en particular al caso en el que $\Omega=\Omega'$, obteniendo una propiedad de conmutatividad general para sumas de familias en $[0,\infty]$.

    Ahora, ¿se tendrá una propiedad para la asociatividad general? La respuesta es que sí, se tiene un resultado que nos permite obtener esta propiedad para sumar familias.

    \begin{theor}[Sumación por paquetes de familias]
        Sea $\left(a_\alpha\right)_{\alpha\in I}$ una familia en $[0,\infty]$ y $\left(I_\lambda\right)_{\lambda\in L}$ una partición aritraria de subconjuntos de $I$. Si
        \begin{equation*}
            \Delta=\sum_{\alpha\in I}a_\alpha\quad\textup{y}\quad\Delta_\lambda=\sum_{\alpha\in I_\lambda}a_\alpha,\quad\forall\lambda\in L
        \end{equation*}
        entonces,
        \begin{equation*}
            \Delta=\sum_{\lambda\in L}\Delta_\lambda
        \end{equation*}
    \end{theor}

    \begin{proof}
        Sea $J\in\mathcal{F}(I)$ y sea
        \begin{equation*}
            M=\left\{\lambda\in L\big|I_\lambda\cap J\neq\emptyset \right\}
        \end{equation*}
        Entonces $M\in \mathcal{F}(L)$ y
        \begin{equation*}
            \begin{split}
                \sum_{\alpha\in J}a_\alpha=&\sum_{\lambda\in M} \sum_{\alpha\in J\cap I_\lambda}a_\alpha\\
                \leq&\sum_{\lambda\in M}\Delta_\lambda\\
                \leq&\sum_{\lambda\in Ls}\Delta_\lambda\\
            \end{split}
        \end{equation*}
        tomando supremo respecto a $J$ se sigue que:
        \begin{equation}
            \Delta\leq\sum_{\lambda\in L}\Delta_\lambda
        \end{equation}

        Sea $M\in\mathcal{F}(L)$. Fijemos arbitrariamente una $H_\lambda\in\mathcal{F}(I_\lambda)$, para todo $\lambda\in M$. Se tiene
        \begin{equation*}
            \begin{split}
                \sum_{\lambda\in M}\sum_{\alpha\in H_\lambda}a_\alpha=&\sum_{\alpha\in \bigcup_{\lambda\in M}H_\lambda}a_\alpha\\
                \leq&\Delta\\
            \end{split}
        \end{equation*}
        Manteniendo a $M$ fijo y tomando supremo con respecto a $H_\lambda\in\mathcal{F}(I_\lambda)$,resulta:
        \begin{equation*}
            \sum_{\lambda\in M}\Delta_\lambda\leq\Delta
        \end{equation*}
        tomando ahora el supremo con respecto a $M$ se obtiene que:
        \begin{equation}
            \sum_{\lambda\in L}\Delta_\lambda\leq\Delta
        \end{equation}
        de (1.10) y (1.11) se sigue la igualdad.

    \end{proof}

    \begin{exa}
        ¿Es cierto que $\sum_{n\in\mathbb{N}}a_n=\sum_{n=1}^{\infty}a_n$?
    \end{exa}

    La respuesta a esta pregunta la da el siguiente teorema:

    \begin{theor}
        Para toda sucesión $\left\{a_n \right\}_{n=1}^\infty$ en $[0,\infty]$ se cumple:
        \begin{equation*}
            \sum_{n\in\mathbb{N}}a_n=\sum_{n=1}^{\infty}a_n
        \end{equation*}
    \end{theor}

    \begin{proof}
        Sea $\Delta=\sum_{n\in\mathbb{N}}a_n$ y $\Sigma=\sum_{n=1}^{\infty}a_n$. Como la colección de sumas parciales de la serie $\sum_{n=1}^{\infty}a_n$ está contenida en la colección de sumas parciales de $\sum_{n\in J }a_n$ donde $J\in \mathcal{F}(\mathbb{N})$, entonces:
        \begin{equation*}
            \Sigma\leq\Delta
        \end{equation*}
        
        Sea $J\in \mathcal{F}(\mathbb{N})$. Tomando $k=\max_{i\in J} i$ se obtiene que:
        \begin{equation*}
            \sum_{n\in J }a_n\leq\sum_{n=1 }^{k}a_n
        \end{equation*}
        tomando supremos se sigue que $\Delta\leq\Sigma$. Finalmente, se obtiene que $\Delta=\Sigma$.

    \end{proof}

    \begin{cor}][Propiedad de conmutatividad para series]
        Sea $\left\{a_n \right\}_{n=1}^\infty$ en $[0,\infty]$, y sea $\sigma$ una biyección de $\mathbb{N}$ sobre $\mathbb{N}$. Entonces:
        \begin{equation*}
            \sum_{n=1}^{\infty}a_n=\sum_{n=1}^{\infty}a_{\sigma(n) }
        \end{equation*}
    \end{cor}

    \begin{proof}
        
    \end{proof}

    \begin{cor}
        Sea $\left\{a_{i,j} \right\}{(i,j)\in\mathbb{N}\times\mathbb{N}}$ una sucesión doble en $[0,\infty]$ y, $\sigma$ una biyección de $\mathbb{N}\times\mathbb{N}$ sobre $\mathbb{N}$.

        Tomemos $a_{i,j }=b_{\sigma(i,j)}$ para todo $(i,j)\in\mathbb{N}\times\mathbb{N}$. Entonces:
        \begin{equation*}
            \sum_{(i,j)\in\mathbb{N}\times\mathbb{N}}a_{i,j }=\sum_{k=1 }^{\infty}b_k
        \end{equation*}

        Además, sumando por paquetes, se tiene en particular que:
        \begin{equation*}
            \sum_{(i,j)\in\mathbb{N}\times\mathbb{N}}a_{i,j }=\sum_{i=1 }^{\infty} \sum_{j=1 }^{\infty}a_{ i,j}=\sum_{j=1 }^{\infty} \sum_{is=1 }^{\infty}a_{ i,j}
        \end{equation*}
    \end{cor}

    \begin{proof}
        
    \end{proof}

    \begin{theor}
        Para que una familia $\left(a_\alpha \right)_{\alpha\in\Omega }$ de elementos de $[0,\infty]$ sea sumable, son necesarias y suficientes las condiciones siguientes:
        \begin{enumerate}
            \item El conjunto:
            \begin{equation*}
                \Omega_0=\left\{\alpha\in\Omega\Big|a_\alpha\neq0 \right\}
            \end{equation*}
            sea a lo sumo numerable.
            \item En el caso de que $\Omega_0$ sea numerable, si tenemos una numeración $n\mapsto\alpha(n)$ es una biyección de $\mathbb{N}$ sobre $\Omega_0$ se tenga que:
            \begin{equation*}
                \sum_{n=1 }^{\infty}a_{\alpha(n)}<\infty
            \end{equation*}
        \end{enumerate}
        En este caso:
        \begin{equation*}
            \sum_{\alpha\in\Omega }a_\alpha=\sum_{n=1 }^{\infty}a_{\alpha(n)}
        \end{equation*}

    \end{theor}
    
    \begin{proof}
        La suficiencia es clara. (Ejercicio) %TODO%

        Veamos la necesidad. Supona que la familia $(a_\alpha)_{\alpha\in\Omega}$ de números no negativos es sumable de suma digamos $\Delta$. Sea:
        \begin{equation*}
            A_\nu=\left\{\alpha\Big|a_\alpha\geq\frac{1}{\nu} \right\},\quad\forall\nu\in\mathbb{N}
        \end{equation*}
        probaremos que los $A_\nu$ son finitos. Sea $\left\{\alpha_1,...,\alpha_k\right\}$ una familia finita de índices en $A_\nu$ con $\nu\in\mathbb{N}$. Entonces:
        \begin{equation*}
            \frac{k}{\nu}\leq a_{\alpha_1}+...+a_{\alpha_k}\leq\Delta
        \end{equation*}
        por tanto, $k\leq \nu\Delta$. Esto prueba que para cada $\nu\in\mathbb{N}$, $A_\nu$ es finito.

        Como $\Omega_0=\bigcup_{\nu=1 }^\infty A_\nu$, entonces $\Omega_0$ es a lo sumo numerable.

        El resto se deja como ejercicio al lector.
    \end{proof}

    \section{Familias sumables de números complejos}

    \begin{mydef}
        Sea $\left(u_\alpha \right)_{\alpha\in\Omega}$ una familia arbitraria de números complejos. Se dice que dicha familia es \textbf{sumable} si la familia de los módulos $\left(\abs{u_\alpha } \right)_{\alpha\in\Omega}$ es una familia sumable de números no negativos.
    \end{mydef}

    \begin{propo}
        Sea $\left(u_\alpha \right)_{\alpha\in\Omega}$ una familia sumable de números complejos. Sea:
        \begin{equation*}
            \Omega_0=\left\{\alpha\in\Omega\Big|u_\alpha\neq0 \right\}
        \end{equation*}
        entonces $\Omega_0$ es a lo sumo numerable. Además, si $i\mapsto\alpha(i)$ es una biyección de $\mathbb{N}$ sobre $\Omega_0$, entonces la serie
        \begin{equation*}
            \sum_{i=1 }^{\infty}a_{\alpha(i)}
        \end{equation*}
        es absolutamente convergente, y la suma de dicha serie es independiente la biyección $\alpha$ elegida, la cual se denomina \textbf{suma de la familia sumable $\left(u_\alpha \right)_{\alpha\in\Omega}$}, y se escribe
        \begin{equation*}
            \sum_{\alpha\in\Omega }a_\alpha=\sum_{i=1 }^{\infty}a_{\alpha(i)}
        \end{equation*}
        Si $\Omega'$ es otro conjunto numerable tal que $\Omega_0\subseteq\Omega'\subseteq\Omega$ y $i\mapsto\alpha(i)$ es una biyección de $\mathbb{N}$ sobre $\Omega'$, entonces:
        \begin{equation*}
            \sum_{\alpha\in\Omega }a_\alpha=\sum_{i=1 }^{\infty}a_{\alpha(i)}
        \end{equation*}
    \end{propo}

    \begin{proof}
        %TODO%
        Solo se probará la penúltima parte. Sea $i\mapsto\beta(i)$ otra biyección de $\mathbb{N}$ sobre $\Omega_0$. Hay que probar que:
        \begin{equation*}
            s=\sum_{i=1 }^{\infty}u_{\alpha(i)}=\sum_{i=1 }^{\infty}u_{\beta(i)}=t
        \end{equation*}
        Dado $\varepsilon>0$ existe $n_0\in\mathbb{N}$ tal que:
        \begin{equation*}
            \sum_{i>n_0 }\abs{u_{\alpha(i)} }<\frac{\varepsilon}{3}\quad\textup{y}\quad \sum_{i>n_0 }\abs{u_{\beta(i)} }<\frac{\varepsilon}{3}
        \end{equation*}
        también existe $n_1\in\mathbb{N}$ tal que $n_1>n_0$ y $\left\{\alpha(1),...,\alpha(n_0)\right\}\subseteq\left\{\beta(1),...,\beta(n_1) \right\}$ (básicamente podemos cubrir todos los índices de $\alpha$ con los $\beta$ eventualmente). Se tiene:
        \begin{equation*}
            \begin{split}
                \abs{s-t}\leq&\abs{s-\sum_{i=1 }^{n_0}u_{\alpha(i)}}+\abs{\sum_{i=1 }^{n_0 }u_{\alpha(i)}-\sum_{i=1 }^{n_1 }u_{\beta(i)}}+\abs{\sum_{i=1 }^{n_1}u_{\beta(i)}-t} \\
                < &\frac{2\varepsilon}{3}+\abs{\sum_{i=1 }^{n_0 }u_{\alpha(i)}-\sum_{i=1 }^{n_1 }u_{\beta(i)}}\\
            \end{split}
        \end{equation*}
        donde la primera desigualdad se da por la convergencia de la suma de los módulos. El último término, después de la reducción, se convierte en la suma de unos cuantos $u_\alpha(i)$ con $i\geq n_0$, los cuales al ser mayorizados con sus módulos suman algo menor que $\frac{\varepsilon}{3}$. Por tanto:
        \begin{equation*}
            \abs{s-t}<\varepsilon
        \end{equation*}
        luego, $s=t$.
    \end{proof}

    \begin{mydef}
        Sea $\Omega$ un conjunto arbitrario.
        \begin{enumerate}
            \item $l_1(\Omega,\mathbb{K})$ denota al conjunto de funciones $\cf{f}{\Omega}{\mathbb{K}}$ tales que $\left(f(\alpha) \right)_{\alpha\in\Omega}$ es una familia sumable en $\mathbb{K}$. Si $f\in l_1(\Omega,\mathbb{K})$, se escribe:
            \begin{equation*}
                \mathcal{N}_1(f)=\sum_{\alpha\in\Omega }\abs{f(\alpha)}
            \end{equation*}
            \item $l_2(\Omega,\mathbb{K})$ denota al conjunto de funciones $\cf{f}{\Omega}{\mathbb{K}}$ tales que $\left(f(\alpha)^2 \right)_{\alpha\in\Omega}$ es una familia sumable en $\mathbb{K}$. Si $f\in l_2(\Omega,\mathbb{K})$, se escribe:
            \begin{equation*}
                \mathcal{N}_2(f)=\left[\sum_{\alpha\in\Omega }\abs{f(\alpha)}^2\right]^{1/2}
            \end{equation*}
        \end{enumerate}
    \end{mydef}

    \begin{propo}
        $l_1(\Omega,\mathbb{K})$ es un espacio vectorial sobre el campo $\mathbb{K}$, y $\mathcal{N}_1$ es una norma sobre $l_1(\Omega,\mathbb{K})$. Además, si $f,g\in l_1(\Omega,\mathbb{K})$ entonces,
        \begin{equation*}
            \sum_{\alpha\in\Omega }(f+g)(\alpha)=\sum_{\alpha\in\Omega }f(\alpha)+\sum_{\alpha\in\Omega }g(\alpha)
        \end{equation*}
    \end{propo}

    \begin{proof}
        Sea $f\in l_1(\Omega,\mathbb{K})$ y $\lambda\in\mathbb{K}$. Sea $J\in\mathcal{F}(\Omega)$, se tiene que:
        \begin{equation*}
            \sum_{\alpha\in J }\abs{\lambda f(\alpha)}=\abs{\lambda} \sum_{\alpha\in J }\abs{f(\alpha)}\leq\abs{\lambda}\mathcal{N}_1(f)
        \end{equation*}
        tomando supremos se sigue que $\lambda f\in l_1(\Omega,\mathbb{K})$, pues la familia de sus módulos es sumable.

        Sean ahora $f,g\in l_1(\Omega,\mathbb{K})$ y $J\in\mathcal{F}(\Omega)$. Se sabe que:
        \begin{equation*}
            \sum_{\alpha\in J }\abs{(f+g)(\alpha)}\leq \sum_{\alpha\in J }\abs{f(\alpha)}+\sum_{\alpha\in J }\abs{g(\alpha)}=\mathcal{N}_1(f)+\mathcal{N}_1(g)
        \end{equation*}
        por tanto, $f+g\in l_1(\Omega,\mathbb{K})$ y $\mathcal{N}_1(f+g)\leq\mathcal{N}_1(f)+\mathcal{N}_1(g)$.

        Finalmente, se tiene que $\mathcal{N}_1(f)=0$ si y sólo si $f(\alpha)=0$ para todo $\alpha\in\Omega$, si y sólo si $f=0$.

        Por tanto, $\mathcal{N}_1$ es una norma sobre $l_1(\Omega,\mathbb{K})$.

        Sean ahora $f,g\in l_1(\Omega,\mathbb{K})$. Tomemos:
        \begin{equation*}
            \Omega_1=\left\{\alpha\in\Omega\Big|f(\alpha)\neq0 \right\}\quad\textup{y}\quad\Omega_2=\left\{\alpha\in\Omega\Big|g(\alpha)\neq0 \right\}
        \end{equation*}
        Defina $\Omega_0=\Omega_1\cup\Omega_2$. $\Omega_0,\Omega_1$ y $\Omega_2$ son a lo sumo numerables. Sea $i\mapsto\alpha(i)$ una biyección de $\mathbb{N}$ sobre $\Omega_0$. Entonces:
        \begin{equation*}
            \begin{split}
                \sum_{\alpha\in\Omega }(f+g)(\alpha)=&\sum_{i=1 }^\infty(f+g)(\alpha)\\
                =&\sum_{i=1 }^\infty f(\alpha)+\sum_{i=1 }^\infty g(\alpha)\\
                =&\sum_{\alpha\in\Omega }f(\alpha)+\sum_{\alpha\in\Omega }g(\alpha)\\
            \end{split}
        \end{equation*}
    \end{proof}

    \begin{obs}
        En la sumatoria con $\Omega_0$ se usó el último resultado de la proposición 1.3.1, ya que puede que la familia $\Omega_0$ no coincida con aquella en la que $\alpha\in\Omega$ es tal que $(f+g)(\alpha)=0$, sin embargo este conjunto $\Omega_0$ contiene a este conjunto que se especificó.
    \end{obs}

    \begin{theor}
        El espacio normado $l_1(\Omega,\mathbb{K})$ con la norma $\mathcal{N}_1$ es un espacio de Banach.
    \end{theor}

    \begin{proof}
        (Se seguirá adaptando la demostración correspondiente para $l_2$).
    \end{proof}

    \begin{theor}
        Sean $f,g\in l_2(\Omega,\mathbb{K})$. Entonces, $fg \in l_1(\Omega,\mathbb{K})$ y:
        \begin{equation*}
            \mathcal{N}_1(fg)\leq\mathcal{N}_2(f)\mathcal{N}_2(g)
        \end{equation*}
        Además, $l_2(\Omega,\mathbb{K})$ es un espacio vectorial sobre el campo $\mathbb{K}$. Se define $\forall f,g\in l_2(\Omega,\mathbb{K})$
        \begin{equation*}
            \pint{f}{g}=\sum_{\alpha\in\Omega}f(\alpha)\conj{g(\alpha)}
        \end{equation*}
        La aplicación $(f,g)\mapsto\pint{f}{g}$ hace de $l_2(\Omega,\mathbb{K})$ un espacio hilbertiano. La norma inducida por este producto escalar es $\mathcal{N}_2$.
    \end{theor}

    \begin{proof}
        Sea $J\in\mathcal{F}(\Omega)$. Por Cauchy-Schwartz para sumas finitas se tiene que:
        \begin{equation*}
            \begin{split}
                \sum_{\alpha\in J }\abs{f(\alpha)g(\alpha)}\leq&\left( \sum_{\alpha\in J }\abs{f(\alpha)}^2 \right)^{1/2}+\left( \sum_{\alpha\in J }\abs{g(\alpha)}^2 \right)^{1/2}\\
                \leq&\mathcal{N}_2(f)\mathcal{N}_2(g) \\
            \end{split}
        \end{equation*}
        tomando supremo respecto a $J$ se obtiene que $\mathcal{N}_1(fg)\leq\mathcal{N}_2(f)\mathcal{N}_2(g)$.

        Sean $f\in l_2(\Omega,\mathbb{K})$ y $\lambda\in\mathbb{K}$. Para todo $J\in\mathcal{F}(\Omega)$ se tiene que:
        \begin{equation*}
            \sum_{\alpha\in J}\abs{\lambda f(\alpha)}^2\leq\abs{\lambda}^2\sum_{\alpha\in J}\abs{f(\alpha)}^2\leq\abs{\lambda}^2\mathcal{N}_2(f)^2
        \end{equation*}
        tomando supremos se sigue que $\lambda f\in l_2(\Omega,\mathbb{K})$, y que $\mathcal{N}_2(\lambda f)=\abs{\lambda}\mathcal{N}_2(f)$.

        Sean $f,g\in l_2(\Omega,\mathbb{K})$. Para todo $J\in\mathcal{F}(\Omega)$ se tiene que:
        \begin{equation*}
            \begin{split}
                \sum_{\alpha\in J }\abs{f(\alpha)+g(\alpha)}^2=&\sum_{\alpha\in J }\abs{f(\alpha)}^2+\sum_{\alpha\in J }\abs{g(\alpha)}^2+\sum_{\alpha\in J }2\Re(f(\alpha)\conj{g(\alpha)})\\
                \leq&\sum_{\alpha\in J }\abs{f(\alpha)}^2+\sum_{\alpha\in J }\abs{g(\alpha)}^2+\sum_{\alpha\in J }2\abs{f(\alpha)+g(\alpha)}\\
                \leq& \mathcal{N}_2(f)+\mathcal{N}_2(g)+2\mathcal{N}_1(fg)
            \end{split}
        \end{equation*}
        tomando supremos respecto a $J$ se sigue que $f+g\in l_2(\Omega,\mathbb{K})$.

        La definición $\pint{f}{g}=\sum_{\alpha\in\Omega}f(\alpha)\conj{g(\alpha)}$ tiene sentido pues $f,g\in l_2(\Omega,\mathbb{K})$ implica que $fg\in l_1(\Omega,\mathbb{K})$. Se verifica de inmediato que $\pint{f}{g}$ es un producto escalar el cual induce $\mathcal{N}_2$.

        Ahora probaremos que es completo. Sea $\left\{f_\nu \right\}$ una sucesión de Cauchy en $l_2(\Omega,\mathbb{K})$. Sea $\varepsilon>0$. Existe $n_0\in\mathbb{N}$ tal que:
        \begin{equation*}
            \mathcal{N}_2(f_p-f_q)<\varepsilon,\quad\forall p,q\geq n_0
        \end{equation*}
        ya que para cada $\alpha\in\Omega$:
        \begin{equation*}
            \abs{f_p(\alpha)-f_q(\alpha)}\leq\mathcal{N}_2(f_p-f_q)<\varepsilon ,\quad\forall p,q\geq n_0
        \end{equation*}
        Como $\mathbb{K}$ es completo, existe $f(\alpha)\in\mathbb{K}$ tal que:
        \begin{equation*}
            \lim_{\nu\rightarrow\infty}f_\nu(\alpha)=f(\alpha)
        \end{equation*}
        Sea $J\in\mathcal{F}(\Omega)$. Se tiene entonces que:
        \begin{equation*}
            \sum_{\alpha\in J}\abs{f_p(\alpha)-f_q(\alpha)}^2\leq\varepsilon^2,\quad\forall p,q\geq n_0
        \end{equation*}
        Manteniendo a $p\geq n_0$ fijo y tomando límite cuando $q\rightarrow\infty$ y siendo $J$ finito,
        \begin{equation*}
            \sum_{\alpha\in J}\abs{f_p(\alpha)-f(\alpha)}^2\leq\varepsilon^2
        \end{equation*}
        Esto prueba que $f_p-f\in l_2(\Omega,\mathbb{K})$, de donde $f\in l_2(\Omega,\mathbb{K})$ y
        \begin{equation*}
            \mathcal{N}_2(f_p-f)\leq \varepsilon,\quad\forall p,q\geq n_0
        \end{equation*}
        luego, $l_2(\Omega,\mathbb{K})$ es completo.
    \end{proof}

    \section{Familias Ortonormales (O.N.)}

    \begin{mydef}
        Una familia de vectores, digamos $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ de vectores en un espacio prehilbertiano $H$ es \textbf{ortonormal} si:
        \begin{equation*}
                \pint{\vec{u_\alpha}}{\vec{u_\beta}}
                =\left\{\begin{array}{lcr}
                    0 & \textup{ si } \alpha\neq\beta\\
                    1 & \textup{ si } \alpha=\beta\\
                \end{array}
                \right.\quad\forall\alpha,\beta\in\Omega
        \end{equation*}
    \end{mydef}
    
    Recuerde que una familia $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ en $H$ es \textbf{linealmente independiente} si cualquier subcolección finita es linealmente independiente. Se tiene que si $\left(\vec{u_\alpha} \right)_{\alpha\in\Omega}$ es una familia O.N., entonces dicha familia es l.l. (linealmente independiente). En efecto, si $(\vec{u_1},...,\vec{u_n})$ son O.N., entonces
    \begin{eqnarray}
        \begin{split}
            \norm{\sum_{i=1 }^{n}\alpha_i\vec{u_i}}^2=& \sum_{i=1 }^{n}\abs{\alpha_i}^2\norm{\vec{u_i}}^2 \\
            =& \sum_{i=1 }^{n}\abs{\alpha_i}^2 \\
        \end{split}
    \end{eqnarray}
    (esto por Pitágoras), la cual es 0 si todos los $\alpha_i$ son cero, es decir si los vectores son l.i.

    \begin{propo}
        Se cumple lo siguiente:
        \begin{enumerate}
            \item Todo espacio hilbertiano $H$ de dimensión finita posee una base O.N.
            \item Sea $M$ un subespacio de dimensión finita de un espacio prehilbertiano $H$. Sea $(\vec{e_1},...,\vec{e_n})$ una base O.N. de $M$. Dado $\vec{x}\in H$. La proyección ortogonal de $\vec{x}$ sobre $M$ es:
            \begin{equation*}
                \vec{x_0}=\sum_{ i=1}^{n}\pint{\vec{x}}{\vec{e_i}}\vec{e_i}
            \end{equation*}
        \end{enumerate}
    \end{propo}

    \begin{proof}
        De (1): La prueba se hará por inducción sobre la dimensión de $H$.
        \begin{itemize}
            \item Suponga que la dimensión es 1. Existe $\vec{u}\in H$ tal que $\vec{u}\neq\vec{0}$. Una base O.N. de $H$ es $(\frac{\vec{u}}{\norm{\vec{u}}})$.
            \item Suponga que el resultado es cierto para dimensión $n-1$. Sea $H$ de dimensión $n$. Sea $\vec{u}\in H$ diferente de $\vec{0}$, defina:
            \begin{equation*}
                \vec{e_1}=\frac{\vec{u}}{\norm{\vec{u}}}
            \end{equation*}
            Sea $M=\mathcal{L}(\vec{e_1})$. Ya que $H=M\oplus M^\perp$ (ya que $M$ es distinguido), necesariamente $\dim M^\perp = n-1$. Por hipótesis inductiva $M^\perp$ posee una base O.N. digamos $(\vec{e_2},...,\vec{e_n})$.
            
            Entonces, $(\vec{e_1},...,\vec{e_n})$ es base O.N. de $H$.
        \end{itemize}
        aplicando inducción se sigue el resultado.

        De (2): Sea $\vec{x_0}=\sum_{ i=1}^{n}\pint{\vec{x}}{\vec{e_i}}\vec{e_i}$. Se tiene
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}-\vec{x_0}}{\vec{e_i}}=&\pint{\vec{x}}{\vec{e_i}}-\pint{\vec{x_0}}{\vec{e_i}} \\
                =&\pint{\vec{x}}{\vec{e_i}}-\pint{\sum_{ j=1}^{n}\pint{\vec{x}}{\vec{e_j}}\vec{e_j}}{\vec{e_i}} \\
                =&\pint{\vec{x}}{\vec{e_i}}-\pint{\pint{\vec{x}}{\vec{e_j}}\vec{e_j}}{\vec{e_i}} \\
                =&\pint{\vec{x}}{\vec{e_i}}-\pint{\vec{x}}{\vec{e_j}}\pint{\vec{e_j}}{\vec{e_i}} \\
                =&\pint{\vec{x}}{\vec{e_i}}-\pint{\vec{x}}{\vec{e_j}}\\
                =&0\\
            \end{split}
        \end{equation*}
        Siendo $M=\mathcal{L}(\vec{e_1},...,\vec{e_n})$, necesariamente $\pint{\vec{x}-\vec{x_0}}{\vec{y}}=0$, para todo $\vec{y}\in M$. Por tanto, $\vec{x_0}$ es efectivamente la proyección ortogonal de $\vec{x}$ sobre $M$.
    \end{proof}

    \begin{mydef}
        Sea $H$ prehilbertiano y sea $(\vec{u}_\alpha)_{\alpha\in\Omega}$ una familia O.N. en $H$. Para cada $\vec{x}\in H$ se define una función $\cf{\hat{x}}{\Omega}{\mathbb{K}}$ dada por:
        \begin{equation*}
            \hat{x}(\alpha)=\pint{\vec{x}}{\vec{u_\alpha}},\quad\forall\alpha\in\Omega
        \end{equation*}
        los escalares $\hat{x}(\alpha)$ se llaman \textbf{los coeficientes de Fourier de $\vec{x}$ con respecto a la familia ortonormal $(\vec{u}_\alpha)_{\alpha\in\Omega}$}.
    \end{mydef}

    \begin{theor}
        Con las hipótesis y notaciones de la definición anterior, $\forall\vec{x}\in H$ se tiene que $\hat{x}\in l_2(\Omega,\mathbb{K})$, y se cumple
        \begin{equation*}
            \mathcal{N}_2(\hat{x})\leq\norm{\vec{x}}
        \end{equation*}
        es decir:
        \begin{equation*}
            \sum_{\alpha\in\Omega}\abs{\hat{x}(\alpha)}^2\leq\norm{\vec{x}}^2
        \end{equation*}
        la desigualdad anterior es llamada \textbf{desigualdad de Bessel}.
    \end{theor}

    \begin{proof}
        Sea $J\in\mathcal{F}(\Omega)$ y defina $M_J=\mathcal{L}((\vec{u_\alpha})_{\alpha\in\Omega})$. Entonces $M_J$ es un subespacio de dimensión finita de $H$ provisto de la base O.N. $(\vec{u_\alpha})_{\alpha\in J}$. Entonces, la proyección ortogonal de $\vec{x}$ sobre $M_J$ debe ser:
        \begin{equation*}
            \vec{x_0}=\sum_{\alpha\in J }\pint{\vec{x}}{\vec{u_\alpha}}\vec{u_\alpha}=\sum_{\alpha\in J }\hat{x}(\alpha)\vec{u_\alpha}
        \end{equation*}
        por la proposición anterior. Por Pitágoras:
        \begin{equation*}
            \sum_{\alpha\in J}\abs{\hat{x}(\alpha)}^2=\norm{\sum_{\alpha\in J}\hat{x}(\alpha)\vec{u_\alpha}}^2=\norm{\vec{x_0}}^2=\norm{\vec{x}}^2-d(\vec{x},M)^2\leq\norm{\vec{x}}^2
        \end{equation*}
        tomando supremos respecto a $J$ se tiene el resultado.
    \end{proof}

    \begin{cor}
        La aplicación $\vec{x}\mapsto\hat{x}$ de $H$ en $l_2(\Omega,\mathbb{K})$ es una aplicación lineal continua de norma menor o igual a 1.
    \end{cor}

    \begin{proof}
        
    \end{proof}
        
    \begin{cor}
        Si $(\vec{u}_\alpha)_{\alpha\in\Omega}$ es un sistema O.N. de $H$ y $\vec{x}\in H$, entonces:
        \begin{equation*}
            \hat{x}(\alpha)=\pint{\vec{x}}{\vec{u_\alpha}}\neq0
        \end{equation*}
        para una cantidad a lo sumo numerable de índices $\alpha\in\Omega$.
    \end{cor}

    \begin{proof}
        
    \end{proof}

    \begin{theor}[Teorema de Riesz-Fischer]
        Sea $H$ prehilbertiano y sea $(\vec{u_\alpha})_{\alpha\in\Omega}$ una familia O.N. en $H$. Se supone que el subespacio cerrado
        \begin{equation*}
            M=\overline{\mathcal{L}((\vec{u_\alpha})_{\alpha\in\Omega})}
        \end{equation*}
        es completo (lo cual se cumple en particular si $H$ es hilbertiano). Entonces la aplicación $\vec{x}\mapsto\hat{x}$ es suprayectiva de $H$ sobre $l_2(\Omega,\mathbb{K})$. Más precisamente, dado $\varphi\in l_2(\Omega,\mathbb{K})$ existe un único $\vec{x_0}\in M$ tal que sus coeficientes de Fourier son, i.e. $\hat{x_0}=\varphi$.
        
        Además, para cualquier $\vec{x}\in H$ se cumple que $\hat{x}=\varphi$ si y sólo si $\vec{x}-\vec{x_0}\perp M$. 
    \end{theor}

    \begin{proof}
        Se harán varias cosas:
        \begin{enumerate}
            \item Sea $\varphi\in l_2(\Omega,\mathbb{K})$ y sea $\Omega_0=\left\{\alpha\in\Omega\Big|\varphi(\alpha)\neq0 \right\}$ el cual es a lo sumo numerable. Suponiendo que $\Omega_0$ es a lo sumo numerable, sea $i\mapsto\alpha(i)$ una biyección de $\mathbb{N}$ sobre $\Omega_0$.
            
            Considere la serie
            \begin{equation*}
                \sum_{i=1} ^{\infty}\varphi(\alpha(i))\vec{u_{\alpha(i)}}
            \end{equation*}
            en $M$. Como
            \begin{equation*}
                \sum_{i=1}^{\infty}\abs{\varphi(\alpha(i))}^2=\mathcal{N}_2(\varphi)<\infty
            \end{equation*}
            dado $\varepsilon>0$ existe $n_0\in\mathbb{N}$ tal que si $q>p\geq n_0$:
            \begin{equation*}
                \begin{split}
                    \norm{\sum_{i=p }^{q}\varphi(\alpha(i))\vec{u_{\alpha(i)}}}^2=&\sum_{i=p }^{q}\abs{\varphi(\alpha(i))^2}\\
                    \leq&\varepsilon^2 \\
                \end{split}
            \end{equation*}
            se cumple pues la condición de Cauchy para la convergencia de la serie en $M$. Como $M$ es completo, existe $\vec{x_0}\in M$ único tal que
            \begin{equation*}
                \vec{x_0}=\sum_{i=1} ^{\infty}\varphi(\alpha(i))\vec{u_{\alpha(i)}}
            \end{equation*}
            Se tiene
            \begin{equation*}
                \begin{split}
                    \pint{\sum_{i=1 }^{m}\varphi(\alpha)\vec{u_{\alpha(i)}}}{\vec{u_{\alpha(k)}}}=\varphi(\alpha(k)),\quad\forall m\geq k \\
                \end{split}
            \end{equation*}
            tomando límite cuando $m\rightarrow\infty$ y usando la continuidad de $\pint{}{}$:
            \begin{equation*}
                \Rightarrow \hat{x_0}(\alpha(k))=\pint{\vec{x_0}}{\vec{u_{\alpha(k)}}}=\varphi(\alpha(k)),\quad\forall k\in\mathbb{N}
            \end{equation*}
            Sea $\alpha\in\Omega\backslash\Omega_0$. Se tiene:
            \begin{equation*}
                \begin{split}
                    \pint{\sum_{i=1 }^{m}\varphi(\alpha)\vec{u_{\alpha(i)}}}{\vec{u_{\alpha}}}=\varphi(\alpha(k)),\quad\forall m\in\mathbb{N} \\
                \end{split}
            \end{equation*}
            tomando límite cuando $m\rightarrow\infty$ y usando la continuidad de $\pint{}{}$ se obtiene que:
            \begin{equation*}
                \Rightarrow \hat{x_0}(\alpha)=\pint{\vec{x_0}}{\vec{u_{\alpha}}}=\varphi(\alpha)
            \end{equation*}
            por tanto, $\hat{x_0}=\varphi$.

            \item Sea $\vec{x}\in H$ tal que $\hat{x}=\varphi$. Entonces,
            \begin{equation*}
                \widehat{x-x_0}=\hat{x}-\hat{x_0}=\varphi-\varphi=0
            \end{equation*}
            luego
            \begin{equation*}
                \pint{\vec{x}-\vec{x_0}}{\vec{u_\alpha}}=0\quad\forall\alpha\in\Omega
            \end{equation*}
            Así pues, $\vec{x}-\vec{x_0}\perp\mathcal{L}((\vec{u_\alpha})_{\alpha\in\Omega})$. Siendo los elementos de $M$ límites de sucesiones en $\mathcal{L}((\vec{u_\alpha})_{\alpha\in\Omega})$, por la continuidad de $\pint{}{}$ se tiene que $\vec{x}-\vec{x_0}\perp M$.

            Recíprocamente, si $\vec{x}-\vec{x_0}\perp M$, es claro que $\hat{0}=\widehat{x-x_0}=\hat{x}-\hat{x_0}=\hat{x}-\varphi$. Por tanto, $\hat{x}=\varphi$. En particular, si $x\in M$ y $\hat{x}=\varphi$, resulta que $\vec{x}-\vec{x_0}\in M$ y $\vec{x}-\vec{x_0}\perp M$, luego $\vec{x}-\vec{x_0}$, o sea $\vec{x}=\vec{x_0}$, es decir que el $\vec{x_0}$ es único.
        \end{enumerate}
    \end{proof}

\end{document}