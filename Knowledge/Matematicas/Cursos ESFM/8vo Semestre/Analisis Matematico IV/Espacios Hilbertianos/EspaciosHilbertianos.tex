\documentclass[12pt]{report}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{array}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[a4paper, margin = 1.5cm]{geometry}

%En esta parte se hacen redefiniciones de algunos comandos para que resulte agradable el verlos%

\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand{\theenumi}{\arabic{enumi})}

\def\proof{\paragraph{Demostración:\\}}
\def\endproof{\hfill$\blacksquare$}

\def\sol{\paragraph{Solución:\\}}
\def\endsol{\hfill$\square$}

%En esta parte se definen los comandos a usar dentro del documento para enlistar%

\newtheoremstyle{largebreak}
  {}% use the default space above
  {}% use the default space below
  {\normalfont}% body font
  {}% indent (0pt)
  {\bfseries}% header font
  {}% punctuation
  {\newline}% break after header
  {}% header spec

\theoremstyle{largebreak}

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!60!red!30
]{exa}{Ejemplo}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!50!blue!30
]{obs}{Observación}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    rightline = false,
    leftline = false
]{theor}{Teorema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    rightline = false,
    leftline = false
]{propo}{Proposición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    rightline = false,
    leftline = false
]{cor}{Corolario}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    rightline = false,
    leftline = false
]{lema}{Lema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    roundcorner=5pt,
    backgroundcolor = gray!30,
    hidealllines = true
]{mydef}{Definición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    roundcorner=5pt
]{excer}{Ejercicio}[section]

%En esta parte se colocan comandos que definen la forma en la que se van a escribir ciertas funciones%

\newcommand\abs[1]{\ensuremath{\big|#1\big|}}
\newcommand\divides{\ensuremath{\bigm|}}
\newcommand\cf[3]{\ensuremath{#1:#2\rightarrow#3}}
\newcommand\norm[1]{\ensuremath{\|#1\|}}
\newcommand\ora[1]{\ensuremath{\vec{#1}}}
\newcommand\pint[2]{\ensuremath{\left(#1\big| #2\right)}}
\newcommand\conj[1]{\ensuremath{\overline{#1}}}

%recuerda usar \clearpage para hacer un salto de página

\begin{document}
    \title{Espacios Hilbertianos}
    \author{Cristo Daniel Alvarado}
    \maketitle

    \tableofcontents %Con este comando se genera el índice general del libro%

    \chapter{Espacios Hilbertianos}
    
    %apostol de análisis matemático, lang de análisis real

    \section{Conceptos básicos. Proyecciones ortogonales}

    \begin{mydef}
        Sea $H$ un espacio vectorial sobre el campo $\mathbb{K}$. Decimos que $H$ es un \textbf{espacio prehilbertiano} si está dotado de una aplicación $(\ora{x},\ora{y})\mapsto \pint{\ora{x}}{\ora{y}}$ con las propiedades siguientes:
        \begin{enumerate}
            \item $\forall \ora{y}\in H$ fijo, $\ora{x}\mapsto\pint{\ora{x}}{\ora{y}}$ es una aplicación lineal de $H$ en $\mathbb{K}$, o sea
            \begin{equation*}
                \begin{split}
                    \pint{\ora{x_1}+\ora{x_2}}{\ora{y}}=&\pint{\ora{x_1}}{\ora{y}}+\pint{\ora{x_2}}{\ora{y}}\\
                    \pint{\alpha\ora{x}}{\ora{y}}=&\alpha\cdot\pint{\ora{x}}{\ora{y}}\\
                \end{split}
            \end{equation*}
            para todo $\ora{x},\ora{x_1},\ora{x_2}\in H$ y $\alpha\in\mathbb{K}$.
            \item $(\ora{y}\big| \ora{x})=\conj{\pint{\ora{x}}{\ora{y}}}$, para todo $\ora{x}\in H$.
            \item $\pint{\ora{x}}{\ora{x}}\geq0$, para todo $\ora{x}\in H$.
            \item $\pint{\ora{x}}{\ora{x}}=0$ si y sólo si $\ora{x}=0$.
        \end{enumerate}
    \end{mydef}

    \begin{obs}
        Si $\mathbb{K}=\mathbb{R}$, entonces 1) y 2) implican que $\forall \ora{x}\in H$ fijo, la aplicación $\ora{y}\mapsto\pint{\ora{x}}{\ora{y}}$ de $H$ en $\mathbb{R}$ es lineal. En este caso se dice que $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ es una \textbf{forma bilineal sobre $H$}.

        Si $\mathbb{K}=\mathbb{C}$, entonces
        \begin{equation*}
            \begin{split}
                \pint{\ora{x}}{\ora{y_1}+\ora{y_2}}=&\pint{\ora{x}}{\ora{y_1}}+\pint{\ora{x}}{\ora{y_2}}\\
                \pint{\ora{x}}{\alpha\ora{y}}=&\conj{\alpha}\pint{\ora{x}}{\ora{y}}\\
            \end{split}
        \end{equation*}
        Se dice que $\ora{y}\mapsto\pint{\ora{x}}{\ora{y}}$ es entonces \textbf{semilineal} y que $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ es \textbf{sesquilineal} ($1\frac{1}{2}$-lineal).

        La aplicación $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ se llama \textbf{producto escalar sobre $H$}.
    \end{obs}

    \begin{mydef}
        Para todo $\ora{x}\in H$ se define la \textbf{norma de $\ora{x}$} como: $\norm{\ora{x}}=\sqrt{\pint{\ora{x}}{\ora{x}}}$.
    \end{mydef}

    \begin{exa}
        Sea $H=\mathbb{K}^n$
        %El producto interior usual en ese espacio ya conocido y es prehilbertiano
    \end{exa}

    \begin{exa}
        Sea $S\subseteq\mathbb{R}^n$ medible y sea $H=L_2(S,\mathbb{K})$. Para todo $f,g\in H$ se define
        \begin{equation*}
            \pint{f}{g}=\int_Sf\conj{g}
        \end{equation*}
        La integral existe por Hölder con $p=p^*=2$. Este es un producto escalar sobre $H$ y, en este caso:
        \begin{equation*}
            \norm{f}=\left[\int_S\big|f\big|^2 \right]^{\frac{1}{2}}=\mathcal{N}_2(f),\quad \forall f\in H
        \end{equation*}
    \end{exa}

    \begin{exa}
        Sea $H=l_2(\mathbb{K})$ el espacio de sucesiones en $\mathbb{K}$ que son cuadrado sumables. Se sabe que $\ora{x}=(x_1,x_2,...)\in l_2(\mathbb{K})$ si y sólo si
        \begin{equation*}
            \sum_{i=1}^{\infty}|x_i|^2<\infty
        \end{equation*}
        $l_2(\mathbb{K})$ es un espacio prehilbertiano con el producto escalar:
        \begin{equation*}
            \pint{\ora{x}}{\ora{y}}=\sum_{i=1}^{\infty}x_i\conj{y_i}
        \end{equation*}
        donde la serie es convergente por Hölder. En este caso:
        \begin{equation*}
            \norm{\ora{x}}=\left[\sum_{i=1}^{\infty}\abs{x_i}^2\right]^{\frac{1}{2}}=\mathcal{N}_2(\ora{x}),\quad\forall\ora{x}\in l_2(\mathbb{K})
        \end{equation*}
    \end{exa}

    \begin{theor}[Desigualdad de Cauchy-Schwartz]
        Sea $H$ un espacio prehilbertiano. Entonces:
        \begin{enumerate}
            \item Se cumple la desigualdad de Cauchy-Schwartz:
            \begin{equation*}
                \abs{\pint{\vec{x}}{\vec{y}}}\leq \norm{\vec{x}}\norm{\vec{y}}, \quad\forall\vec{x},\vec{y}\in H
            \end{equation*}
            y, la igualdad se da si y sólo si los vectores son linealmente dependientes.
            \item Se cumple la desigualdad triangular:
            \begin{equation*}
                \norm{\vec{x}+\vec{y}}\leq\norm{\vec{x}}+\norm{\vec{y}},\quad\forall\vec{x},\vec{y}\in H
            \end{equation*}
            y la igualdad se da si y sólo si uno de los vectores es múltiplo no negativo del otro.
        \end{enumerate}
    \end{theor}

    \begin{proof}
        De 1): Se supondrá que $\mathbb{K}=\mathbb{C}$ (el caso en que sea $\mathbb{R}$ es similar y se deja como ejercicio).

        Sean $\vec{x},\vec{y}\in H$. En el caso de que alguno de los vectores sea $\vec{0}$, el resultado es inmediato (ambos miembros de la desigualdad son cero). Por lo cual, supongamos que ambos son no cero. Se tiene para todo $\lambda\in\mathbb{K}$ que
        \begin{equation}
            \begin{split}
                0\leq& \pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}\\
                =& \pint{\vec{x}}{\vec{x}}+\conj{\lambda}\pint{\vec{x}}{\vec{y}}+\lambda\pint{\vec{y}}{\vec{x}}+\lambda\conj{\lambda}\pint{\vec{y}}{\vec{y}}\\
                =& \pint{\vec{x}}{\vec{x}}+\conj{\lambda\pint{\vec{y}}{\vec{x}}}+\lambda\pint{\vec{y}}{\vec{x}}+\lambda\conj{\lambda}\pint{\vec{y}}{\vec{y}}\\
                &= \norm{\vec{x}}^2+2\Re{\lambda\pint{\vec{y}}{\vec{x}}}+\abs{\lambda}^2\norm{\vec{y}}^2\\
            \end{split}
        \end{equation}
        En particular, para
        \begin{equation*}
            \lambda(t)=\left\{\begin{array}{lcr}
                    t\frac{\pint{\vec{x}}{\vec{y}}}{\abs{\pint{\vec{x}}{\vec{y}}}} & \textup{si} & \pint{\vec{x}}{\vec{y}}\neq0\\
                    t & \textup{si} & \pint{\vec{x}}{\vec{y}}=0
                \end{array}
            \right.
        \end{equation*}
        con $t\in\mathbb{R}$, la desigualdad (1) se convierte en
        \begin{equation}
            0\leq\norm{\vec{x}}^2+2t\abs{\pint{\vec{y}}{\vec{x}}}+t^2\norm{\vec{y}}^2
        \end{equation}
        El trinomio anterior es mayor o igual a cero si y sólo si su discriminante:
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}^2-\norm{\vec{x}}^2\norm{\vec{y}}^2\leq0
        \end{equation*}
        es decir
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}\leq\norm{\vec{x}}\norm{\vec{y}}
        \end{equation*}
        Si $\abs{\pint{\vec{x}}{\vec{y}}}=\norm{\vec{x}}^2\norm{\vec{y}}^2$, entonces el trinomio en (2) tiene una raíz doble. Luego, existe $\lambda\in\mathbb{C}$ tal que
        \begin{equation*}
            \pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}=0
        \end{equation*}
        pero lo anterior solo sucede si y sólo si $\vec{x}+\lambda\vec{y}=0$, es decir si $\vec{x}$ y $\vec{y}$ son linealmente dependientes.

        De 2): Se tiene lo siguiente:
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}+\vec{y}}^2=&\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}\\
                =&\norm{\vec{x}}+2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}}^2\\
                \leq&\norm{\vec{x}}+2\abs{\pint{\vec{y}}{\vec{x}}}+\norm{\vec{y}}^2\\
                \leq &\norm{\vec{x}}+2\norm{\vec{x}}\norm{\vec{y}}+\norm{\vec{y}}^2\\
                =& (\norm{\vec{x}}+\norm{\vec{y}})^2\\
            \end{split}
        \end{equation*}
        lo cual implica la desigualdad que se quiere probar. Ahora, la igualdad se cumple si y sólo si
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}=\Re\pint{\vec{x}}{\vec{y}}\textup{ y }\abs{\pint{\vec{x}}{\vec{y}}}=\norm{\vec{x}}\norm{\vec{y}}
        \end{equation*}
        la primera igualdad implica que $\pint{\vec{x}}{\vec{y}}$ es real (en particular, $\geq0$ por el valor absoluto) y la segunda implica que $\vec{x}$ y $\vec{y}$ son linealmente dependientes. Es decir, si y sólo si un vector es multiplo no negativo del otro.
    \end{proof}

    Se concluye del teorema anterior que $\norm{\cdot}$ es una norma sobre $H$. En lo sucesivo se consdierará a $H$ como espacio normado dotado de esta norma.

    \begin{propo}
        La aplicación $(\vec{x},\vec{y})\mapsto\pint{\vec{x}}{\vec{y}}$
        es una función continua del espacio normado producto $H\times H$ en $\mathbb{K}$.
    \end{propo}

    \begin{proof}
        Sean $\vec{x},\vec{y}\in H$ y, $\left\{\vec{x_n} \right\}_{n=1}^\infty$ y $\left\{\vec{y_n} \right\}_{n=1}^\infty$ dos sucesiones que convergen a $\vec{x}$ y $\vec{y}$, respectivamente. Se probará que $\left\{\pint{\vec{x_n}}{\vec{y_n}} \right\}_{n=1}^\infty$ converge a $\pint{\vec{x}}{\vec{y}}$ en $\mathbb{K}$.
        Se tiene que
        \begin{equation}
            \begin{split}
                \abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}
                \leq&\abs{\pint{\vec{x}-\vec{x_n}}{\vec{y}}}+\abs{\pint{\vec{x_n}}{\vec{y}-\vec{y_n}}}\\
                \leq&\norm{\vec{x}-\vec{x_n}}\norm{\vec{y}}+\norm{\vec{x_n}}\norm{\vec{y}-\vec{y_n}}\\
            \end{split}
        \end{equation}
        para todo $n\in\mathbb{N}$. Como $\left\{\vec{x_n} \right\}$ es convergente, es acotada. Luego existe $M>0$ tal que
        \begin{equation*}
            \norm{\vec{x_n}}\leq M,\quad\forall n\in\mathbb{N}
        \end{equation*}
        Se sigue de (3) que
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}
            \leq\norm{\vec{x}-\vec{x_n}}\norm{\vec{y}}+M\norm{\vec{y}-\vec{y_n}}
        \end{equation*}
        y, por ende
        \begin{equation*}
            \lim_{n\rightarrow\infty}\abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}=0
        \end{equation*}
        con lo que se tiene el resultado.
    \end{proof}

    \begin{mydef}
        Decimos que un espacio prehilbertiano se llama \textbf{Hilbertiano}, si la norma $\norm{\cdot}$ hace de él un espacio normado completo (o sea, un espacio normado de Banach).
    \end{mydef}

    \begin{exa}
        Los espacios $L_2(S,\mathbb{K})$, $l_2(\mathbb{K})$ y todo espacio prehilbertiano de dimensión finita ($\mathbb{K}^n$) son hilbretianos (ya que, todo espacio prehilbertiano de dimensión finita es isomorfo a $\mathbb{R}^k$, para algún $k\in\mathbb{N}$).
    \end{exa}

    De ahora en adelante, $H$ denotará siempre a un espacio prehilbertiano (a menos que se indique lo contrario).

    \begin{mydef}
        Sean $\vec{x},\vec{y}\in H$. Se dice que \textbf{$\vec{x}$ y $\vec{y}$ son ortogonales} y se escribe $\vec{x}\perp\vec{y}$, si $\pint{\vec{x}}{\vec{y}}=0$.
    \end{mydef}

    \begin{obs}
        La condición $\vec{x}\perp\vec{y}$ para todo $\vec{x}\in H$ implica que $\vec{y}=\vec{0}$, pues en particular $\pint{\vec{y}}{\vec{y}}=0\Rightarrow\vec{y}=\vec{0}$.
    \end{obs}

    \begin{theor}[Teorema de Pitágoras]
        Si $\left(\vec{x_1},\dots,\vec{x_n} \right)$ es un sistema de vectores ortogonales (a pares), entonces
        \begin{equation*}
            \norm{\vec{x_1}+\dots+\vec{x_n}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2
        \end{equation*}
    \end{theor}

    \begin{proof}
        Se procederá por inducción sobre $n$. Veamos el caso $n=2$. En este caso, veamos que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x_1}+\vec{x_2}}^2=&\pint{\vec{x_1}+\vec{x_2}}{\vec{x_1}+\vec{x_2}}\\
                =&\norm{\vec{x_1}}^2+\pint{\vec{x_1}}{\vec{x_2}}+\pint{\vec{x_2}}{\vec{x_1}}+\norm{\vec{x_2}}^2\\
                =&\norm{\vec{x_1}}^2+\norm{\vec{x_2}}^2\\
            \end{split}
        \end{equation*}
        Suponga que el resultado se cumple para $n\geq2$. Sea $\vec{x_1},...\vec{x_{n+1}}\in H$ un sistema de vectores ortogonales. Observemos que
        \begin{equation*}
            \begin{split}
                \pint{\vec{x_1}+\cdots+\vec{x_n}}{\vec{x_{n+1}}}&=\pint{\vec{x_1}}{\vec{x_{n+1}}}+\cdots+\pint{\vec{x_n}}{\vec{x_{n+1}}}\\
                &=0+\cdots+0\\
                &=0\\
            \end{split}
        \end{equation*}
        por lo cual, $x_{n+1}\perp\vec{x_1}+\cdots+\vec{x_n}$. Por el caso $n=2$ se sigue que:
        \begin{equation*}
            \norm{\vec{x_1}+\cdots+\vec{x_{n+1}}}^2=\norm{\vec{x_1}+\cdots+\vec{x_n}}^2+\norm{\vec{x_{n+1}}}^2
        \end{equation*}
        Pero, por hipótesis de inducción:
        \begin{equation*}
            \norm{\vec{x_1}+\dots+\vec{x_n}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2
        \end{equation*}
        Por lo cual:
        \begin{equation*}
            \norm{\vec{x_1}+\cdots+\vec{x_{n+1}}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2+\norm{\vec{x_{n+1}}}^2
        \end{equation*}
        Aplicando inducción se sigue el resultado.
    \end{proof}

    \begin{propo}[Identidad del paralelogramo]
        Para todo $\vec{x},\vec{y}\in H$ se cumple la identidad del paralelogramo:
        \begin{equation*}
            \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2=2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)
        \end{equation*}
    \end{propo}

    \begin{proof}
        Sean $\vec{x},\vec{y}\in H$. Veamos que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2
                =&\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}+\pint{\vec{x}-\vec{y}}{\vec{x}-\vec{y}}\\
                =&\norm{\vec{x}}^2+2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}^2}+\norm{\vec{x}}^2-2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}}^2\\
                =&2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)\\
            \end{split}
        \end{equation*}
    \end{proof}

    Este resultado anterior es importante, pues en espacios donde la norma no venga de un producto escalar, no necesariamente se cumple la igualdad.

    \begin{exa}
        Los vectores $\chi_{[0,1]}$ y $\chi_{[1,2]}$ son ortogonales en $L_2(\mathbb{R},\mathbb{R})$ (es inmediato del producto escalar en $L_2(\mathbb{R},\mathbb{R})$).
    \end{exa}

    \begin{exa}
        Los vectores $\sen$ y $\cos$ son ortogonales en $L_2([-\pi,\pi[,\mathbb{R})$. En efecto, veamos que
        \begin{equation*}
            \pint{\sen}{\cos}=\int_{-\pi}^{\pi}\sen x\cos xdx=\frac{1}{2}\int_{-\pi}^{\pi}\sen 2xdx=0
        \end{equation*}
        En particular, por Pitágoras se tiene que
        \begin{equation*}
            \int_{-\pi}^{\pi}\abs{\sen x+\cos x}^2dx=\int_{-\pi}^{\pi}\abs{\sen x}^2dx+\int_{-\pi}^{\pi}\abs{\cos x}^2dx
        \end{equation*}
    \end{exa}

    \begin{exa}
        Si $\vec{x}=(1,1,\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{3}{3},...)$ y $\vec{x}=(1,-1,\frac{1}{2},-\frac{1}{2},\frac{1}{2},-\frac{3}{3},)$ son elementos de $l_2(\mathbb{R})$, se tiene que $\vec{x}\perp\vec{y}$. En efecto, veamos que
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{y}}=&\sum_{n=1}^{\infty}x_n\conj{y_n}\\
                =&\lim_{n\rightarrow\infty}s_n
            \end{split}
        \end{equation*}
        donde $\left\{s_n\right\}_{n=1}^\infty$ es la sucesión de sumas parciales, siendo $s_{2m}=0$ y $s_{2m-1}=\frac{1}{m}$. Por lo cual
        \begin{equation*}
            \pint{\vec{x}}{\vec{y}}=\lim_{n\rightarrow\infty}s_n=0
        \end{equation*}
    \end{exa}

    \begin{theor}
        Sea $M$ un subespacio de un espaco prehilbertiano $H$ y sea $\vec{x}\in H$.
        \begin{enumerate}
            \item Suponiendo que existe $\vec{x_0}\in M$ tal que $\vec{x}-\vec{x_0}\perp M$, es decir que $\vec{x}-\vec{x_0}\perp\vec{y}$, para todo $\vec{y}\in M$, se tiene
            \begin{equation*}
                \norm{\vec{x}-\vec{x_0}}<\norm{\vec{x}-\vec{y}},\quad\forall\vec{y}\in M,\vec{y}\neq\vec{x_0}
            \end{equation*}
            Así pues, si existe $\vec{x_0}$, tal vector es único y es llamado \textbf{la proyección ortogonal de $\vec{x}$ sobre $M$}. Además
            \begin{equation*}
                d(\vec{x},M)^2=\norm{\vec{x}-\vec{x_0}}^2=\norm{\vec{x}}^2-\norm{\vec{x_0}}^2
            \end{equation*}
            \item Recíprocamente, si existe un $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$, entonces $\vec{x_0}$ es la proyección ortogonal de $\vec{x}$ sobre $M$. En particular, si $\vec{x}\in M$ entonces $\vec{x}=\vec{x_0}$, es decir que $\vec{x}$ es su propia proyección ortogonal sobre $M$.
        \end{enumerate}
    \end{theor}

    \begin{proof}
        De 1): Suponga que existe $\vec{x_0}\in M$ con la condición especificada. Sea $\vec{y}\in M$ distinto de $\vec{x_0}$. Como $\vec{x_0}-\vec{x}\perp\vec{x_0}-\vec{y}$, por el Teorema de Pitágoras se tiene que
        \begin{equation}
            \norm{\vec{x}-\vec{y}}^2=\norm{\vec{x}-\vec{x_0}}^2+\vec{x_0}-\vec{y}^2>\norm{\vec{x}-\vec{x_0}}^2
        \end{equation}
        pues $\vec{x_0}\neq \vec{y}$. Así pues, $\vec{x_0}$ es único. Además $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Aplicando la ecuación 4) con $\vec{y}=\vec{0}$ se tiene que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}}^2=&\norm{\vec{x}-\vec{x_0}}^2+\norm{\vec{x_0}}^2\\
                \Rightarrow d(\vec{x},M)^2=\norm{\vec{x}-\vec{x_0}}^2=&\norm{\vec{x}}^2-\norm{\vec{x_0}}^2\\
            \end{split}
        \end{equation*}
        
        De 2) Si existe $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$, entonces $\vec{x_0}$ debe ser la proyección ortogonal de $\vec{x}$ sobre $M$. En efecto, para todo $\vec{y}\in M$ y para todo $\lambda\in\mathbb{K}$ se tiene
        \begin{equation}
            \begin{split}
                \norm{\vec{x}-(\vec{x_0}+\lambda\vec{y})}^2\geq&\norm{\vec{x}-\vec{x_0}}^2\\
                \Rightarrow \norm{(\vec{x}-\vec{x_0})-\lambda\vec{y}}^2=&\norm{\vec{x}-\vec{x_0}}^2+2\Re[\conj{\lambda} \pint{\vec{x}-\vec{x_0}}{\vec{y}}]+\abs{\lambda}^2\norm{\vec{y}}^2\\
                =&\norm{\vec{x}-\vec{x_0}}^2-2\Re[\lambda\pint{\vec{x}-\vec{x_0}}{\vec{y}}]+\abs{\lambda}^2\norm{\vec{y}}^2\\
                \Rightarrow -2\Re[\lambda\pint{\vec{x}-\vec{x_0}}{\vec{y}}]+\abs{\lambda}^2\norm{\vec{y}}^2\geq& 0\\
            \end{split}
        \end{equation}
        en particular, para $\lambda=t\pint{\vec{x}-\vec{x_0}}{\vec{y}}$, con $t\in\mathbb{R}$, la ecuación anterior se transforma en:
        \begin{equation*}
            \begin{split}
                %=&-2t\abs{\pint{\vec{x}-\vec{x_0}}{\vec{y}}}^2+t^2\abs{\pint{\vec{x}-\vec{x_0}}}^2+\norm{\vec{y}}^2\\
                =&\abs{\pint{\vec{x}-\vec{x_0}}{\vec{y}}}^2\left[-2t+t^2\norm{\vec{y}}\right]\\
            \end{split}
        \end{equation*}
        para todo $t\in\mathbb{R}$. Esto exige que $\pint{\vec{x}-\vec{x_0}}{\vec{y}}=0$, o sea que $\vec{x}-\vec{x_0}\perp \vec{y}$.
    \end{proof}

    Dado un subespacio $M$ de un espacio prehilbertiano $H$ un vector $\vec{x}\in H$, puede no existir la proyección ortogonal de $\vec{x}$ sobre $M$. Esto motiva la siguiente definición:

    \begin{mydef}
        Un subespacio $M$ de $H$ se dice que es \textbf{distinguido} si para cada $\vec{x}\in H$ existe la proyección ortogonal de $\vec{x}$ sobre $M$.        
    \end{mydef}

    \begin{exa}
        El subespacio $\phi_0$ de las sucesiones eventualmente constantes de calor cero es un subespacio del espacio hilbretiano $l_2(\mathbb{R})$. Sea $M$ el subespacio de $\phi_0$ dado como sigue:
        \begin{equation*}
            M=\left\{\vec{x}\in\phi_0|x_2=0 \right\}
        \end{equation*}
        Sea $\vec{x}=\left(0,\frac{1}{2^{0/2}},\frac{1}{2^{1/2}},\frac{1}{2},\frac{1}{2^{3/2}},... \right)$. Se tiene que:
        \begin{equation*}
            \begin{split}
                d(\vec{x},M)&=\inf_{\vec{y}\in M}\left\{\norm{\vec{x}-\vec{y}} \right\}\\
                &=\inf_{\vec{y}\in M}\left\{\left[\sum_{i=2}^{\infty} \frac{1}{2^{(i-1)/2}}-y_i \right] \right\}\\
                &=1\\
            \end{split}
        \end{equation*}
        (pues, $y_2=0$). Pero $\norm{\vec{x}-\vec{y}}>1$, para todo $\vec{y}\in M$, luego no existe $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Por lo tanto, no existe la proyección ortogonal de $\vec{x}$ sobre $M$ (luego $M$ no es distinguido).

        Sin embargo, si $\vec{x}=(1,1,0,...)\in l_2(\mathbb{R})$, entonces si existe la proyección ortogonal de $\vec{x}$ sobre $M$, pues
        \begin{equation*}
            \begin{split}
                d(\vec{x},M)&=\inf_{\vec{y}\in M}\left\{\norm{\vec{x}-\vec{y}} \right\}\\
                &=\inf_{\vec{y}\in M}\left\{1-y_1+1+\left[\sum_{i=3}^{\infty} y_i \right] \right\}\\
                &=1\\
            \end{split}
        \end{equation*}
        y $\norm{\vec{x}-\vec{e_1}}=1$, donde $\vec{e_1}\in M$. Por tanto, $\vec{e_1}$ es la proyección ortogonal de $\vec{x}$ sobre $M$.
    \end{exa}

    \begin{theor}
        Si $M$ es un subespacio completo de un espacio prehilbertiano, entonces $M$ es distinguido, en particular, todo subespacio de dimensión finita de un espacio prehilbertiano siempre es distinguido. 
    \end{theor}

    \begin{proof}
        Sea $\vec{x}\in H$. Se debe probar que existe un $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$. Sea $a=d(\vec{x},M)$. Existe una sucesión $\left\{\vec{y_\nu} \right\}$ tal que
        \begin{equation}
            \lim_{\nu\rightarrow\infty}\norm{\vec{x}-\vec{y_\nu}}=a
        \end{equation}
        Sean $\nu,\mu\in\mathbb{N}$ arbitrarios. Por la identidad del paralelogramo se tiene que
        \begin{equation*}
            \begin{split}
                2\left(\norm{\vec{x}-\vec{y_\nu}}^2+\norm{\vec{x}-\vec{y_\mu}}^2 \right)
                &=\norm{\vec{y_\nu}-\vec{y_\mu}}^2+\norm{2\vec{x}-(\vec{y_\nu}+\vec{y_\mu})}^2\\
                &=\norm{\vec{y_\nu}-\vec{y_\mu}}^2+4\norm{\vec{x}-\frac{\vec{y_\nu}+\vec{y_\mu}}{2}}^2\\
                &\geq\norm{\vec{y_\nu}-\vec{y_\mu}}^2+4a^2\\
            \end{split}
        \end{equation*}
        de donde
        \begin{equation*}
            \norm{\vec{y_\nu}-\vec{y_\mu}}^2\leq4\left(\norm{\vec{x}-\vec{y_\nu}}^2+\norm{\vec{x}-\vec{y_\mu}}^2 \right)-2a^2
        \end{equation*}
        Tomando límite cuando $\nu,\mu$ tienden a infinito y por (6), se tiene que
        \begin{equation*}
            \lim_{\nu,\mu\rightarrow\infty}\norm{\vec{y_\nu}-\vec{y_\mu}}^2=0
        \end{equation*}
        por tanto, $\left\{\vec{y_\nu} \right\}$ es de Cauchy. Por ser $M$ completo, existe $\vec{x_0}\in M$ tal que $\lim_{\nu\rightarrow\infty}\vec{y_\nu}=\vec{x_0}$. Por (6):
        \begin{equation*}
            a=\lim_{\nu\rightarrow\infty}\norm{\vec{x}-\vec{y_\nu}}=\norm{\vec{x}-\vec{x_0}}
        \end{equation*}
    \end{proof}

    \begin{exa}
        ¿Es distinguido el subespacio de $L_2(\mathbb{R},\mathbb{R})$ dado por:
        \begin{equation*}
            M=\left\{f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})| f(x)=0\textup{ c.t.p. en }[1,2] \right\}
        \end{equation*}
        ?
        
        La respuesta es que sí, ya que $M$ es cerrado. En efecto, sea $\left\{f_\nu\right\}$ una sucesión en $M$ convergente en promedio cuadrático a una $f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})$, es decir:
        \begin{equation*}
            \lim_{\nu\rightarrow\infty}\mathcal{N}_2(f_\nu-f)=0
        \end{equation*}
        Se sabe que existe una subsucesión de $\left\{f_\nu\right\}$, digamos $\left\{f_{\alpha(\nu)}\right\}$ que converge c.t.p. a $f$ en $\mathbb{R}$. Como $f_{\alpha(\nu)}=0$ c.t.p. en $[1,2]$, entonces $f=0$ c.t.p. en $[1,2]$, es decir $f\in M$. Por tanto, $M$ es distinguido.

        Ahora, dada $f\in\mathcal{L}_2(\mathbb{R},\mathbb{R})$, ¿Cuál será la proyección ortogonal de $f$ sobre $M$? Veamos que
        \begin{equation*}
            f_0=f\cdot\chi_{\mathbb{R}\backslash[1,2]}\in M
        \end{equation*}
        es la proyección ortogonal de $f$ sobre $M$, y además $f-f_0\perp M$.
    \end{exa}

    \begin{mydef}
        Sea $S\subseteq H$ un conjunto arbitrario. Para este conjunto se define
        \begin{equation*}
            S^{\perp}=\left\{\vec{x}\in H|\vec{x}\perp\vec{s},\forall\vec{s}\in S \right\}
        \end{equation*}
        Es claro que $S^\perp$ es un subespacio cerrado de $H$.
    \end{mydef}

    \begin{sol}
        En efecto, si $\left\{\vec{x_\nu} \right\}$ es una sucesión en $S^\perp$ que converge a $\vec{x}\in H$, entonces
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{s}}=\lim_{\nu\rightarrow\infty}\pint{\vec{x_\nu}}{\vec{y}}=0,\quad\forall\vec{s}\in S
            \end{split}
        \end{equation*}
        por continuidad y para todo $\vec{s}\in S$. Luego $\vec{x}\in S^\perp$. Otra forma es definiendo una función $\cf{T_{\vec{s}}}{H}{\mathbb{K}}$ como
        \begin{equation*}
            T_{\vec{s}}(\vec{x})=\pint{\vec{x}}{\vec{s}},\quad\forall\vec{x}\in H
        \end{equation*}
        Entonces
        \begin{equation*}
            S^\perp=\bigcap_{\vec{s}\in S}\ker T_{\vec{s}}
        \end{equation*}
        Como $T_{\vec{s}}$ es lineal continua para todo $\vec{s}\in S$, entonces se sigue que $S^\perp$ es cerrado.
    \end{sol}

    \begin{propo}
        Un subespacio $M$ de un espaco prehilbertiano $H$ es distinguido si y sólo si
        \begin{equation*}
            H=M\oplus M^\perp
        \end{equation*}
    \end{propo}

    \begin{proof}
        $\Rightarrow$): Suponga que $M$ es distinguido. Como $M\cap M^\perp=\left\{\vec{0} \right\}$, para probar que $H=M\oplus M^\perp$, basta probar que es la suma simplemente, es decir que $H=M+M^\perp$.
        
        Sea $\vec{x}\in H$, como $M$ es distinguido entonces existe $\vec{x_1}\in M$ tal que $\vec{x}-\vec{x_1}\perp M$, tomando $\vec{x_2}=\vec{x}-\vec{x_1}$ se tiene que $\vec{x_2}\in M^\perp$. Además $\vec{x}=\vec{x_1}+\vec{x_2}$, lo que prueba el resultado.

        $\Leftarrow$): Suponga que $H=M\oplus M^\perp$. Hay que probar que $M$ es distinguido. Sea $\vec{x}\in H$ arbitrario. Por hipótesis existen $\vec{x_1}\in M$ y $\vec{x_2}\in M^\perp$ únicos tales que $\vec{x}=\vec{x_1}+\vec{x_2}$. Se afirma que $\vec{x_1}$ es la proyección ortogonal de $\vec{x}$ sobre $M$. 
        
        En efecto,
        \begin{equation*}
            \vec{x}-\vec{x_1}=\vec{x_2}\in M^\perp
        \end{equation*}
        pero $\vec{x_2}\perp M$, por tanto $\vec{x_1}$ es la proyección ortogonal.
    \end{proof}

\end{document}