\documentclass[12pt]{report}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{array}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[a4paper, margin = 1.5cm]{geometry}

%En esta parte se hacen redefiniciones de algunos comandos para que resulte agradable el verlos%

\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand{\theenumi}{\arabic{enumi})}

\def\proof{\paragraph{Demostración:\\}}
\def\endproof{\hfill$\blacksquare$}

\def\sol{\paragraph{Solución:\\}}
\def\endsol{\hfill$\square$}

%En esta parte se definen los comandos a usar dentro del documento para enlistar%

\newtheoremstyle{largebreak}
  {}% use the default space above
  {}% use the default space below
  {\normalfont}% body font
  {}% indent (0pt)
  {\bfseries}% header font
  {}% punctuation
  {\newline}% break after header
  {}% header spec

\theoremstyle{largebreak}

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!60!red!30
]{exa}{Ejemplo}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!50!blue!30
]{obs}{Observación}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    rightline = false,
    leftline = false
]{theor}{Teorema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    rightline = false,
    leftline = false
]{propo}{Proposición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    rightline = false,
    leftline = false
]{cor}{Corolario}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    rightline = false,
    leftline = false
]{lema}{Lema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    roundcorner=5pt,
    backgroundcolor = gray!30,
    hidealllines = true
]{mydef}{Definición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=-2pt,
    innerbottommargin=8pt,
    roundcorner=5pt
]{excer}{Ejercicio}[section]

%En esta parte se colocan comandos que definen la forma en la que se van a escribir ciertas funciones%

\newcommand\abs[1]{\ensuremath{\big|#1\big|}}
\newcommand\divides{\ensuremath{\bigm|}}
\newcommand\cf[3]{\ensuremath{#1:#2\rightarrow#3}}
\newcommand\norm[1]{\ensuremath{\|#1\|}}
\newcommand\ora[1]{\ensuremath{\vec{#1}}}
\newcommand\pint[2]{\ensuremath{\left(#1\big| #2\right)}}
\newcommand\conj[1]{\ensuremath{\overline{#1}}}

%recuerda usar \clearpage para hacer un salto de página

\begin{document}
    \title{Espacios Hilbertianos}
    \author{Cristo Daniel Alvarado}
    \maketitle

    \tableofcontents %Con este comando se genera el índice general del libro%

    \chapter{Espacios Hilbertianos}
    
    %apostol de análisis matemático, lang de análisis real

    \section{Conceptos básicos. Proyecciones ortogonales}

    \begin{mydef}
        Sea $H$ un espacio vectorial sobre el campo $\mathbb{K}$. Decimos que $H$ es un \textbf{espacio prehilbertiano} si está dotado de una aplicación $(\ora{x},\ora{y})\mapsto \pint{\ora{x}}{\ora{y}}$ con las propiedades siguientes:
        \begin{enumerate}
            \item $\forall \ora{y}\in H$ fijo, $\ora{x}\mapsto\pint{\ora{x}}{\ora{y}}$ es una aplicación lineal de $H$ en $\mathbb{K}$, o sea
            \begin{equation*}
                \begin{split}
                    \pint{\ora{x_1}+\ora{x_2}}{\ora{y}}=&\pint{\ora{x_1}}{\ora{y}}+\pint{\ora{x_2}}{\ora{y}}\\
                    \pint{\alpha\ora{x}}{\ora{y}}=&\alpha\cdot\pint{\ora{x}}{\ora{y}}\\
                \end{split}
            \end{equation*}
            para todo $\ora{x},\ora{x_1},\ora{x_2}\in H$ y $\alpha\in\mathbb{K}$.
            \item $(\ora{y}\big| \ora{x})=\conj{\pint{\ora{x}}{\ora{y}}}$, para todo $\ora{x}\in H$.
            \item $\pint{\ora{x}}{\ora{x}}\geq0$, para todo $\ora{x}\in H$.
            \item $\pint{\ora{x}}{\ora{x}}=0$ si y sólo si $\ora{x}=0$.
        \end{enumerate}
    \end{mydef}

    \begin{obs}
        Si $\mathbb{K}=\mathbb{R}$, entonces 1) y 2) implican que $\forall \ora{x}\in H$ fijo, la aplicación $\ora{y}\mapsto\pint{\ora{x}}{\ora{y}}$ de $H$ en $\mathbb{R}$ es lineal. En este caso se dice que $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ es una \textbf{forma bilineal sobre $H$}.

        Si $\mathbb{K}=\mathbb{C}$, entonces
        \begin{equation*}
            \begin{split}
                \pint{\ora{x}}{\ora{y_1}+\ora{y_2}}=&\pint{\ora{x}}{\ora{y_1}}+\pint{\ora{x}}{\ora{y_2}}\\
                \pint{\ora{x}}{\alpha\ora{y}}=&\conj{\alpha}\pint{\ora{x}}{\ora{y}}\\
            \end{split}
        \end{equation*}
        Se dice que $\ora{y}\mapsto\pint{\ora{x}}{\ora{y}}$ es entonces \textbf{semilineal} y que $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ es \textbf{sesquilineal} ($1\frac{1}{2}$-lineal).

        La aplicación $(\ora{x},\ora{y})\mapsto\pint{\ora{x}}{\ora{y}}$ se llama \textbf{producto escalar sobre $H$}.
    \end{obs}

    \begin{mydef}
        Para todo $\ora{x}\in H$ se define la \textbf{norma de $\ora{x}$} como: $\norm{\ora{x}}=\sqrt{\pint{\ora{x}}{\ora{x}}}$.
    \end{mydef}

    \begin{exa}
        Sea $H=\mathbb{K}^n$
        %El producto interior usual en ese espacio ya conocido y es prehilbertiano
    \end{exa}

    \begin{exa}
        Sea $S\subseteq\mathbb{R}^n$ medible y sea $H=L_2(S,\mathbb{K})$. Para todo $f,g\in H$ se define
        \begin{equation*}
            \pint{f}{g}=\int_Sf\conj{g}
        \end{equation*}
        La integral existe por Hölder con $p=p^*=2$. Este es un producto escalar sobre $H$ y, en este caso:
        \begin{equation*}
            \norm{f}=\left[\int_S\big|f\big|^2 \right]^{\frac{1}{2}}=\mathcal{N}_2(f),\quad \forall f\in H
        \end{equation*}
    \end{exa}

    \begin{exa}
        Sea $H=l_2(\mathbb{K})$ el espacio de sucesiones en $\mathbb{K}$ que son cuadrado sumables. Se sabe que $\ora{x}=(x_1,x_2,...)\in l_2(\mathbb{K})$ si y sólo si
        \begin{equation*}
            \sum_{i=1}^{\infty}|x_i|^2<\infty
        \end{equation*}
        $l_2(\mathbb{K})$ es un espacio prehilbertiano con el producto escalar:
        \begin{equation*}
            \pint{\ora{x}}{\ora{y}}=\sum_{i=1}^{\infty}x_i\conj{y_i}
        \end{equation*}
        donde la serie es convergente por Hölder. En este caso:
        \begin{equation*}
            \norm{\ora{x}}=\left[\sum_{i=1}^{\infty}\abs{x_i}^2\right]^{\frac{1}{2}}=\mathcal{N}_2(\ora{x}),\quad\forall\ora{x}\in l_2(\mathbb{K})
        \end{equation*}
    \end{exa}

    \begin{theor}[Desigualdad de Cauchy-Schwartz]
        Sea $H$ un espacio prehilbertiano. Entonces:
        \begin{enumerate}
            \item Se cumple la desigualdad de Cauchy-Schwartz:
            \begin{equation*}
                \abs{\pint{\vec{x}}{\vec{y}}}\leq \norm{\vec{x}}\norm{\vec{y}}, \quad\forall\vec{x},\vec{y}\in H
            \end{equation*}
            y, la igualdad se da si y sólo si los vectores son linealmente dependientes.
            \item Se cumple la desigualdad triangular:
            \begin{equation*}
                \norm{\vec{x}+\vec{y}}\leq\norm{\vec{x}}+\norm{\vec{y}},\quad\forall\vec{x},\vec{y}\in H
            \end{equation*}
            y la igualdad se da si y sólo si uno de los vectores es múltiplo no negativo del otro.
        \end{enumerate}
    \end{theor}

    \begin{proof}
        De 1): Se supondrá que $\mathbb{K}=\mathbb{C}$ (el caso en que sea $\mathbb{R}$ es similar y se deja como ejercicio).

        Sean $\vec{x},\vec{y}\in H$. En el caso de que alguno de los vectores sea $\vec{0}$, el resultado es inmediato (ambos miembros de la desigualdad son cero). Por lo cual, supongamos que ambos son no cero. Se tiene para todo $\lambda\in\mathbb{K}$ que
        \begin{equation}
            \begin{split}
                0\leq& \pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}\\
                =& \pint{\vec{x}}{\vec{x}}+\conj{\lambda}\pint{\vec{x}}{\vec{y}}+\lambda\pint{\vec{y}}{\vec{x}}+\lambda\conj{\lambda}\pint{\vec{y}}{\vec{y}}\\
                =& \pint{\vec{x}}{\vec{x}}+\conj{\lambda\pint{\vec{y}}{\vec{x}}}+\lambda\pint{\vec{y}}{\vec{x}}+\lambda\conj{\lambda}\pint{\vec{y}}{\vec{y}}\\
                &= \norm{\vec{x}}^2+2\Re{\lambda\pint{\vec{y}}{\vec{x}}}+\abs{\lambda}^2\norm{\vec{y}}^2\\
            \end{split}
        \end{equation}
        En particular, para
        \begin{equation*}
            \lambda(t)=\left\{\begin{array}{lcr}
                    t\frac{\pint{\vec{x}}{\vec{y}}}{\abs{\pint{\vec{x}}{\vec{y}}}} & \textup{si} & \pint{\vec{x}}{\vec{y}}\neq0\\
                    t & \textup{si} & \pint{\vec{x}}{\vec{y}}=0
                \end{array}
            \right.
        \end{equation*}
        con $t\in\mathbb{R}$, la desigualdad (1) se convierte en
        \begin{equation}
            0\leq\norm{\vec{x}}^2+2t\abs{\pint{\vec{y}}{\vec{x}}}+t^2\norm{\vec{y}}^2
        \end{equation}
        El trinomio anterior es mayor o igual a cero si y sólo si su discriminante:
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}^2-\norm{\vec{x}}^2\norm{\vec{y}}^2\leq0
        \end{equation*}
        es decir
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}\leq\norm{\vec{x}}\norm{\vec{y}}
        \end{equation*}
        Si $\abs{\pint{\vec{x}}{\vec{y}}}=\norm{\vec{x}}^2\norm{\vec{y}}^2$, entonces el trinomio en (2) tiene una raíz doble. Luego, existe $\lambda\in\mathbb{C}$ tal que
        \begin{equation*}
            \pint{\vec{x}+\lambda\vec{y}}{\vec{x}+\lambda\vec{y}}=0
        \end{equation*}
        pero lo anterior solo sucede si y sólo si $\vec{x}+\lambda\vec{y}=0$, es decir si $\vec{x}$ y $\vec{y}$ son linealmente dependientes.

        De 2): Se tiene lo siguiente:
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}+\vec{y}}^2=&\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}\\
                =&\norm{\vec{x}}+2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}}^2\\
                \leq&\norm{\vec{x}}+2\abs{\pint{\vec{y}}{\vec{x}}}+\norm{\vec{y}}^2\\
                \leq &\norm{\vec{x}}+2\norm{\vec{x}}\norm{\vec{y}}+\norm{\vec{y}}^2\\
                =& (\norm{\vec{x}}+\norm{\vec{y}})^2\\
            \end{split}
        \end{equation*}
        lo cual implica la desigualdad que se quiere probar. Ahora, la igualdad se cumple si y sólo si
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}}=\Re\pint{\vec{x}}{\vec{y}}\textup{ y }\abs{\pint{\vec{x}}{\vec{y}}}=\norm{\vec{x}}\norm{\vec{y}}
        \end{equation*}
        la primera igualdad implica que $\pint{\vec{x}}{\vec{y}}$ es real (en particular, $\geq0$ por el valor absoluto) y la segunda implica que $\vec{x}$ y $\vec{y}$ son linealmente dependientes. Es decir, si y sólo si un vector es multiplo no negativo del otro.
    \end{proof}

    Se concluye del teorema anterior que $\norm{\cdot}$ es una norma sobre $H$. En lo sucesivo se consdierará a $H$ como espacio normado dotado de esta norma.

    \begin{propo}
        La aplicación $(\vec{x},\vec{y})\mapsto\pint{\vec{x}}{\vec{y}}$
        es una función continua del espacio normado producto $H\times H$ en $\mathbb{K}$.
    \end{propo}

    \begin{proof}
        Sean $\vec{x},\vec{y}\in H$ y, $\left\{\vec{x_n} \right\}_{n=1}^\infty$ y $\left\{\vec{y_n} \right\}_{n=1}^\infty$ dos sucesiones que convergen a $\vec{x}$ y $\vec{y}$, respectivamente. Se probará que $\left\{\pint{\vec{x_n}}{\vec{y_n}} \right\}_{n=1}^\infty$ converge a $\pint{\vec{x}}{\vec{y}}$ en $\mathbb{K}$.
        Se tiene que
        \begin{equation}
            \begin{split}
                \abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}
                \leq&\abs{\pint{\vec{x}-\vec{x_n}}{\vec{y}}}+\abs{\pint{\vec{x_n}}{\vec{y}-\vec{y_n}}}\\
                \leq&\norm{\vec{x}-\vec{x_n}}\norm{\vec{y}}+\norm{\vec{x_n}}\norm{\vec{y}-\vec{y_n}}\\
            \end{split}
        \end{equation}
        para todo $n\in\mathbb{N}$. Como $\left\{\vec{x_n} \right\}$ es convergente, es acotada. Luego existe $M>0$ tal que
        \begin{equation*}
            \norm{\vec{x_n}}\leq M,\quad\forall n\in\mathbb{N}
        \end{equation*}
        Se sigue de (3) que
        \begin{equation*}
            \abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}
            \leq\norm{\vec{x}-\vec{x_n}}\norm{\vec{y}}+M\norm{\vec{y}-\vec{y_n}}
        \end{equation*}
        y, por ende
        \begin{equation*}
            \lim_{n\rightarrow\infty}\abs{\pint{\vec{x}}{\vec{y}}-\pint{\vec{x_n}}{\vec{y_n}}}=0
        \end{equation*}
        con lo que se tiene el resultado.
    \end{proof}

    \begin{mydef}
        Decimos que un espacio prehilbertiano se llama \textbf{Hilbertiano}, si la norma $\norm{\cdot}$ hace de él un espacio normado completo (o sea, un espacio normado de Banach).
    \end{mydef}

    \begin{exa}
        Los espacios $L_2(S,\mathbb{K})$, $l_2(\mathbb{K})$ y todo espacio prehilbertiano de dimensión finita ($\mathbb{K}^n$) son hilbretianos (ya que, todo espacio prehilbertiano de dimensión finita es isomorfo a $\mathbb{R}^k$, para algún $k\in\mathbb{N}$).
    \end{exa}

    De ahora en adelante, $H$ denotará siempre a un espacio prehilbertiano (a menos que se indique lo contrario).

    \begin{mydef}
        Sean $\vec{x},\vec{y}\in H$. Se dice que \textbf{$\vec{x}$ y $\vec{y}$ son ortogonales} y se escribe $\vec{x}\perp\vec{y}$, si $\pint{\vec{x}}{\vec{y}}=0$.
    \end{mydef}

    \begin{obs}
        La condición $\vec{x}\perp\vec{y}$ para todo $\vec{x}\in H$ implica que $\vec{y}=\vec{0}$, pues en particular $\pint{\vec{y}}{\vec{y}}=0\Rightarrow\vec{y}=\vec{0}$.
    \end{obs}

    \begin{theor}[Teorema de Pitágoras]
        Si $\left(\vec{x_1},\dots,\vec{x_n} \right)$ es un sistema de vectores ortogonales (a pares), entonces
        \begin{equation*}
            \norm{\vec{x_1}+\dots+\vec{x_n}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2
        \end{equation*}
    \end{theor}

    \begin{proof}
        Se procederá por inducción sobre $n$. Veamos el caso $n=2$. En este caso, veamos que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x_1}+\vec{x_2}}^2=&\pint{\vec{x_1}+\vec{x_2}}{\vec{x_1}+\vec{x_2}}\\
                =&\norm{\vec{x_1}}^2+\pint{\vec{x_1}}{\vec{x_2}}+\pint{\vec{x_2}}{\vec{x_1}}+\norm{\vec{x_2}}^2\\
                =&\norm{\vec{x_1}}^2+\norm{\vec{x_2}}^2\\
            \end{split}
        \end{equation*}
        Suponga que el resultado se cumple para $n\geq2$. Sea $\vec{x_1},...\vec{x_{n+1}}\in H$ un sistema de vectores ortogonales. Observemos que
        \begin{equation*}
            \begin{split}
                \pint{\vec{x_1}+\cdots+\vec{x_n}}{\vec{x_{n+1}}}&=\pint{\vec{x_1}}{\vec{x_{n+1}}}+\cdots+\pint{\vec{x_n}}{\vec{x_{n+1}}}\\
                &=0+\cdots+0\\
                &=0\\
            \end{split}
        \end{equation*}
        por lo cual, $x_{n+1}\perp\vec{x_1}+\cdots+\vec{x_n}$. Por el caso $n=2$ se sigue que:
        \begin{equation*}
            \norm{\vec{x_1}+\cdots+\vec{x_{n+1}}}^2=\norm{\vec{x_1}+\cdots+\vec{x_n}}^2+\norm{\vec{x_{n+1}}}^2
        \end{equation*}
        Pero, por hipótesis de inducción:
        \begin{equation*}
            \norm{\vec{x_1}+\dots+\vec{x_n}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2
        \end{equation*}
        Por lo cual:
        \begin{equation*}
            \norm{\vec{x_1}+\cdots+\vec{x_{n+1}}}^2=\norm{\vec{x_1}}^2+\dots+\norm{\vec{x_n}}^2+\norm{\vec{x_{n+1}}}^2
        \end{equation*}
        Aplicando inducción se sigue el resultado.
    \end{proof}

    \begin{propo}[Identidad del paralelogramo]
        Para todo $\vec{x},\vec{y}\in H$ se cumple la identidad del paralelogramo:
        \begin{equation*}
            \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2=2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)
        \end{equation*}
    \end{propo}

    \begin{proof}
        Sean $\vec{x},\vec{y}\in H$. Veamos que
        \begin{equation*}
            \begin{split}
                \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2
                =&\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}+\pint{\vec{x}-\vec{y}}{\vec{x}-\vec{y}}\\
                =&\norm{\vec{x}}^2+2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}^2}+\norm{\vec{x}}^2-2\Re\pint{\vec{y}}{\vec{x}}+\norm{\vec{y}}^2\\
                =&2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)\\
            \end{split}
        \end{equation*}
    \end{proof}

    Este resultado anterior es importante, pues en espacios donde la norma no venga de un producto escalar, no necesariamente se cumple la igualdad.

    \begin{exa}
        Los vectores $\chi_{[0,1]}$ y $\chi_{[1,2]}$ son ortogonales en $L_2(\mathbb{R},\mathbb{R})$ (es inmediato del producto escalar en $L_2(\mathbb{R},\mathbb{R})$).
    \end{exa}

    \begin{exa}
        Los vectores $\sen$ y $\cos$ son ortogonales en $L_2([-\pi,\pi[,\mathbb{R})$. En efecto, veamos que
        \begin{equation*}
            \pint{\sen}{\cos}=\int_{-\pi}^{\pi}\sen x\cos xdx=\frac{1}{2}\int_{-\pi}^{\pi}\sen 2xdx=0
        \end{equation*}
        En particular, por Pitágoras se tiene que
        \begin{equation*}
            \int_{-\pi}^{\pi}\abs{\sen x+\cos x}^2dx=\int_{-\pi}^{\pi}\abs{\sen x}^2dx+\int_{-\pi}^{\pi}\abs{\cos x}^2dx
        \end{equation*}
    \end{exa}

    \begin{exa}
        Si $\vec{x}=(1,1,\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{3}{3},...)$ y $\vec{x}=(1,-1,\frac{1}{2},-\frac{1}{2},\frac{1}{2},-\frac{3}{3},)$ son elementos de $l_2(\mathbb{R})$, se tiene que $\vec{x}\perp\vec{y}$. En efecto, veamos que
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{y}}=&\sum_{n=1}^{\infty}x_n\conj{y_n}\\
                =&\lim_{n\rightarrow\infty}s_n
            \end{split}
        \end{equation*}
        donde $\left\{s_n\right\}_{n=1}^\infty$ es la sucesión de sumas parciales, siendo $s_{2m}=0$ y $s_{2m-1}=\frac{1}{m}$. Por lo cual
        \begin{equation*}
            \pint{\vec{x}}{\vec{y}}=\lim_{n\rightarrow\infty}s_n=0
        \end{equation*}
    \end{exa}

    \begin{theor}
        Sea $M$ un subespacio de un espaco prehilbertiano $H$ y sea $\vec{x}\in H$.
        \begin{enumerate}
            \item Suponiendo que existe $\vec{x_0}\in M$ tal que $\vec{x}-\vec{x_0}\perp M$, es decir que $\vec{x}-\vec{x_0}\perp\vec{y}$, para todo $\vec{y}\in M$, se tiene
            \begin{equation*}
                \norm{\vec{x}-\vec{x_0}}<\norm{\vec{x}-\vec{y}},\quad\forall\vec{y}\in M,\vec{y}\neq\vec{x_0}
            \end{equation*}
            Así pues, si existe $\vec{x_0}$, tal vector es único y es llamado \textbf{la proyección ortogonal de $\vec{x}$ sobre $M$}. Además
            \begin{equation*}
                d(\vec{x},M)^2=\norm{\vec{x}-\vec{x_0}}^2=\norm{\vec{x}}^2-\norm{\vec{x_0}}^2
            \end{equation*}
            \item Recíprocamente, si existe un $\vec{x_0}\in M$ tal que $d(\vec{x},M)=\norm{\vec{x}-\vec{x_0}}$, entonces $\vec{x_0}$ es la proyección ortogonal de $\vec{x}$ sobre $M$. En particular, si $\vec{x}\in M$ entonces $\vec{x}=\vec{x_0}$, es decir que $\vec{x}$ es su propia proyección ortogonal sobre $M$.
        \end{enumerate}
    \end{theor}

    \begin{proof}
        
    \end{proof}

    Dado un subespacio $M$ de un espacio prehilbertiano $H$ un vector $\vec{x}\in H$, puede no existir la proyección ortogonal de $\vec{x}$ sobre $M$. Esto motiva la siguiente definición:

    \begin{mydef}
        Un subespacio $M$ de $H$ se dice que es \textbf{distinguido} si para cada $\vec{x}\in H$ existe la proyección ortogonal de $\vec{x}$ sobre $M$.
        
    \end{mydef}

    \begin{obs}
        
    \end{obs}

\end{document}