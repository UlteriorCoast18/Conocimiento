\documentclass[12pt]{report}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{subfigure}
\usepackage{lipsum}
\usepackage{array}
\usepackage{multicol}
\usepackage{enumerate}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[a4paper, margin = 1.5cm]{geometry}

%En esta parte se hacen redefiniciones de algunos comandos para que resulte agradable el verlos%

\renewcommand{\theenumii}{\roman{enumii}}

\def\proof{\paragraph{Demostración:\\}}
\def\endproof{\hfill$\blacksquare$}

\def\sol{\paragraph{Solución:\\}}
\def\endsol{\hfill$\square$}

%En esta parte se definen los comandos a usar dentro del documento para enlistar%

\newtheoremstyle{largebreak}
  {}% use the default space above
  {}% use the default space below
  {\normalfont}% body font
  {}% indent (0pt)
  {\bfseries}% header font
  {}% punctuation
  {\newline}% break after header
  {}% header spec

\theoremstyle{largebreak}

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!60!red!30
]{exa}{Ejemplo}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    hidealllines = true,
    roundcorner = 5pt,
    backgroundcolor = gray!50!blue!30
]{obs}{Observación}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{theor}{Teorema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{propo}{Proposición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{cor}{Corolario}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    rightline = false,
    leftline = false
]{lema}{Lema}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    roundcorner=5pt,
    backgroundcolor = gray!30,
    hidealllines = true
]{mydef}{Definición}[section]

\newmdtheoremenv[
    leftmargin=0em,
    rightmargin=0em,
    innertopmargin=0pt,
    innerbottommargin=5pt,
    roundcorner=5pt
]{excer}{Ejercicio}[section]

%En esta parte se colocan comandos que definen la forma en la que se van a escribir ciertas funciones%

\newcommand\abs[1]{\ensuremath{\big|#1\big|}}
\newcommand\divides{\ensuremath{\bigm|}}
\newcommand\cf[3]{\ensuremath{#1:#2\rightarrow#3}}
\newcommand{\contradiction}{\ensuremath{\#_c}}
\newcommand\adj[1]{\ensuremath{\widetilde{#1}}}
\newcommand\pint[2]{\ensuremath{\left(#1\big|#2\right)}}
\newcommand\conj[1]{\ensuremath{\overline{#1}}}
\newcommand\norm[1]{\ensuremath{\|#1\|}}

%recuerda usar \clearpage para hacer un salto de página

\begin{document}
    \title{Ejercicios Análisis Matemático IV}
    \author{Cristo Daniel Alvarado}
    \maketitle

    \tableofcontents %Con este comando se genera el índice general del libro%

    \chapter{Espacios Hilbertianos}

    \section{Ejercicios}

    \renewcommand{\theenumi}{\roman{enumi}}

    \begin{excer}
        Pruebe lo siguiente:
        \begin{enumerate}
            \item Sean $H,H'$ espacios hilbertianos y sea $T$ una aplicación lineal continua de $H$ en $H'$. \textbf{Demuestre} que existe una única aplicación lineal $\cf{\adj{T}}{H'}{H}$ tal que
            \begin{equation*}
                \pint{\vec{x}}{\adj{T}\vec{x'}}=\pint{T\vec{x}}{\vec{x'}},\quad\forall\vec{x}\in H\textup{ y }\forall\vec{x'}\in H'
            \end{equation*}
            \textbf{Pruebe} también que $\adj{T}$ es continua, $\adj{\adj{T}}=T$ y $\norm{\adj{T}}=\norm{T}$. El operador $\adj{T}$ se llama la \textbf{adjunta de $T$}.
            \item \textbf{Demuestre} las reglas:
            \begin{equation*}
                \adj{T_1+T_2}=\adj{T_1}+\adj{T_2}\quad\textup{y}\quad\adj{\alpha T}=\conj{\alpha}\adj{T}
            \end{equation*}
            \item Sea $H''$ un tercer espacio hilbertiano. Sean $T$ una aplicación lineal continua de $H$ en $H'$ y $U$ una aplicación lineal continua de $H'$ en $H''$. \textbf{Pruebe} que:
            \begin{equation*}
                \adj{U\circ T}=\adj{T}\circ\adj{U}
            \end{equation*}
        \end{enumerate}
    \end{excer}

    \begin{proof}
        De (i): Se probarán dos cosas:
        \begin{itemize}
            \item \textbf{Unicidad}. Suponga que existen $\cf{S,W}{H'}{H}$ tales que:
            \begin{equation*}
                \pint{\vec{x}}{S\vec{x'}}=\pint{T\vec{x}}{\vec{x'}}\quad\textup{y}\quad\pint{\vec{x}}{W\vec{x'}}=\pint{T\vec{x}}{\vec{x'}},\quad\forall\vec{x}\textup{ y }\vec{x'}\in H'
            \end{equation*}
            entonces, se tiene que para $\vec{x'}\in H'$ fijo:
            \begin{equation}
                \begin{split}
                    \pint{\vec{x}}{S\vec{x'}}=&\pint{\vec{x}}{W\vec{x'}}\\
                    \Rightarrow \pint{\vec{x}}{S\vec{x'}}-\pint{\vec{x}}{W\vec{x'}}=&0\\
                    \Rightarrow \pint{\vec{x}}{S\vec{x'}-W\vec{x'}}=&0\forall\vec{x}\in H \\
                \end{split}
            \end{equation}
            Por tanto, $S\vec{x'}=W\vec{x'}$. Como el $\vec{x'}\in H'$ fue arbitrario, se sigue que $S=W$.

            \item \textbf{Existencia}. Para cada $\vec{x'}\in H'$, sea $\cf{L_{\vec{x'}}}{H}{\mathbb{K}}$ definida como sigue:
            \begin{equation*}
                L_{\vec{x'}}(\vec{x})=\pint{T\vec{x}}{\vec{x'}}
            \end{equation*}
            Afirmamos que $L_{\vec{x'}}$ es lineal continuo. En efecto, si $\vec{x},\vec{y}\in H$ y $\alpha\in\mathbb{K}$, tenemos que:
            \begin{equation*}
                \begin{split}
                    L_{\vec{x'}}(\vec{x}+\alpha\vec{y})=&\pint{T(\vec{x}+\alpha\vec{y})}{\vec{x'}} \\
                    =&\pint{T\vec{x}}{\vec{x'}}+\alpha\pint{T\vec{y}}{\vec{x'}} \\
                    =& L_{\vec{x'}}(\vec{x})+\alpha L_{\vec{x'}}(\vec{y})\\
                \end{split}
            \end{equation*}
            luego es lineal, y es continuo ya que
            \begin{equation*}
                \begin{split}
                    \abs{L_{\vec{x}}(\vec{x'})}=&\abs{\pint{T\vec{x}}{\vec{x'}}}\\
                    \leq&\norm{T\vec{x}}\norm{\vec{x'}} \\
                    \leq&(\norm{T}\norm{\vec{x'}})\norm{\vec{x}} \\
                \end{split}
            \end{equation*}
            donde la primera desigualdad es por Cauchy-Schwarts, y la segunda es por el hecho de que $T$ es un funcional lineal continuo. Por tanto: $\norm{L_{\vec{x'}}}\leq\norm{T}\norm{\vec{x'}}$. Luego, $L_{\vec{x'}}$ es lineal continuo, i.e. $L_{\vec{x'}}\in H^*$.

            Por el teorema de Riesz, como la aplicación $\cf{G}{H}{H^*}$ es suprayectiva, para $\vec{x'}\in H'$ existe $\adj{T}\vec{x'}\in H$ tal que $L_{\vec{x'}}=G_{\adj{T}\vec{x'}}$, es decir que:
            \begin{equation*}
                L_{\vec{x'}}\vec{x}=\pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T}\vec{x'}}=G_{\adj{T}\vec{x'}}\vec{x},\quad\forall\vec{x}\in H
            \end{equation*}
            Afirmamos que la aplicación $\cf{\adj{T}}{H'}{H}$ está bien definida y es lineal. En efecto, si $\adj{T}\vec{x_1'},\adj{T}\vec{x_2'}\in H$ son tales que $L_{\vec{x'}}=G_{\adj{T}\vec{x_1'}}$ y $L_{\vec{x}}=G_{\adj{T}\vec{x_1'}}$, entonces:
            \begin{equation*}
                \pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T}\vec{x_1'}}\quad\textup{y}\quad\pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T}\vec{x_2'}},\quad\forall\vec{x}\in H
            \end{equation*}
            entonces:
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x}}{\adj{T}\vec{x_1'}}=&\pint{\vec{x}}{\adj{T}\vec{x_2'}},\quad\forall\vec{x}\in H\\
                    \Rightarrow \pint{\vec{x}}{\adj{T}\vec{x_1'}-\adj{T}\vec{x_2'}}=&0\quad\forall\vec{x}\in H\\
                    \Rightarrow\adj{T}\vec{x_1'}-\adj{T}\vec{x_2'}=&\vec{0}\\
                    \Rightarrow\adj{T}\vec{x_1'}=&\adj{T}\vec{x_2'}\\ 
                \end{split}
            \end{equation*}
            por tanto, $\cf{\adj{T}}{H'}{H}$ está bien definida. Comprobemos ahora la linealidad, sean $\vec{x'},\vec{y'}\in H'$, entonces:
            \begin{equation*}
                \begin{split}
                    &\pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T}\vec{x'}},\pint{T\vec{x}}{\vec{y'}}=\pint{\vec{x}}{\adj{T}\vec{y'}}\textup{ y }\pint{T\vec{x}}{\vec{x'}+\vec{y'}}=\pint{\vec{x}}{\adj{T}(\vec{x'}+\vec{y'})}\quad\forall\vec{x}\in H\\
                    \Rightarrow& \pint{T\vec{x}}{\vec{x'}}+\pint{T\vec{x}}{\vec{y'}}=\pint{\vec{x}}{\adj{T}\vec{x'}}+\pint{\vec{x}}{\adj{T}\vec{y'}}\textup{ y }\pint{T\vec{x}}{\vec{x'}+\vec{y'}}=\pint{\vec{x}}{\adj{T}(\vec{x'}+\vec{y'})}\quad\forall\vec{x}\in H\\
                    \Rightarrow& \pint{T\vec{x}}{\vec{x'}+\vec{y'}}=\pint{\vec{x}}{\adj{T}\vec{x'}+\adj{T}\vec{y'}}\textup{ y }\pint{T\vec{x}}{\vec{x'}+\vec{y'}}=\pint{\vec{x}}{\adj{T}(\vec{x'}+\vec{y'})}\quad\forall\vec{x}\in H\\
                    \Rightarrow& \pint{\vec{x}}{\adj{T}\vec{x'}+\adj{T}\vec{y'}}=\pint{\vec{x}}{\adj{T}(\vec{x'}+\vec{y'})}\quad\forall\vec{x}\in H\\
                    \Rightarrow& \pint{\vec{x}}{(\adj{T}\vec{x'}+\adj{T}\vec{y'})-\adj{T}(\vec{x'}+\vec{y'})}=0\quad\forall\vec{x}\in H\\
                    \Rightarrow& (\adj{T}\vec{x'}+\adj{T}\vec{y'})-\adj{T}(\vec{x'}+\vec{y'})=0\\
                    \Rightarrow& \adj{T}\vec{x'}+\adj{T}\vec{y'}=\adj{T}(\vec{x'}+\vec{y'})\\
                \end{split}
            \end{equation*}
            y, si $\alpha\in\mathbb{K}$, tenemos que:
            \begin{equation*}
                \begin{split}
                    &\pint{T\vec{x}}{\alpha\vec{x'}}=\pint{\vec{x}}{\adj{T}(\alpha\vec{x'})}\textup{ y }\pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T}\vec{x'}}\quad\forall\vec{x}\in H\\
                    \Rightarrow&\conj{\alpha} \pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T}(\alpha\vec{x'})}\textup{ y }\conj{\alpha}\pint{T\vec{x}}{\vec{x'}}=\conj{\alpha}\pint{\vec{x}}{\adj{T}\vec{x'}}\quad\forall\vec{x}\in H\\
                    \Rightarrow&\conj{\alpha} \pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T}(\alpha\vec{x'})}\textup{ y }\conj{\alpha}\pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\alpha\adj{T}\vec{x'}}\quad\forall\vec{x}\in H\\
                    \Rightarrow&\pint{\vec{x}}{\adj{T}(\alpha\vec{x'})}=\pint{\vec{x}}{\alpha\adj{T}\vec{x'}}\quad\forall\vec{x}\in H\\
                    \Rightarrow&\pint{\vec{x}}{\adj{T}(\alpha\vec{x'})}-\pint{\vec{x}}{\alpha\adj{T}\vec{x'}}=0\quad\forall\vec{x}\in H\\
                    \Rightarrow&\pint{\vec{x}}{\adj{T}(\alpha\vec{x'})-\alpha\adj{T}\vec{x'}}=0\quad\forall\vec{x}\in H\\
                    \Rightarrow&\adj{T}(\alpha\vec{x'})-\alpha\adj{T}\vec{x'}=\vec{0}\\
                    \Rightarrow&\adj{T}(\alpha\vec{x'})=\alpha\adj{T}\vec{x'}\\
                \end{split}
            \end{equation*}
            por tanto $\adj{T}$ es lineal. Además, se cumple para todos $\vec{x}\in H$ y $\vec{x'}\in H'$ que:
            \begin{equation*}
                \pint{T\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T} \vec{x'}}
            \end{equation*}
        \end{itemize}

        Veamos ahora que es continua, en efecto, por Cauchy-Schwartz se tiene que para todo $\vec{x'}\in H'\backslash\left\{\vec{0} \right\}$:
        \begin{equation*}
            \begin{split}
                \norm{\adj{T}\vec{x'}}^2&=\pint{\adj{T}\vec{x'}}{\adj{T}\vec{x'}}\\
                &=\pint{T(\adj{T}\vec{x'})}{\vec{x'}}\\
                &\leq\abs{\pint{T(\adj{T}\vec{x'})}{\vec{x'}}}\\
                &\leq\norm{T(\adj{T}\vec{x'})}\norm{\vec{x'}} \\
                &\leq\norm{T}\norm{\adj{T}\vec{x'}}\norm{\vec{x'}} \\
            \end{split}
        \end{equation*}
        si $\vec{x'}\in\ker \adj{T}$ es claro que
        \begin{equation*}
            0=\norm{\adj{T}\vec{x'}}\leq\norm{T}\norm{\vec{x'}}
        \end{equation*}
        y, en caso de que no esté, por la ecuación anterior se sigue que:
        \begin{equation*}
            \Rightarrow \norm{\adj{T} \vec{x'}}\leq\norm{T}\norm{\vec{x'}}
        \end{equation*}
        En cuyo caso se sigue que $\adj{T}$ es continua y tal que $\norm{\adj{T}}\leq\norm{T}$. Para ver la igualdad se intercambian los papeles de $T$ y $\adj{T}$ en las desigualdades anteriores, con lo que se obtiene que $\norm{T}\leq\norm{\adj{T}}$.

        Y, para ver que $\adj{\adj{T}}$, notemos que para todo $\vec{x}\in H$ y $\vec{x'}\in H'$
        \begin{equation*}
            \pint{\adj{\adj{T}}\vec{x}}{\vec{x'}}=\pint{\vec{x}}{\adj{T}\vec{x'}}=\pint{T\vec{x}}{\vec{x'}}
        \end{equation*}
        por ende
        \begin{equation*}
            \pint{\adj{\adj{T}}\vec{x}}{\vec{x'}}=\pint{T\vec{x}}{\vec{x'}}
        \end{equation*}
        pero, por unicidad de la adjunta debe suceder que $\adj{\adj{T}}=T$.

        De (ii): Probaremos las dos igualdades.
        \begin{enumerate}
            \item $\adj{T_1+T_2}=\adj{T_1}+\adj{T_2}$. Tenemos que:
            \begin{equation*}
                \pint{\vec{x}}{\adj{T_1}\vec{x'}}=\pint{T_1\vec{x}}{\vec{x'}}\quad\textup{y}\quad\pint{\vec{x}}{\adj{T_2}\vec{x'}}=\pint{T_2\vec{x}}{\vec{x'}},\quad\forall\vec{x}\in H,\vec{x'}\in H'
            \end{equation*}
            por tanto
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x}}{\adj{T_1}\vec{x'}}+\pint{\vec{x}}{\adj{T_2}\vec{x'}}=&\pint{T_1\vec{x}}{\vec{x'}}+\pint{T_2\vec{x}}{\vec{x'}}\quad\forall\vec{x}\in H,\vec{x'}\in H'\\
                    \Rightarrow\pint{\vec{x}}{\adj{T_1}\vec{x'}+\adj{T_2}\vec{x'}}=&\pint{T_1\vec{x}+T_2\vec{x}}{\vec{x'}}\quad\forall\vec{x}\in H,\vec{x'}\in H'\\
                    \Rightarrow \pint{\vec{x}}{(\adj{T_1}+\adj{T_2})\vec{x'}}=&\pint{(T_1+T_2)\vec{x}}{\vec{x'}}\quad\forall\vec{x}\in H,\vec{x'}\in H'\\
                \end{split}
            \end{equation*}
            de la unicidad de la adjunta, se sigue que $\adj{T_1+T_2}=\adj{T_1}+\adj{T_2}$.
            \item $\adj{\alpha T}=\conj{\alpha}\adj{T}$. Es similar al caso anterior.
        \end{enumerate}
        De los dos incisos anteriores se sigue el resultado.

        De (iii): Se tiene que:
        \begin{equation*}
            \pint{\vec{x}}{\adj{T}\vec{x'}}=\pint{T\vec{x}}{\vec{x'}}\quad\textup{y}\quad\pint{\vec{x'}}{\adj{U}\vec{x''}}=\pint{U\vec{x'}}{\vec{x''}},\quad\forall\vec{x}\in H,\vec{x'}\in H',\vec{x''}\in H''
        \end{equation*}
        debemos probar que:
        \begin{equation*}
            \pint{\vec{x}}{(\adj{T}\circ\adj{U})\vec{x''}}=\pint{(U\circ T)\vec{x}}{\vec{x''}}\quad\forall\vec{x}\in H,\vec{x''}\in H''
        \end{equation*}
        para usar la unicidad y de forma inmediata dedudcir el resultado.
        Sean $\vec{x}\in H$ y $\vec{x''}\in H''$. Como $\adj{U}\vec{x''}T\vec{x} \in H'$, tenemos que:
        \begin{equation*}
            \pint{\vec{x}}{\adj{T}(\adj{U}\vec{x''})}=\pint{T\vec{x}}{\adj{U}\vec{x''}}\quad\textup{y}\quad\pint{T\vec{x}}{\adj{U}\vec{x''}}=\pint{U(T\vec{x})}{\vec{x''}}
        \end{equation*}
        por tanto:
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\adj{T}(\adj{U}\vec{x''})}=&\pint{U(T\vec{x})}{\vec{x''}}\\
                \Rightarrow \pint{\vec{x}}{(\adj{T}\circ \adj{U})\vec{x''}}=&\pint{(U\circ T)\vec{x}}{\vec{x''}}\\
            \end{split}
        \end{equation*}
        lo cual prueba el resultado al ser los vectores arbitrarios.

    \end{proof}

    \begin{excer}
        Sea $H$ un espacio hilbertiano complejo. A toda aplicación lineal continua $T$ de $H$ en $H$ se le asocia la aplicación $\cf{Q_T}{H}{\mathbb{C}}$ (llamada \textbf{forma hermitiana}) definida por:
        \begin{equation*}
            Q_T(\vec{x})=\pint{T\vec{x}}{\vec{x}},\quad\forall\vec{x}\in H
        \end{equation*}
        Haga lo siguiente:
        \begin{enumerate}
            \item \textbf{Establezca} la fórmula:
            \begin{equation*}
                \pint{T\vec{x}}{\vec{y}}=\frac{1}{4}\left[Q_T(\vec{x}+\vec{y})-Q_T(\vec{x}-\vec{y})+iQ_T(\vec{x}+i\vec{y})-iQ_T(\vec{x}-i\vec{y})\right]
            \end{equation*}
            \item \textbf{Muestre} que
            \begin{equation*}
                Q_{\adj{T}}(\vec{x})=\conj{Q_T(\vec{x})},\quad\forall\vec{x}\in H
            \end{equation*}
            y que $Q_T(\vec{x})$ es real, $\forall\vec{x}\in H$, si y sólo si $T$ es autoadjunto (es decir, que $T=\adj{T}$).
        \end{enumerate}
    \end{excer}

    \begin{sol}
        Establezcamos ambos incisos:
        
        De (i): Sean $\vec{x},\vec{y}\in H$. Tenemos que:
        \begin{equation*}
            \begin{split}
                Q_T(\vec{x}+\vec{y})=&\pint{T(\vec{x}+\vec{y})}{\vec{x}+\vec{y}}\\
                =&\pint{T(\vec{x})+T(\vec{y})}{\vec{x}+\vec{y}}\\
                =&\pint{T(\vec{x})+T(\vec{y})}{\vec{x}}+\pint{T(\vec{x})+T(\vec{y})}{\vec{y}}\\
                =&\pint{T(\vec{x})}{\vec{x}}+\pint{T(\vec{y})}{\vec{x}}+\pint{T(\vec{x})}{\vec{y}}+\pint{T(\vec{y})}{\vec{y}}\\
                =&Q_T(\vec{x})+\pint{T(\vec{y})}{\vec{x}}+\pint{T(\vec{x})}{\vec{y}}+Q_T(\vec{y})\\
            \end{split}
        \end{equation*}
        por lo cual,
        \begin{equation*}
            \begin{split}
                Q_T(\vec{x}-\vec{y})=&Q_T(\vec{x})+\pint{T(-\vec{y})}{\vec{x}}+\pint{T(\vec{x})}{-\vec{y}}+Q_T(-\vec{y})\\
                =&Q_T(\vec{x})-\pint{T(\vec{y})}{\vec{x}}-\pint{T(\vec{x})}{\vec{y}}+Q_T(\vec{y})\\
            \end{split}
        \end{equation*}
        Luego:
        \begin{equation*}
            \begin{split}
                Q_T(\vec{x}+\vec{y})-Q_T(\vec{x}-\vec{y})&=2\left(\pint{T(\vec{y})}{\vec{x}}+\pint{T(\vec{x})}{\vec{y}}\right)\\
            \end{split}
        \end{equation*}
        y, por ende:
        \begin{equation*}
            \begin{split}
                iQ_T(\vec{x}+i\vec{y})-iQ_T(\vec{x}-i\vec{y})&=2i\left(\pint{T(i\vec{y})}{\vec{x}}+\pint{T(\vec{x})}{i\vec{y}}\right)\\
                &=2i\left(i\pint{T(\vec{y})}{\vec{x}}-i\pint{T(\vec{x})}{\vec{y}}\right)\\
                &=2\left(-\pint{T(\vec{y})}{\vec{x}}+\pint{T(\vec{x})}{\vec{y}}\right)\\
            \end{split}
        \end{equation*}
        Finalmente, se sigue que
        \begin{equation*}
            \begin{split}
                \frac{1}{4}\left[Q_T(\vec{x}+\vec{y})-Q_T(\vec{x}-\vec{y})+iQ_T(\vec{x}+i\vec{y})-iQ_T(\vec{x}-i\vec{y})\right]=&\frac{1}{4}\left[4\pint{T(\vec{x})}{\vec{y}}\right] \\
                =&\pint{T(\vec{x})}{\vec{y}}\\
            \end{split}
        \end{equation*}
        lo cual establece la fórmula.

        De (ii): Sea $\vec{x}\in H$, entonces:
        \begin{equation*}
            \begin{split}
                Q_{\adj{T}}(\vec{x})&=\pint{\adj{T}\vec{x}}{\vec{x}}\\
                &=\pint{\adj{T}\vec{x}}{\vec{x}}\\
                &=\conj{\pint{\vec{x}}{\adj{T}\vec{x}}}\\
                &=\conj{\pint{T\vec{x}}{\vec{x}}}\\
                &=\conj{Q_T(\vec{x})}\\
            \end{split}
        \end{equation*}

        Para la otra parte, veamos que:
        \begin{equation*}
            \begin{split}
                Q_T(\vec{x})\in\mathbb{R},\forall\vec{x}\in H\iff&Q_T(\vec{x})=\conj{Q_T(\vec{x})},\forall\vec{x}\in H\\
                \iff&Q_T(\vec{x})=Q_{\adj{T}}(\vec{x}),\forall\vec{x}\in H\\
                \iff&\pint{T\vec{x}}{\vec{x}}=\pint{\adj{T}\vec{x}}{\vec{x}},\forall\vec{x}\in H\\
                \iff&\pint{T\vec{x}}{\vec{x}}-\pint{\adj{T}\vec{x}}{\vec{x}}=0,\forall\vec{x}\in H\\
                \iff&\pint{T\vec{x}-\adj{T}\vec{x}}{\vec{x}}=0,\forall\vec{x}\in H\\
                \iff&\pint{\left[T-\adj{T}\right]\vec{x}}{\vec{x}}=0,\forall\vec{x}\in H\\
            \end{split}
        \end{equation*}
        Veamos que $\pint{\left[T-\adj{T}\right]\vec{x}}{\vec{x}}=0,\forall\vec{x}\in H$ si y sólo si $T=\adj{T}$.

        $\Rightarrow)$ Suponga que $\pint{\left[T-\adj{T}\right]\vec{x}}{\vec{x}}=0,\forall\vec{x}\in H$. Esto es inmediato, pues se tiene que: $Q_{T-\adj{T}}(\vec{x})=0$, para todo $\vec{x}\in H$, luego
        \begin{equation*}
            \pint{\left[T-\adj{T}\right]\vec{x}}{\vec{y}}=0,\quad\forall\vec{x},\vec{y}\in H
        \end{equation*}
        en particular para $\vec{x}$ fijo, $\pint{\left[T-\adj{T}\right]\vec{x}}{\vec{y}}=0$ para todo $\vec{y}\in H$, luego $\left[T-\adj{T}\right]\vec{x}=\vec{0}$. Como fue arbitrario se sigue entonces que $T=\adj{T}$.

        $\Leftarrow)$ Suponga que $T=\adj{T}$. De forma inmediata se sigue que $\pint{\left[T-\adj{T}\right]\vec{x}}{\vec{x}}=0,\forall\vec{x}\in H$.

    \end{sol}

    \begin{excer}
        Sea $A$ un endomorfismo lineal continuo de un espacio prehilbertiano $H$. Defina $\cf{Q_A}{H}{\mathbb{K}}$ como:
        \begin{equation*}
            Q_A(\vec{x})=\pint{A\vec{x}}{\vec{x}},\quad\forall\vec{x}\in H
        \end{equation*}
        Sea
        \begin{equation*}
            \alpha=\sup\left\{\frac{\abs{Q_A(\vec{x})}}{\norm{\vec{x}}^2}\big| \vec{x}\in H,\vec{x}\neq\vec{0} \right\}
        \end{equation*}
        \begin{enumerate}
            \item \textbf{Pruebe} que $\alpha\leq\norm{A}$.
            \item Al suponer $A$ autoadjunto, \textbf{demuestre} la igualdad opuesta. Luego, si $A$ es autoadjunto se tiene que
            \begin{equation*}
                \norm{A}=\sup\left\{\frac{\abs{Q_A(\vec{x})}}{\norm{\vec{x}}^2}\big| \vec{x}\in H,\vec{x}\neq\vec{0} \right\}
            \end{equation*}
        \end{enumerate}
        \textit{Indicación}. Compruebe que $\forall\vec{x}\in H$ y $\forall\lambda>0$,
        \begin{equation*}
            \pint{A\vec{x}}{A\vec{x}}=\frac{1}{4}\left(Q_A(\lambda\vec{x}+\lambda^{-1}A\vec{x})-Q_A(\lambda\vec{x}-\lambda^{-1}A\vec{x})\right)
        \end{equation*}
        de ahí obtenga que $\norm{A\vec{x}}^2\leq\frac{\alpha}{2}\left(\lambda^2\norm{\vec{x}}^2+\frac{1}{\lambda^2}\norm{A\vec{x}}^2 \right)$ y elija $\lambda$ convenientemente.
    \end{excer}

    \begin{proof}
        Demostremos cada inciso.

        De (i): Basta con ver que $\norm{A}$ es cota superior del conjunto al que se le quiere sacar el supremo. Para ello, notemos que al ser $A$ lineal continuo, se tiene que:
        \begin{equation*}
            \begin{split}
                \abs{\pint{A\vec{x}}{\vec{x}}} \leq \norm{A\vec{x}}\norm{\vec{x}}\leq\norm{A}\norm{\vec{x}}^2
            \end{split}
        \end{equation*}
        para todo $\vec{x}\in H$. En particular, para $\vec{x}\neq\vec{0}$ se tiene que:
        \begin{equation*}
            \begin{split}
                \frac{\abs{\pint{A\vec{x}}{\vec{x}}}}{\norm{\vec{x}}^2}\leq&\norm{A}\\
                \Rightarrow \frac{\abs{Q_A(\vec{x})}}{\norm{\vec{x}}^2}\leq&\norm{A}\\
            \end{split}
        \end{equation*}
        luego, $\norm{A}$ es cota superior del conjunto. Por tanto $\alpha\leq\norm{A}$.

        De (ii): Suponga que $A$ es autoadjunto. Sean $\vec{x}\in H$ y $\lambda>0$. Entonces:
        \begin{equation*}
            \begin{split}
                Q_A(\lambda\vec{x}+\lambda^{-1}A\vec{x})=&\pint{A(\lambda\vec{x}+\lambda^{-1}A\vec{x})}{\lambda\vec{x}+\lambda^{-1}A\vec{x}} \\
                =&\pint{\lambda A\vec{x}+\lambda^{-1}(A\circ A)\vec{x}}{\lambda\vec{x}+\lambda^{-1}A\vec{x}} \\
                =&\pint{\lambda A\vec{x}+\lambda^{-1}(A\circ A)\vec{x}}{\lambda\vec{x}}+\pint{\lambda A\vec{x}+\lambda^{-1}(A\circ A)\vec{x}}{\lambda^{-1}A\vec{x}}\\
                =&\pint{\lambda A\vec{x}}{\lambda\vec{x}}+\pint{\lambda^{-1}(A\circ A)\vec{x}}{\lambda\vec{x}}+\pint{\lambda A\vec{x}}{\lambda^{-1}A\vec{x}}+\pint{\lambda^{-1}(A\circ A)\vec{x}}{\lambda^{-1}A\vec{x}}\\
            \end{split}
        \end{equation*}
        y
        \begin{equation*}
            \begin{split}
                Q_A(\lambda\vec{x}-\lambda^{-1}A\vec{x})=&\pint{A(\lambda\vec{x}-\lambda^{-1}A\vec{x})}{\lambda\vec{x}-\lambda^{-1}A\vec{x}} \\
                =&\pint{\lambda A\vec{x}-\lambda^{-1}(A\circ A)\vec{x}}{\lambda\vec{x}-\lambda^{-1}A\vec{x}} \\
                =&\pint{\lambda A\vec{x}-\lambda^{-1}(A\circ A)\vec{x}}{\lambda\vec{x}}-\pint{\lambda A\vec{x}-\lambda^{-1}(A\circ A)\vec{x}}{\lambda^{-1}A\vec{x}}\\
                =&\pint{\lambda A\vec{x}}{\lambda\vec{x}}-\pint{\lambda^{-1}(A\circ A)\vec{x}}{\lambda\vec{x}}-\pint{\lambda A\vec{x}}{\lambda^{-1}A\vec{x}}+\pint{\lambda^{-1}(A\circ A)\vec{x}}{\lambda^{-1}A\vec{x}}\\
            \end{split}
        \end{equation*}
        por tanto:
        \begin{equation*}
            \begin{split}
                Q_A(\lambda\vec{x}+\lambda^{-1}A\vec{x})-Q_A(\lambda\vec{x}-\lambda^{-1}A\vec{x})=&2(\pint{\lambda^{-1}(A\circ A)\vec{x}}{\lambda\vec{x}}+\pint{\lambda A\vec{x}}{\lambda^{-1}A\vec{x}})\\
                =&2(\pint{(A\circ A)\vec{x}}{\vec{x}}+\pint{A\vec{x}}{A\vec{x}})\\
                =&2(\pint{A\vec{x}}{A\vec{x}}+\pint{A\vec{x}}{A\vec{x}})\\
                =&4\pint{A\vec{x}}{A\vec{x}}\\
            \end{split}
        \end{equation*}
        pues, $A$ es autoadjunto. Luego:
        \begin{equation*}
            \pint{A\vec{x}}{A\vec{x}}=\frac{1}{4}(Q_A(\lambda\vec{x}+\lambda^{-1}A\vec{x})-Q_A(\lambda\vec{x}-\lambda^{-1}A\vec{x}))
        \end{equation*}
        por tanto:
        \begin{equation*}
            \begin{split}
                \norm{A\vec{x}}^2=&\frac{1}{4}\abs{Q_A(\lambda\vec{x}+\lambda^{-1}A\vec{x})-Q_A(\lambda\vec{x}-\lambda^{-1}A\vec{x})}\\
                \leq&\frac{1}{4}(\abs{Q_A(\lambda\vec{x}+\lambda^{-1}A\vec{x})}+\abs{Q_A(\lambda\vec{x}-\lambda^{-1}A\vec{x})})\\
                =&\frac{1}{4}(\frac{\norm{\lambda\vec{x}+\lambda^{-1}A\vec{x}}^2}{\norm{\lambda\vec{x}+\lambda^{-1}A\vec{x}}^2} \abs{Q_A(\lambda\vec{x}+\lambda^{-1}A\vec{x})}+\frac{\norm{\lambda\vec{x}-\lambda^{-1}A\vec{x}}^2}{\norm{\lambda\vec{x}-\lambda^{-1}A\vec{x}}^2}\abs{Q_A(\lambda\vec{x}-\lambda^{-1}A\vec{x})})\\
                \leq&\frac{1}{4}(\alpha\norm{\lambda\vec{x}+\lambda^{-1}A\vec{x}}^2+\alpha\norm{\lambda\vec{x}-\lambda^{-1}A\vec{x}}^2)\\
                =&\frac{\alpha}{4}(\pint{\lambda\vec{x}+\lambda^{-1}A\vec{x}}{\lambda\vec{x}+\lambda^{-1}A\vec{x}}+\pint{\lambda\vec{x}-\lambda^{-1}A\vec{x}}{\lambda\vec{x}-\lambda^{-1}A\vec{x}})\\
                =&\frac{\alpha}{4}(\pint{\lambda\vec{x}+\lambda^{-1}A\vec{x}}{\lambda\vec{x}}+\pint{\lambda\vec{x}+\lambda^{-1}A\vec{x}}{\lambda^{-1}A\vec{x}}+\pint{\lambda\vec{x}-\lambda^{-1}A\vec{x}}{\lambda\vec{x}}-\pint{\lambda\vec{x}-\lambda^{-1}A\vec{x}}{\lambda^{-1}A\vec{x}})\\
                =&\frac{\alpha}{4}(\lambda^2\pint{\vec{x}}{\vec{x}}+\pint{A\vec{x}}{\vec{x}}+\pint{\vec{x}}{A\vec{x}}+\lambda^{-2}\pint{A\vec{x}}{A\vec{x}}+\lambda^2\pint{\vec{x}}{\vec{x}}-\pint{A\vec{x}}{\vec{x}}-\pint{\vec{x}}{A\vec{x}}+\lambda^{-2}\pint{A\vec{x}}{A\vec{x}})\\
                =&\frac{\alpha}{2}(\lambda^2\pint{\vec{x}}{\vec{x}}+\lambda^{-2}\pint{A\vec{x}}{A\vec{x}})\\
                =&\frac{\alpha}{2}(\lambda^2\norm{\vec{x}}^2+\lambda^{-2}\norm{A\vec{x}}^2)\\
            \end{split}
        \end{equation*}
        por Cauchy-Schwartz y usando la definición de $\alpha$. Por tanto, si consideramos que $\alpha>0$:
        \begin{equation*}
            \begin{split}
                \norm{A\vec{x}}^2&\leq\frac{\alpha}{2}(\lambda^2\norm{\vec{x}}^2+\lambda^{-2}\norm{A\vec{x}}^2)\\
                \Rightarrow \left(1-\frac{\alpha}{2\lambda^2}\right)\norm{A\vec{x}}^2&\leq\frac{\alpha\lambda^2}{2}\norm{\vec{x}}^2\\
                \Rightarrow \frac{2\lambda^2-\alpha}{2\lambda^2}\norm{A\vec{x}}^2&\leq\frac{\alpha\lambda^2}{2}\norm{\vec{x}}^2\\
                \Rightarrow \norm{A\vec{x}}^2&\leq\frac{2\alpha\lambda^4}{2(2\lambda^2-\alpha)}\norm{\vec{x}}^2\\
                \Rightarrow \norm{A\vec{x}}^2&\leq\frac{\alpha\lambda^4}{2\lambda^2-\alpha}\norm{\vec{x}}^2\\
            \end{split}
        \end{equation*}
        tomemos $\lambda>0$ tal que:
        \begin{equation*}
            \begin{split}
                \alpha^2=\frac{\alpha\lambda^4}{2\lambda^2-\alpha}&\iff\alpha=\frac{\lambda^4}{2\lambda^2-\alpha}\\
                &\iff\alpha(2\lambda^2-\alpha)=\lambda^4\\
                &\iff0=\lambda^4-2\alpha\lambda^2+\alpha^2\\
                &\iff0=\left(\lambda^2-\alpha\right)^2 \\
                &\iff0=\left(\lambda^2-\alpha\right)^2 \\
                &\iff0=\lambda^2-\alpha \\
                &\iff\sqrt{\alpha}=\lambda \\
            \end{split}
        \end{equation*}
        de esta forma:
        \begin{equation*}
            \begin{split}
                \norm{A\vec{x}}^2&\leq\alpha^2\norm{\vec{x}}^2 \\
                \norm{A\vec{x}}&\leq\alpha\norm{\vec{x}}\\
            \end{split}
        \end{equation*}
        es decir que $\norm{A}\leq\alpha$ y por ende $\alpha=\norm{A}$, esto si $\alpha>0$. Si $\alpha=0$, entonces:
        \begin{equation*}
            \begin{split}
                \frac{\abs{Q_A(\vec{x})}}{\norm{\vec{x}}^2}=&0\quad\forall\vec{x}\in H\\
                \Rightarrow \abs{Q_A(\vec{x})}=&0\quad\forall\vec{x}\in H\\
                \Rightarrow \pint{A\vec{x}}{\vec{x}}=&0\quad\forall\vec{x}\in H\\
            \end{split}
        \end{equation*}
        pero, por (i) de 1.4 se sigue que $A=0$, pues $\pint{A\vec{x}}{\vec{y}}=0$ para todo $\vec{x},\vec{y}\in H\backslash\left\{\vec{0} \right\}$. En este caso $\alpha=0=\norm{A}$. En cualquier caso, se concluye que $\alpha=\norm{A}$.
    \end{proof}

    \begin{excer}
        \textbf{Muestre} que todo endomorfismo continuo $T$ de un espacio hilbertiano $H$ se expresa únicamente en la forma:
        \begin{equation*}
            T=A+iB
        \end{equation*}
        donde $A$ y $B$ son endomorfismos autoadjuntos de $H$.
    \end{excer}

    \begin{proof}
        Tomemos $A=\frac{1}{2}(T+\adj{T})$ y $B=\frac{1}{2i}(T-\adj{T})$, siendo $\cf{\adj{T}}{H}{H}$ la adjunta de $T$. Es claro que $T=A+iB$ y, que tanto $A$ como $B$ son adjuntos, pues:
        \begin{equation*}
            \begin{split}
                \adj{A}=&\adj{\frac{1}{2}(T+\adj{T})} \\
                =&\frac{1}{2}\adj{(T+\adj{T})} \\
                =&\frac{1}{2}(\adj{T}+\adj{\adj{T}}) \\
                =&\frac{1}{2}(T+\adj{T}) \\
                =&A\\
            \end{split}
        \end{equation*}
        y
        \begin{equation*}
            \begin{split}
                \adj{B}=&\adj{\frac{1}{2i}(T-\adj{T})} \\
                =&\adj{\frac{-i}{2}(T-\adj{T})} \\
                =&\conj{-\frac{i}{2}}\adj{(T-\adj{T})} \\
                =&\frac{i}{2}(\adj{T}-\adj{\adj{T}}) \\
                =&-\frac{1}{2i}(\adj{T}-T) \\
                =&\frac{1}{2i}(T-\adj{T}) \\
                =&B \\
            \end{split}
        \end{equation*}
        además, son endomorfismos (pues van de $H$ en $H$). Veamos que éstos son únicos. En efecto, si $\cf{A',B'}{H}{H}$ son lineales adjuntos tales que
        \begin{equation*}
            T=A'+iB'
        \end{equation*}
        se tiene que:
        \begin{equation*}
            i(B'-B)=A-A'
        \end{equation*}
        en particular, son continuos, por lo cual podemos tomar la adjunta de ambos lados:
        \begin{equation*}
            \begin{split}
                \adj{i(B'-B)}&=\adj{A-A'} \\
                \Rightarrow -i\adj{(B'-B)}&=A-A'\\
                \Rightarrow -i(B'-B)&=A-A'\\
                \Rightarrow -i(B'-B)&=i(B'-B)\\
            \end{split}
        \end{equation*}
        ya que son adjuntos. Por tanto $B'-B=0\Rightarrow B'=B$, con lo cual $A=A'$.
    \end{proof}

    \begin{excer}
        Sea $H$ un espacio prehilbertiano. \textbf{Construya} un espacio hilbertiano $\hat{H}$ y una inyección lineal $\cf{j}{H}{\hat{H}}$ tal que
        \begin{equation*}
            \pint{j\vec{x}}{j\vec{y}}=\pint{\vec{x}}{\vec{y}},\quad\forall\vec{x},\vec{y}\in H.
        \end{equation*}
        y que $j(H)$ sea denso en $\hat{H}$. El espacio hilbertiano $\hat{H}$ se llama la \textbf{completación} del espacio prehilibertiano $H$. \textbf{Formule y demuestre} un teorema de unicidad de esta completación.
    \end{excer}

    \begin{proof}
        Sea
        \begin{equation*}
            \hat{H}=\left\{\left\{\vec{x_n} \right\}_{n=1}^\infty\Big|\left\{\vec{x_n} \right\}_{n=1}^\infty\textup{ es sucesión de Cauchy en }H \right\}
        \end{equation*}
        se definen sobre $\hat{H}'$ dos operaciones, para todo $\hat{x'}=\left\{\vec{x_n} \right\}_{n=1}^\infty,\hat{y'}=\left\{\vec{y_n} \right\}_{n=1}^\infty\in \hat{H}'$ y $\alpha\in K$:
        \begin{equation*}
            \hat{x'}+\hat{y'}=\left\{\vec{x_n}+\vec{y_n} \right\}_{n=1 }^\infty\quad\textup{y}\quad\alpha\hat{x'}=\left\{\alpha\vec{x_n} \right\}_{n=1 }^\infty
        \end{equation*}
        Con estas operaciones $\hat{H}'$ es un espacio vectorial sobre $\mathbb{K}$. Definimos una relación $\sim$ en $\hat{H}'$ dada como sigue:
        \begin{equation*}
            \hat{x'}\sim\hat{y'}\iff\lim_{n\rightarrow\infty }\norm{\vec{x_n}-\vec{y_n}}=0
        \end{equation*}
        donde $\norm{\cdot}$ es la norma inducida por el producto interno sobre $H$. Tomemos $\hat{0'}=\left\{\vec{0} \right\}_{ n=1}^\infty\in\hat{H}'$, y sea:
        \begin{equation*}
            \hat{K}=\left\{\hat{x'}\in\hat{H}'\Big|\hat{x'}\sim\hat{0'} \right\}
        \end{equation*}
        Afirmamos que $\hat{K}$ es subespacio vectorial de $\hat{H}$. En efecto, sean $\hat{x'}=\left\{\vec{x_n} \right\}_{n=1}^\infty,\hat{y'}=\left\{\vec{y_n} \right\}_{n=1}^\infty\in \hat{K}'$ y $\alpha\in\mathbb{K}$, entonces:
        \begin{equation*}
            \begin{split}
                0\leq&\lim_{n\rightarrow\infty}\norm{\vec{x_n}+\alpha\vec{y_n}-\vec{0}}\\
                =&\lim_{n\rightarrow\infty}\norm{\vec{x_n}+\alpha\vec{y_n}}\\
                \leq&\lim_{n\rightarrow\infty}\norm{\vec{x_n}}+\lim_{n\rightarrow\infty}\norm{\alpha\vec{y_n}}\\
                =&\lim_{n\rightarrow\infty}\norm{\vec{x_n}}+\lim_{n\rightarrow\infty}\abs{\alpha}\cdot\norm{\vec{y_n}}\\
                =&\lim_{n\rightarrow\infty}\norm{\vec{x_n}-\vec{0}}+\abs{\alpha}\lim_{n\rightarrow\infty}\norm{\vec{y_n}-\vec{0}}\\
                =&0+\abs{\alpha}\cdot0\\
                =&0\\
            \end{split}
        \end{equation*}
        por tanto, $\hat{x'}+\alpha\hat{y'}\in\hat{K}'$. Así $\hat{K}'$ es espacio vectorial. Tomemos
        \begin{equation*}
            \hat{H}=\hat{H}'/\hat{K}'
        \end{equation*}
        el espacio vectorial cociente, cuyos elementos los denotaremos por $\hat{x}=[\hat{x'}]=\hat{x'}+\hat{K}'$. Definimos un producto escalar en $\hat{H}$ como sigue; para cada $\hat{x},\hat{y}\in H$:
        \begin{equation*}
            \pint{\hat{x}}{\hat{y}}=\lim_{n\rightarrow\infty }\pint{\vec{x_n}}{\vec{y_n}}
        \end{equation*}
        %TODO%
    \end{proof}

    \begin{excer}
        Si $E$ es un espacio vectorial complejo, la adición de elementos de $E$ y la multiplicación de elementos de $E$ por números reales, hacen de $E$ un espacio vectorial real que se designa por $E_\mathbb{R}$.
        \begin{enumerate}
            \item Sea $H$ un espacio prehilbertiano complejo. Se designa por $\pint{\vec{x}}{\vec{y}}$ un producto escalar en $H$. \textbf{Muestre} que la aplicación:
            \begin{equation*}
                (\vec{x},\vec{y})\mapsto\pint{\vec{x}}{\vec{y}}_\mathbb{R}=\Re\pint{\vec{x}}{\vec{y}}
            \end{equation*}
            hace de $H_\mathbb{R}$ un espacio prehilbertiano real para el que se cumple:
            \begin{equation*}
                \pint{i\vec{x}}{i\vec{y}}_\mathbb{R}=\pint{\vec{x}}{\vec{y}}_\mathbb{R}
            \end{equation*}
            \textbf{Pruebe} la relación:
            \begin{equation}
                \pint{\vec{x}}{\vec{y}}=\pint{\vec{x}}{\vec{y}}_\mathbb{R}+i\pint{\vec{x}}{i\vec{y}}_\mathbb{R}
            \end{equation}
            \item Sea $H$ un espacio vectorial complejo. Se supone que $H_\mathbb{R}$ está provisto de un producto escalar $(\vec{x},\vec{y})\mapsto\pint{\vec{x}}{\vec{y}}_\mathbb{R}$ que hace de $H_\mathbb{R}$ un espacio prehilbertiano real. Se supone también que $\pint{i\vec{x}}{i\vec{y}}_\mathbb{R}=\pint{\vec{x}}{\vec{y}}_\mathbb{R}$, para todo $\vec{x},\vec{y}\in H$. Se define en $H$ un producto $\pint{\vec{x}}{\vec{y}}$ por la fórmula (1.1). \textbf{Demuestre} que $(\vec{x},\vec{y})\mapsto\pint{\vec{x}}{\vec{y}}$ es un producto escalar complejo que hace de $H$ un espacio prehilbertiano complejo. 
        \end{enumerate}
    \end{excer}

    \begin{proof}
        De (i): Hay que verificar que se cumplen cuatro condiciones:
        \begin{enumerate}
            \item Para todo $\vec{y}\in H_{\mathbb{R}}$ fijo, la aplicación $\vec{x} \mapsto\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}$ es lineal de $H_{\mathbb{R}}$ en $\mathbb{R}$. En efecto, sea $\vec{y}\in H_{\mathbb{R}}$. Si $\vec{x_1},\vec{x_2},\vec{x} \in H_{\mathbb{R}}$ y $\alpha\in\mathbb{R}$, tenemos que
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x_1}+\vec{x_2}}{\vec{y}}_{\mathbb{R}}&=\Re\pint{\vec{x_1}+\vec{x_2}}{\vec{y}}\\
                    &=\Re[\pint{\vec{x_1}}{\vec{y}}+\pint{\vec{x_2}}{\vec{y}}]\\
                    &=\Re\pint{\vec{x_1}}{\vec{y}}+\Re\pint{\vec{x_2}}{\vec{y}}\\
                    &=\pint{\vec{x_1}}{\vec{y}}_{\mathbb{R}}+\Re\pint{\vec{x_2}}{\vec{y}}_{\mathbb{R}}\\
                \end{split}
            \end{equation*}
            y,
            \begin{equation*}
                \begin{split}
                    \pint{\alpha\vec{x}}{\vec{y}}_{\mathbb{R}}&=\Re\pint{\alpha\vec{x}}{\vec{y}}\\
                    &=\Re\alpha\pint{\vec{x}}{\vec{y}}\\
                    &=\alpha\Re\pint{\vec{x}}{\vec{y}}\\
                    &=\alpha\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}\\
                \end{split}
            \end{equation*}
            con lo cual, la aplicación es lineal.
            \item $\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}=\conj{\pint{\vec{y}}{\vec{x}}_{\mathbb{R}}}=\pint{\vec{y}}{\vec{x}}_{\mathbb{R}}$, para todo $\vec{x},\vec{y}\in H_{\mathbb{R}}$. En efecto, sean $\vec{x},\vec{y}\in H_{\mathbb{R}}$, se tiene que:
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x}}{\vec{y}}_{\mathbb{R}}&=\Re\pint{\vec{x}}{\vec{y}} \\
                    &=\Re\conj{\pint{\vec{y}}{\vec{x}}} \\
                    &=\Re\pint{\vec{y}}{\vec{x}} \\
                    &=\pint{\vec{y}}{\vec{x}}_{\mathbb{R}}\\
                    &=\conj{\pint{\vec{y}}{\vec{x}}_{\mathbb{R}}}\\
                \end{split}
            \end{equation*}
            pues, el producto escalar toma valores reales.
            \item $\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}\geq0$, para todo $\vec{x}\in H_{\mathbb{R}}$. En efecto, como $\pint{\cdot}{\cdot}$ es un producto escalar sobre $H$, se cumple que $\pint{\vec{x}}{\vec{x}}\geq0$, por ende $\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}=\pint{\vec{x}}{\vec{x}}\geq0$.
            \item Sea $\vec{x}\in H_{\mathbb{R}}$, entonces $\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}=0$ si y sólo si $\vec{x}=\vec{0}$. La vuelta es inmediata, suponga que $\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}=0$, como $\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}=\Re\pint{\vec{x}}{\vec{x}}=\pint{\vec{x}}{\vec{x}}$, se sigue que $\vec{x}=\vec{0}$.
        \end{enumerate}
        con lo cual, por los 4 incisos anteriores se sigue que $\pint{\cdot}{\cdot}_{\mathbb{R}}$ es un producto escalar sobre $H_{\mathbb{R}}$, es decir que $H_{\mathbb{R}}$ es un espacio prehilbertiano real.
        
        Verifiquemos que se cumple que:
        \begin{equation*}
            \pint{i\vec{x}}{i\vec{y}}_{\mathbb{R}}=\pint{\vec{x}}{\vec{y}}_{\mathbb{R}},\quad\forall\vec{x},\vec{y}\in H_{\mathbb{R}}
        \end{equation*}
        en efecto, si $\vec{x},\vec{y}\in H_{\mathbb{R}}$, entonces:
        \begin{equation*}
            \begin{split}
                \pint{i\vec{x}}{i\vec{y}}_{\mathbb{R}}&=\Re\pint{i\vec{x}}{i\vec{y}}\\
                &=\Re(i\cdot(-i))\pint{\vec{x}}{\vec{y}}\\
                &=\Re(-(-1))\pint{\vec{x}}{\vec{y}}\\
                &=\Re\pint{\vec{x}}{\vec{y}}\\
                &=\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}\\
            \end{split}
        \end{equation*}
        con lo que se verifica la igualdad. Probemos la relación. Sean $\vec{x},\vec{y}\in H$, se tiene entonces que:
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{y}}&=\Re\pint{\vec{x}}{\vec{y}}+i\Im\pint{\vec{x}}{\vec{y}}\\
                &=\Re\pint{\vec{x}}{\vec{y}}+i\Re(-i\pint{\vec{x}}{\vec{y}})\\
                &=\Re\pint{\vec{x}}{\vec{y}}+i\Re(\conj{i}\pint{\vec{x}}{\vec{y}})\\
                &=\Re\pint{\vec{x}}{\vec{y}}+i\Re(\pint{\vec{x}}{i\vec{y}})\\
                &=\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}+i\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
            \end{split}
        \end{equation*}
        lo cual prueba la relación.

        De (ii): Hay que verificar que se cumplen cuatro condiciones:
        \begin{enumerate}
            \item Para todo $\vec{y}\in H$ fijo, la aplicación $\vec{x}\mapsto\pint{\vec{x}}{\vec{y}}$ es lineal de $H$ en $\mathbb{C}$. En efecto, sea $\vec{y}\in H$. Si $\vec{x_1},\vec{x_2},\vec{x}\in H$ y $\alpha=a+ib\in\mathbb{C}$, se tiene que:
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x_1}+\vec{x_2}}{\vec{y}}&=\pint{\vec{x_1}+\vec{x_2}}{\vec{y}}_{\mathbb{R}}+i\pint{\vec{x_1}+\vec{x_2}}{i\vec{y}}_{\mathbb{R}}\\
                    &=\pint{\vec{x_1}}{\vec{y}}_{\mathbb{R}}+\pint{\vec{x_2}}{\vec{y}}_{\mathbb{R}}+i\pint{\vec{x_1}}{i\vec{y}}_{\mathbb{R}}+i\pint{\vec{x_2}}{i\vec{y}}_{\mathbb{R}}\\
                    &=(\pint{\vec{x_1}}{\vec{y}}_{\mathbb{R}}i\pint{\vec{x_1}}{i\vec{y}}_{\mathbb{R}})+(\pint{\vec{x_2}}{\vec{y}}_{\mathbb{R}}+i\pint{\vec{x_2}}{i\vec{y}}_{\mathbb{R}})\\
                    &=\pint{\vec{x_1}}{\vec{y}}+\pint{\vec{x_2}}{\vec{y}}\\
                \end{split}
            \end{equation*}
            y,
            \begin{equation*}
                \begin{split}
                    \pint{\alpha\vec{x}}{\vec{y}}&=\pint{\alpha\vec{x}}{\vec{y}}_{\mathbb{R}}+i\pint{\alpha\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                    &=\pint{[a+ib]\vec{x}}{\vec{y}}_{\mathbb{R}}+i\pint{[a+ib]\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                    &=\pint{a\vec{x}}{\vec{y}}_{\mathbb{R}}+\pint{ib\vec{x}}{\vec{y}}_{\mathbb{R}}+i\pint{a\vec{x}}{i\vec{y}}_{\mathbb{R}}+i\pint{ib\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                    &=a\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}+b\pint{i\vec{x}}{\vec{y}}_{\mathbb{R}}+ia\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}+ib\pint{i\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                    &=a\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}-b\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}+ia\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}+ib\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}\\
                    &=(a+ib)\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}+(ia-b)\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                    &=(a+ib)\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}+i(a+ib)\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                    &=\alpha\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}+i\alpha\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                    &=\alpha(\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}+i\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}})\\
                    &=\alpha\pint{\vec{x}}{\vec{y}}
                \end{split}
            \end{equation*}
            por tanto, es lineal de $H$ en $\mathbb{C}$.
            \item Sean $\pint{\vec{x}}{\vec{y}}=\conj{\pint{\vec{y}}{\vec{x}}}$, para todo $\vec{x},\vec{y}\in H$. En efecto, si $\vec{x},\vec{y}\in H$, se tiene que:
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x}}{\vec{y}}&=\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}+i\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                    &=\pint{\vec{y}}{\vec{x}}_{\mathbb{R}}+i\pint{i\vec{x}}{-\vec{y}}_{\mathbb{R}}\\
                    &=\pint{\vec{y}}{\vec{x}}_{\mathbb{R}}-i\pint{\vec{y}}{i\vec{x}}_{\mathbb{R}}\\
                    &=\conj{\pint{\vec{y}}{\vec{x}}_{\mathbb{R}}+i\pint{\vec{y}}{i\vec{x}}_{\mathbb{R}}}\\
                    &=\conj{\pint{\vec{y}}{\vec{x}}}\\
                \end{split}
            \end{equation*}
            \item $\pint{\vec{x}}{\vec{x}}\geq 0$ para todo $\vec{x}\in H$. En efecto, si $\vec{x}\in H$, se tiene primeramente que:
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x}}{i\vec{x}}&=\pint{i\vec{x}}{-\vec{x}}\\
                    &=-\pint{i\vec{x}}{\vec{x}}\\
                    &=-\pint{\vec{x}}{i\vec{x}}\\
                    \Rightarrow \pint{\vec{x}}{i\vec{x}} &=0\\
                \end{split}
            \end{equation*}
            por tanto,
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x}}{\vec{x}}&=\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}+i\pint{\vec{x}}{i\vec{x}}_{\mathbb{R}}\\
                    &=\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}\\
                    &\geq0\\
                \end{split}
            \end{equation*}
            donde $\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}\geq0$. Luego se tiene el resultado.
            \item Sea $\vec{x}\in H$. Entonces, $\pint{\vec{x}}{\vec{x}}=0$ si y sólo si $\vec{x}=\vec{0}$. La vuelta es inmediata. Suponga que $\pint{\vec{x}}{\vec{x}}=0$, entonces:
            \begin{equation*}
                0=\pint{\vec{x}}{\vec{x}}=\pint{\vec{x}}{\vec{x}}_{\mathbb{R}}
            \end{equation*}
            usando lo obtenido en (iii), pero $\pint{\vec{x}}{\vec{x}}=0$ implica $\vec{x}=\vec{0}$, luego $\vec{x}=\vec{0}$ con lo que se tiene el resultado.
        \end{enumerate}
        por los cuatro incisos se sigue que $\pint{\cdot}{\cdot}$ es un producto escalar complejo sobre $H$ que hace de él un espacio prehilbertiano complejo.
    \end{proof}

    \begin{excer}
        Haga lo sugiente:
        \begin{enumerate}
            \item \textbf{Muestre} que en todo espacio prehilbertiano real se cumple
            \begin{equation*}
                \pint{\vec{x}}{\vec{y}}=\frac{1}{4}\left(\norm{\vec{x}+\vec{y}}^2-\norm{\vec{x}-\vec{y}}^2 \right)
            \end{equation*}
            y en todo espacio prehilbertiano complejo se cumple
            \begin{equation*}
                \pint{\vec{x}}{\vec{y}}=\frac{1}{4}\left(\norm{\vec{x}+\vec{y}}^2-\norm{\vec{x}-\vec{y}}^2+i\norm{\vec{x}+i\vec{y}}^2-i\norm{\vec{x}-i\vec{y}}^2 \right)
            \end{equation*}
            \item Sea $E$ un espacio vectorial normado real en el que se verifica la identidad del paralelogramo:
            \begin{equation*}
                \norm{\vec{x}+\vec{y}}^2+\norm{\vec{x}-\vec{y}}^2=2(\norm{\vec{x}}^2+\norm{\vec{y}}^2)
            \end{equation*}
            \textbf{Pruebe} que se puede definir de manera única un producto escalar $\pint{\cdot}{\cdot}$ sobre $E$ que hace de $E$ un espacio prehilbertiano real para el cual $\norm{\vec{x}}^2=\pint{\vec{x}}{\vec{x}}$, $\forall\vec{x}\in E$.

            \textit{Indicación.} Defina $\pint{\vec{x}}{\vec{y}}$ por la primera fórmula del inciso (i). Usando la fórmula del paralelogramo compruebe que $\pint{\vec{x}}{2\vec{y}}=2\pint{\vec{x}}{\vec{y}}$. Transforme $\pint{\vec{x_1}}{\vec{y_1}}+\pint{\vec{x_2}}{\vec{y_2}}$ por la identidad del paralelogramo y deduzca la fórmula $\pint{\vec{x_1}}{\vec{y}}+\pint{\vec{x_2}}{\vec{y}}=\pint{\vec{x_1}+\vec{x_2}}{\vec{y}}$.

            \item Misma pregunta que en (ii) en el caso de ser $E$ espacio vectorial complejo. 
            
            \textit{Indicación.} Use (ii) y el problema 1.6.
        \end{enumerate}
    \end{excer}

    \begin{sol}
        De (i): Sea $H$ un espacio prehilbertiano real, y sean $\vec{x},\vec{y}\in H$, se tiene entonces que:
        \begin{equation*}
            \begin{split}
                \frac{1}{4}\left(\norm{\vec{x}+\vec{y}}^2-\norm{\vec{x}-\vec{y}}^2 \right)
                &=\frac{1}{4}\left(\pint{\vec{x}+\vec{y}}{\vec{x}+\vec{y}}-\pint{\vec{x}-\vec{y}}{\vec{x}-\vec{y}} \right) \\
                &=\frac{1}{4}\left(\pint{\vec{x}}{\vec{x}}+\pint{\vec{x}}{\vec{y}}+\pint{\vec{y}}{\vec{x}}+\pint{\vec{y}}{\vec{y}}-\pint{\vec{x}}{\vec{x}}+\pint{\vec{x}}{\vec{y}}+\pint{\vec{y}}{\vec{x}}-\pint{\vec{y}}{\vec{y}} \right) \\
                &=\frac{1}{4}\left(2\pint{\vec{x}}{\vec{y}}+2\pint{\vec{y}}{\vec{x}}\right) \\
                &=\frac{1}{4}\left(4\pint{\vec{x}}{\vec{y}}\right) \\
                &=\pint{\vec{x}}{\vec{y}}\\
            \end{split}
        \end{equation*}
        para un espacio prehilbertiano complejo, sean $\vec{x},\vec{y}\in H$. Por el ejercicio 1.1.6 (i) se tiene que:
        \begin{equation*}
            \begin{split}
                \pint{\vec{x}}{\vec{y}}&=\pint{\vec{x}}{\vec{y}}_{\mathbb{R}}+i\pint{\vec{x}}{i\vec{y}}_{\mathbb{R}}\\
                &=\frac{1}{4}\left(\norm{\vec{x}+\vec{y}}_{\mathbb{R}}^2-\norm{\vec{x}-\vec{y}}_{\mathbb{R}}^2 \right)+i\frac{1}{4}\left(\norm{\vec{x}+i\vec{y}}_{\mathbb{R}}^2-\norm{\vec{x}-i\vec{y}}_{\mathbb{R}}^2 \right)\\
                &=\frac{1}{4}\left(\norm{\vec{x}+\vec{y}}^2-\norm{\vec{x}-\vec{y}}^2+i\norm{\vec{x}+i\vec{y}}^2-i\norm{\vec{x}-i\vec{y}}^2 \right)\\
            \end{split}
        \end{equation*}
        pues, $\norm{\cdot}_{\mathbb{R}}=\norm{\cdot}$.

        De (ii). Defina:
        \begin{equation*}
            \pint{\vec{x}}{\vec{y}}=\frac{1}{4}\left(\norm{\vec{x}+\vec{y}}^2-\norm{\vec{x}-\vec{y}}^2 \right),\quad\forall\vec{x},\vec{y}\in H
        \end{equation*}
        veamos que $(\vec{x},\vec{y})\mapsto\pint{\vec{x}}{\vec{y}}$ verifica las cuatro condiciones para ser producto escalar real sobre $H$:

        Notemos antes que si $\vec{x},\vec{y}\in H$:
        \begin{equation*}
            \begin{split}
                2\pint{\vec{x}}{\vec{y}}&=\frac{1}{4}\left(2\norm{\vec{x}+\vec{y}}^2-2\norm{\vec{x}-\vec{y}}^2 \right) \\
                =\frac{1}{4}\left(2\norm{\vec{x}+\vec{y}}^2-2\norm{\vec{x}-\vec{y}}^2 \right) \\
            \end{split}
        \end{equation*}
        %TODO
        \begin{enumerate}
            \item Sea $\vec{y}\in H$. Hay que ver que la función $\vec{x}\mapsto\pint{\vec{x}}{\vec{y}}$ es lineal. En efecto, sean $\vec{x},\vec{x_1},\vec{x_2}\in H$ y $\alpha\in\mathbb{R}$, tenemos que:
            \begin{equation*}
                \begin{split}
                    \pint{\vec{x_1}+\vec{x_2}}{\vec{y}}&=\frac{1}{4}\left(\norm{\vec{x_1}+\vec{x_2}+\vec{y}}^2-\norm{\vec{x_1}+\vec{x_2}-\vec{y}}^2 \right) \\
                \end{split}
            \end{equation*}
        \end{enumerate}
    \end{sol}

    \begin{excer}
        Para todo $s\in\mathbb{R}$ sea $\cf{u_s}{\mathbb{R}}{\mathbb{C}}$ la función definida por:
        \begin{equation*}
            u_s(x)=e^{isx},\quad\forall x\in\mathbb{R}.
        \end{equation*}
        Sea $X$ el espacio vectorial complejo compuesto de todas las combinaciones lineales finitas de estas funciones $u_s$, $\forall f,g\in X$ se define:
        \begin{equation*}
            \pint{f}{g}=\lim_{R\rightarrow\infty}\frac{1}{2R}\int_{-R}^Rf\conj{g}.
        \end{equation*}
        \textbf{Pruebe} que esta definición tiene sentido y que la aplicación $(f,g)\mapsto\pint{f}{g}$ es un producto escalar que hace de $X$ un espacio prehilbertiano.

        Sea $H$ el espacio prehilbertiano, completación del espacio prehilbertiano $X$ (ver problema 1.5). \textbf{Muestre} que $H$ es un espacio hilbertiano no separable y que la familia $\left(u_s\right)_{s\in\mathbb{R}}$ es un sistema ortonormal maximal en $H$.
    \end{excer}

    \begin{proof}
        Para la primera parte, notemos que por linealidad de la integral, basta probar que este mapeo tiene sentido para las funciones $u_s$. Sean $r,s\in\mathbb{R}$, entonces:
        \begin{equation*}
            \begin{split}
                \pint{u_s}{u_r}&=\lim_{R\rightarrow\infty}\frac{1}{2R}\int_{-R}^{R}u_s(x)\conj{u_r(x)}dx \\
                &=\lim_{R\rightarrow\infty}\frac{1}{2R}\int_{-R}^{R}e^{isx}\conj{e^{irx}}dx \\
                &=\lim_{R\rightarrow\infty}\frac{1}{2R}\int_{-R}^{R}e^{isx}e^{-irx}dx \\
                &=\lim_{R\rightarrow\infty}\frac{1}{2R}\int_{-R}^{R}e^{i(s-r)x}dx \\
            \end{split}
        \end{equation*}
        si $s=r$, entonces
        \begin{equation*}
            \begin{split}
                \pint{u_s}{u_r}&=\lim_{R\rightarrow\infty}\frac{1}{2R}\int_{-R}^{R}dx \\
                &=\lim_{R\rightarrow\infty}\frac{1}{2R}2R\\
                &=\lim_{R\rightarrow\infty}1\\
                &=1\\
            \end{split}
        \end{equation*}
        si $s\neq r$, se tiene que:
        \begin{equation*}
            \begin{split}
                \pint{u_s}{u_r}&=\lim_{R\rightarrow\infty}\frac{1}{2R}\frac{e^{i(s-r)x}}{i(s-r)}\Big|_{x=-R}^{x=R } \\
                &=\lim_{R\rightarrow\infty}\frac{1}{2R}\frac{e^{i(s-r)R}-e^{-i(s-r)R}}{i(s-r)}\\
                &=\lim_{R\rightarrow\infty}\frac{1}{2R}\frac{\cos((s-r)R)+i\sen((s-r)R)-\cos(-(s-r)R)-i\sin(-(s-r)R)}{i(s-r)}\\
                &=\lim_{R\rightarrow\infty}\frac{1}{2R}\frac{\cos((s-r)R)+i\sen((s-r)R)-\cos((s-r)R)+i\sin((s-r)R)}{i(s-r)}\\
                &=\lim_{R\rightarrow\infty}\frac{1}{2R}\frac{2i\sin((s-r)R)}{i(s-r)}\\
                &=\lim_{R\rightarrow\infty}\frac{\sin((s-r)R)}{(s-r)R}\\
                &=0\\
            \end{split}
        \end{equation*}
        (usar teorema del emparedado para verificar que ese es el límite). Por tanto, $\pint{\cdot}{\cdot}$ está bien definida.

        Veamos ahora que esa aplicación es un producto escalar sobre $X$. En efecto, se deben verificar cuatro condiciones:
        \begin{enumerate}
            \item Sea $g\in X$ fijo, hay que ver que la aplicación $f\mapsto \pint{f}{g}$ es lineal. En efecto, sean $f,f_1,f_2\in X$ y $\alpha\in \mathbb{C}$, se tiene que:
            \begin{equation*}
                \begin{split}
                    \pint{f_1+f_2}{g}&=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}(f_1+f_2)\conj{g}\\
                    &=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}f_1\conj{g}+f_2\conj{g}\\
                    &=\lim_{R\rightarrow\infty }\left(\frac{1}{2R}\int_{-R}^{R}f_1\conj{g}+\frac{1}{2R}\int_{-R}^{R}f_2\conj{g}\right)\\
                    &=\lim_{R\rightarrow\infty}\frac{1}{2R}\int_{-R}^{R}f_1\conj{g}+\lim_{R\rightarrow\infty}\frac{1}{2R}\int_{-R}^{R}f_2\conj{g}\\
                    &=\pint{f_1}{g}+\pint{f_2}{g}\\
                \end{split}
            \end{equation*}
            pues, ambos límites existen. Además:
            \begin{equation*}
                \begin{split}
                    \pint{\alpha f}{g}&=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}\alpha f\conj{g}\\
                    &=\lim_{R\rightarrow\infty }\frac{\alpha}{2R}\int_{-R}^{R} f\conj{g}\\
                    &=\alpha\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R} f\conj{g}\\
                    &=\alpha\pint{f}{g}\\
                \end{split}
            \end{equation*}
            por tanto, esa aplicación es lineal.
            \item $\pint{g}{f}=\conj{\pint{f}{g}}$, para todo $f,g\in X$. En efecto, sean $f,g\in X$, se tiene que:
            \begin{equation*}
                \begin{split}
                    \conj{\pint{f}{g}}&=\conj{\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}f\conj{g}} \\
                    &=\lim_{R\rightarrow\infty }\conj{\frac{1}{2R}\int_{-R}^{R}f\conj{g}} \\
                    &=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}\conj{f\conj{g}} \\
                    &=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}\conj{f}g \\
                    &=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}g\conj{f} \\
                    &=\pint{g}{f}\\
                \end{split}
            \end{equation*}
            con lo que se tiene el resultado.
            \item $\pint{f}{f}\geq0$, para todo $f\in X$. En efecto, sea $f\in X$, se tiene que:
            \begin{equation*}
                \begin{split}
                    \pint{f}{f}&=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}f\conj{f}\\
                    &=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}\abs{f}^2 \\
                    &\geq0\\
                \end{split}
            \end{equation*}
            pues $\abs{f}^2\geq0$.
            \item $\pint{f}{f}=0$ si y sólo si $f=0$. La vuelta es inmediata, suponga que $\pint{f}{f}=0$, digamos que $f=\lambda_1 u_{s_1}+...+\lambda_nu_{s_n}$ donde $\lambda_i\in\mathbb{C}$ y $s_i\in\mathbb{R}$ para todo $i\in\left[|1,n|\right]$, se tiene entonces que:
            \begin{equation*}
                \begin{split}
                    \pint{f}{f}&= \lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}f\conj{f}\\
                    &= \lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}(\lambda_1 u_{s_1}+...+\lambda_nu_{s_n})\conj{\lambda_1 u_{s_1}+...+\lambda_nu_{s_n}}\\
                    &= \lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}(\lambda_1 u_{s_1}+...+\lambda_nu_{s_n})(\conj{\lambda_1} u_{-s_1}+...+\conj{\lambda_n}u_{-s_n})\\
                    &= \lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}(\lambda_1 u_{s_1}+...+\lambda_nu_{s_n})(\conj{\lambda_1} u_{-s_1}+...+\conj{\lambda_n}u_{-s_n})\\
                    &= \lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}\sum_{i,j=1 }^n \lambda_i u_{s_i }\conj{\lambda_j}u_{ -s_j} \\
                    &= \lim_{R\rightarrow\infty }\sum_{i,j=1 }^n\lambda_i\conj{\lambda_j}\frac{1}{2R}\int_{-R}^{R}  u_{s_i }u_{ -s_j} \\
                    &= \sum_{i,j=1 }^n\lambda_i\conj{\lambda_j}\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R} u_{s_i }u_{ -s_j} \\
                \end{split}
            \end{equation*}
            sean $i,j\in\left[|1,n|\right]$, se tienen dos casos, $i\neq j$ o $i=j$. Por lo analizado anteriormente si $i\neq j$ se sigue que
            \begin{equation*}
                \lambda_i\conj{\lambda_j}\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}u_{s_i }u_{ -s_j}=0
            \end{equation*}
            y, si $i=j$:
            \begin{equation*}
                \lambda_i\conj{\lambda_i}\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}u_{s_i }u_{ -s_i}=\abs{\lambda_i}^2
            \end{equation*}
            Por tanto:
            \begin{equation*}
                \pint{f}{f}=\sum_{ i=1}^n\abs{\lambda_i}^2=0
            \end{equation*}
            esto ocurre si y sólo si $\lambda_i=0$ para todo $i\in\left[|1,n|\right]$, es decir si $f=0$.
        \end{enumerate}
        por los cuatro incisos anteriores, se sigue que $\pint{\cdot}{\cdot}$ es un producto escalar sobre $X$ que lo hace un espacio prehilbertiano.

        Para la segunda parte, al inicio ya se vió que $\left(u_s \right)_{s\in\mathbb{R}}$ es un sistema O.N. de vectores en $X$. Para ver que no es separable, considere la familia de bolas:
        \begin{equation*}
            \mathcal{B}=\left\{B_s\Big|s\in\mathbb{R}^+ \right\}
        \end{equation*}
        donde $B_s=\left\{f\in H\Big|\norm{f-u_s}<\frac{1}{2} \right\}$, para todo $s\in\mathbb{R}^+$. Afirmamos que estas bolas son disjuntas a pares, en efecto, sean $s,r\in\mathbb{R}^+$ con $s\neq r$. Si $f\in B_s$ y $f\in B_r$, entonces:
        \begin{equation*}
            \norm{f-u_s}<\frac{1}{2} \quad\textup{y}\quad\norm{f-u_r}<\frac{1}{2}
            \Rightarrow  \norm{u_s-u_r}\leq\norm{u_s-f}+\norm{f-u_r}<1
        \end{equation*}
        pero,
        \begin{equation*}
            \begin{split}
                \pint{u_s-u_r}{u_s-u_r}&=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}(u_s-u_r)\conj{(u_s-u_r)}\\
                &=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}(u_s-u_r)(u_{-s}-u_{-r})\\
                &=\lim_{R\rightarrow\infty }\frac{1}{2R}\int_{-R}^{R}\left(u_s u_{-s} - u_s u_{-r} - u_r u_{-s} + u_{-s}u_{-r}\right) \\
                &=1+0+0+1\\
                &=2\\
                \Rightarrow \norm{u_s-u_r}&=\sqrt{2}\\
            \end{split}
        \end{equation*}
        el cual no es menor que 1, por tanto tal $f$ no puede existir, así $B_s\cap B_r=\emptyset$. Con lo que $\mathcal{B}$ es una familia no numerable de bolas abiertas disjuntas a pares, por una proposición de análisis I debe suceder que $H$ no es separable.

        Veamos que $\left(u_s \right)_{s\in\mathbb{R}}$ es O.N. maximal. Ya se vió que es O.N., veamos que es maximal. Por el teorema de Parserval al ser $H$ hilbertiano, basta con ver que:
        \begin{equation*}
            \overline{\mathcal{L}\left(\left(u_s \right)_{s\in\mathbb{R}}\right)}=H
        \end{equation*}
        esto es inmediato, pues $\mathcal{L}\left(\left(u_s \right)_{s\in\mathbb{R}}\right)=X$ ya que el conjunto de la izquierda es el generado por todas las combinaciones lineales finitas de elementos de $\left(u_s \right)_{s\in\mathbb{R}}$, y el conjunto $i(X)$ es denso en $H$ (usando la notación del ejercicio 1.1.5), luego se tiene que:
        \begin{equation*}
            \overline{i(X)}=H\Rightarrow\overline{\mathcal{L}\left(\left(u_s \right)_{s\in\mathbb{R}}\right)}=H
        \end{equation*}
        pues, $i(u_s)=u_s$. Por tanto, $\left(u_s \right)_{s\in\mathbb{R}}$ es O.N. maximal.
    \end{proof}

    \begin{excer}
        Sea $H$ un espacio hilbertiano de dimensión infinita. \textbf{Demuestre} que existe una aplicación continua inyectiva $\gamma$ de $[0,1]$ en $H$ (un \textbf{camino simple} en $H$) tal que si $0\leq a\leq b\leq c\leq d\leq 1$, los vectores $\gamma(b)-\gamma(a)$ y $\gamma(d)-\gamma(c)$ son ortogonales.

        \textit{Indicación.} Tome $H=L_2([0,1],\mathbb{K})$ y considere funciones características de ciertos subconjuntos de $[0,1]$.
    \end{excer}

    \begin{proof}
        Consideremos primero $H'=L_2([0,1],\mathbb{K})$. Sea $\cf{\gamma'}{[0,1]}{H'}$ dada por:
        \begin{equation*}
            \gamma'(x)=\chi_{[0,x]},\quad\forall x\in[0,1]
        \end{equation*}
        es claro que $\gamma'$ es inyectiva. Además, si $0\leq a\leq b\leq c\leq d\leq 1$, se tiene que:
        \begin{equation*}
            \begin{split}
                \gamma'(b)-\gamma'(a)&=\chi_{[0,b]}-\chi_{[0,a]}\\
                &=\chi_{]a,b]}\\
            \end{split}
        \end{equation*}
        y, $\gamma'(d)-\gamma'(c)=\chi_{]c,d]}$. Por tanto:
        \begin{equation*}
            \begin{split}
                \pint{\gamma'(b)-\gamma'(a)}{\gamma'(d)-\gamma'(c)}&=\int_{0}^{1}\chi_{]a,b]}\conj{\chi_{]c,d]}}\\
                &=\int_{0}^{1}\chi_{]a,b]}\chi_{]c,d]}\\
                &=0\\
            \end{split}
        \end{equation*}
        por tanto, $\gamma'(b)-\gamma'(a)$ y $\gamma'(d)-\gamma'(c)$ son ortogonales.

        Ahora, sea $H$ un espacio hilbertiano arbitrario de dimensión infinita. Entonces, existe $\Omega$ tal que $H\equiv l_2(\Omega,\mathbb{K})$, sea $\cf{F}{l_2(\Omega,\mathbb{K})}{H}$ la isometría suprayectiva entre estos dos espacios (en particular, es biyectiva por lo que es un isomorfismo de espacios hilbertianos).
        
        Notemos que como $H'=L_2([0,1],\mathbb{K})$ siendo $[0,1]\subseteq\mathbb{R}$ medible, entonces $H'$ es separable, es decir que $H'\equiv l_2(\mathbb{N},\mathbb{K})$ (son isométricos), sea $\cf{G}{H'}{l_2(\mathbb{N},\mathbb{K})}$ la isometría suprayectiva entre estos dos espacios (nuevamente, se tiene que en particular es biyectiva, por lo que es un isomorfismo de espacios hilbertianos).

        Como $\Omega$ es infinito (ya que el espacio $H$ es de dimensión infinita, luego la familia O.N. maximal debe ser infinita, es decir a lo sumo numerable) existe una subfamilia $\left\{\omega_n \right\}_{n=1}^\infty$ numerable de elementos de $\Omega$, defina así $\cf{j}{\mathbb{N}}{\Omega}$ tal que $n\mapsto \omega_n$. Definimos con esto la función $\cf{J}{l_2(\mathbb{N},\mathbb{K})}{l_2(\Omega,\mathbb{K})}$ tal que:
        \begin{equation*}
            \left\{x(n) \right\}_{n\in\mathbb{N}}\mapsto\left\{y(\omega) \right\}_{\omega\in\Omega}
        \end{equation*}
        donde
        \begin{equation*}
            y(\omega)=\left\{
                \begin{array}{lr}
                    x(n) & \textup{ si }\omega=\omega_n\textup{ para algún }n\in\mathbb{N}\\
                    0 & \textup{ en otro caso} \\
                \end{array}
            \right.
            ,\quad\forall\omega\in\Omega
        \end{equation*}
        Esta función está bien definida y es claro que es inyectiva (más aún, es una isometría). Por lo cual la función
        \begin{equation*}
            \gamma=F\circ J\circ G\circ \gamma':[0,1]\rightarrow H
        \end{equation*}
        es inyectiva, y cumple que (al ser $F$, $J$ y $G$ isometrías):
        \begin{equation*}
            \begin{split}
                \pint{\gamma(b)-\gamma(a)}{\gamma(d)-\gamma(d)}&=\pint{\gamma'(b)-\gamma'(a)}{\gamma'(d)-\gamma'(d)}\\
                &=0\\
            \end{split}
        \end{equation*}
        para todo $0\leq a\leq b\leq c\leq d\leq 1$ (pues, $F\circ J\circ G$ es una isometría por lo cual preserva el producto escalar). Así, $\gamma$ es la función buscada.
    \end{proof}

    \begin{excer}
        Sea $\left\{\vec{x_\nu} \right\}_{\nu=1}^\infty$ una sucesión de elementos de un espacio hilbertiano $H$. La sucesión $\left\{\vec{x_\nu} \right\}_{\nu=1}^\infty$ se llama \textbf{martingala} (en el sentido amplio) si, $\forall\nu\in\mathbb{N}$, $\vec{x_\nu}$ es el vector de $\mathcal{L}(\vec{x_1},...,\vec{x_\nu})$ menos alejado de $\vec{x_{\nu+1}}$ (básicamente $\vec{x_v}$ es la proyección ortogonal de $\vec{x_{v+1}}$ sobre el subespacio generado por los vectores anteriores).
        \begin{enumerate}
            \item Sea $\left\{\vec{x_\nu} \right\}_{\nu=1}^\infty$ una martingala. Se definen:
            \begin{equation*}
                \vec{y_1}=\vec{x_1}\quad\textup{e}\quad\vec{y_\nu}=\vec{x_\nu}-\vec{x_{\nu-1}},\quad\forall\nu\geq2.
            \end{equation*}
            \textbf{Muestre} que los vectores $\vec{y_\nu}$ son ortogonales a pares y que $\left\{\norm{\vec{x_\nu}} \right\}_{\nu=1}^\infty$ es una sucesión creciente de números no negativos.
            \item Sea $\left\{\vec{y_\nu} \right\}_{\nu=1}^\infty$ una sucesión de vectores en $H$ ortogonales a pares. Se define
            \begin{equation*}
                \vec{x_\nu}=\sum_{k=1}^\nu\vec{y_k},\quad\forall\nu\in\mathbb{N}.
            \end{equation*}
            \textbf{Pruebe} que $\left\{\vec{y_\nu} \right\}_{\nu=1}^\infty$ es una martingala.
        \end{enumerate}
    \end{excer}

    \begin{proof}
        De (i): Procederemos por inducción sobre $\nu$. Para $\nu=2$ el resultado se cumple, pues notemos que $\vec{x_1}$ es el vector de $\mathcal{L}(\vec{x_1})$ menos alejado de $\vec{x_{2}}$, es decir que $\vec{x_1}$ es la proyección ortogonal de $\vec{x_2}$, luego $\vec{x_1}\perp \vec{x_2}-\vec{x_1}$, es decir que $\vec{y_1}\perp\vec{y_2}$. Además, por este hecho (y, usando Pitágoras) se sigue que:
        \begin{equation*}
            \begin{split}
                \norm{\vec{x_1}}^2+\norm{\vec{x_2}-\vec{x_1}}^2&=\norm{\vec{x_2}}^2\\
                \Rightarrow \norm{\vec{x_1}}\leq\norm{\vec{x_2}}
            \end{split}
        \end{equation*}

        Suponga que el resultado se cumple para algún $\nu$. Veamos que se cumple para $\nu+1$. En efecto, ya se tiene que $\vec{y_1},...,\vec{y_\nu}$ son ortogonales a pares. Como $\vec{x_\nu}$ es el vector menos alejado de $\vec{x_{\nu+1}}$, es decir que es su proyección ortogonal sobre $\mathcal{L}(\vec{x_1},...,\vec{x_\nu})$, por ende, $\vec{y_{\nu+1}}=\vec{x_{\nu+1}}-\vec{x_\nu}\perp \mathcal{L}(\vec{x_1},...,\vec{x_\nu})$, en particular, $\vec{y_1},...,\vec{y_\nu}\in\mathcal{L}(\vec{x_1},...,\vec{x_\nu})$, luego los vectores $\vec{y_1},...,\vec{y_{\nu+1}}$ son ortogonales a pares.
        
        Además, como en el paso inductivo, se tiene que:
        \begin{equation*}
            \begin{split}
                \norm{\vec{x_{\nu}}}^2+\norm{\vec{x_{\nu+1}}-\vec{x_{\nu}}}^2&=\norm{\vec{x_{\nu+1}}}^2\\
                    \Rightarrow \norm{\vec{x_{\nu}}}\leq\norm{\vec{x_{\nu+1}}}
            \end{split}
        \end{equation*}
        
        Aplicando inducción, el resultado se tiene para todo $\nu\in\mathbb{N}$.

        De (ii): Sea $\nu\in\mathbb{N}$. Hay que probar que $\vec{x_\nu}$ es la proyección de $\vec{x_{\nu+1}}$ sobre $\mathcal{L}(\vec{x_1},...,\vec{x_\nu})$. En efecto, veamos que
        \begin{equation*}
            \begin{split}
                \vec{x_{\nu+1}}&=\sum_{ k=1}^{\nu+1}\vec{y_{k}}\\
                &=\sum_{ k=1}^\nu\vec{y_{k}}+\vec{y_{\nu+1}} \\
                &=\vec{x_\nu}+\vec{y_{\nu+1}} \\
            \end{split}
        \end{equation*}
        siendo $\vec{y_\nu+1}\perp M=\mathcal{L}(\vec{x_1},...,\vec{x_\nu})$, por tanto al ser $M$ distinguido (por ser de dimensión finita), se sigue que esta descomposición es única, es decir que el vector $\vec{x_\nu}$ es la proyección ortogonal de $\vec{x_{\nu+1}}$ sobre $M$, que es lo que se quería demostrar.

    \end{proof}

    \begin{excer}
        Sea $\cf{f}{\mathbb{R}^n}{\mathbb{K}}$ una función medible, integrable en todo subconjunto de $\mathbb{R}^n$ de medida finita. Si
        \begin{equation*}
            \int f=0,\quad\forall\textup{ rectángulo acotado }P\subseteq\mathbb{R}^n,
        \end{equation*}
        \textbf{demuestre} que $f=0$ c.t.p. en $\mathbb{R}^n$.
        
        \textit{Indicación.} Redúzcase a un corolario del lema de los promedios.
    \end{excer}

    \begin{proof}
        Sea $P\subseteq\mathbb{R}^n$ rectángulo acotado con medida positiva, es decir, $m(P)>0$. Probaremos que:
        \begin{equation*}
            \int_P\abs{f}=0
        \end{equation*}
        en efecto, como $f$ es integrable en $\mathbb{R}^n$ en todo subconjunto con medida finita, entonces $\abs{f}$ también lo es. Además:
        \begin{equation*}
            \int_P f=\int_{P}f^++\int_{P}f^-=0
        \end{equation*}
        si $\int_{P}f^+=\int_{P}f^-=0$, ya se tiene el resultado. Suponga que alguna de las dos integrales es positiva, digamos
        \begin{equation*}
            \int_{P}f^+>0
        \end{equation*}
        entonces, el conjunto:
        \begin{equation*}
            A=\left\{x\in P\Big|f(x)>0 \right\}
        \end{equation*}
        tiene medida positiva. Se tiene que:
        \begin{equation*}
            A=\bigcup_{ \nu=1}^\infty A_\nu
        \end{equation*}
        donde $A_\nu=\left\{x\in P\Big|f(x)\geq\frac{1}{\nu}\right\}$. Por el teorema de continuidad, existe $\nu_0\in\mathbb{N}$ tal que $m(A_{\nu_0})>0$. Ahora, como todas las normas sobre $\mathbb{R}^n$ son equivalentes, existe $r>0$ tal que:
        \begin{equation*}
            B_\infty(x_0,r)=\left\{x\in\mathbb{R}^n\Big|\norm{x_0-x}_{\infty}<r \right\}\subseteq A_{\nu_0}
        \end{equation*}
        (donde $\norm{\cdot}_{\infty}$ es la norma infinito sobre $\mathbb{R}^n$ y $x_0\in A_{\nu_0}$). Este conjunto es un rectángulo acotado en el que se cumple que:
        \begin{equation*}
            \int_{A_{\nu_0}} f^+=\int_{A_{\nu_0}} f\geq \int_{B_\infty(x_0,r)}f\geq \frac{1}{\nu_0}\int_{B_\infty(x_0,r)}dx=\frac{1}{\nu_0}m(B_\infty(x_0,r))>0
        \end{equation*}
        lo cual contradice la hipótesis. Por tanto,
        \begin{equation*}
            \int_{P}f^+=\int_{P}f^-=0
        \end{equation*}
        es decir, que
        \begin{equation*}
            \int_{P}\abs{f}=0
        \end{equation*}
        siendo $P$ un rectángulo acotado con medida positiva. Notemos que si la medida de este rectangulo fuese $0$, el resultado seguiría siendo válido. Por ende:
        \begin{equation*}
            \int \abs{f}=0,\quad\forall\textup{ rectángulo acotado }P\subseteq\mathbb{R}^n,
        \end{equation*}
        Sea $A\subseteq\mathbb{R}^n$ conjunto medible tal que $0<m(A)<\infty$. Por un resultado de Análisis II, existe $\left\{P_\nu \right\}_{\nu=1}^\infty$ sucesión de rectángulos acotados disjuntos con medida finita tales que:
        \begin{equation*}
            A\subseteq \bigcup_{\nu=1}^\infty P_\nu=P
        \end{equation*}
        donde $m(P)<\infty$. Se tiene entonces que, al ser $\abs{f}\geq 0$:
        \begin{equation*}
            0\leq\frac{1}{m(A)} \int_{A}\abs{f}\leq \frac{1}{m(P)}\int_{P}\abs{f}
        \end{equation*}
        pero, por un corolario de Beppo-Levi, se tiene que:
        \begin{equation*}
            \int_{P}\abs{f}=\int_{\bigcup_{\nu=1}^\infty P_\nu}\abs{f}=\sum_{\nu=1}^\infty\int_{P_\nu}\abs{f}=0
        \end{equation*}
        por lo probado anteriormente, luego:
        \begin{equation*}
            \frac{1}{m(A)}\int_{A}\abs{f}=0\in F
        \end{equation*}
        donde $F=\left\{ 0\right\}$ es cerrado. Por el lema de los promedios se sigue que $f=0$ c.t.p. en $\mathbb{R}^n$.
    \end{proof}

    \begin{excer}[\textbf{Funciones de Hermite}]
        Por inducción se ve inmediatamente que
        \begin{equation*}
            D^ne^{-x^2}=(-1)^nH_n(x)e^{-x^2}, \quad n=0,1,2,...
        \end{equation*}
        donde $(-1)H_n$ es un polinomio de grado $n$. Estos polinomios $(-1)H_n$ se llaman \textbf{polinomios de Hermite}. Se definen las \textbf{funciones de Hermite} $\varphi_n$ por:
        \begin{equation*}
            \varphi_n(x)=H_n(x)e^{-\frac{x^2}{2}},\quad n=0,1,2,...
        \end{equation*}
        equivalentemente,
        \begin{equation*}
            \varphi_n(x)=(-1)^ne^{\frac{x^2}{2}}D^ne^{-x^2},\quad n=0,1,2,...
        \end{equation*}
        \begin{enumerate}
            \item \textbf{Demuestre} que las funciones de Hermite satisfacen la relación:
            \begin{equation*}
                \varphi_n''(x)=(x^2-2n-1)\varphi_n(x),\quad\forall x\in\mathbb{R}.
            \end{equation*}
            \textit{Indicación.} Exprese a $\varphi_n''(x)$ mediante $D^ne^{-x^2}$, $D^{n+1}e^{-x^2}$ y $D^{n+2}e^{-x^2}$ y calcule $D^{n+2}e^{-x^2}=D^{n+1}(-2xe^{-x^2})$ por la fórmula de Leibniz para la derivada $n+1$-enésima de un producto de factores.

            \item \textbf{Muestre} que las funciones de Hermite consistituyen un sistema ortogonal en el espacio hilbertiano $L_2(\mathbb{R},\mathbb{K})$
            
            \textit{Indicación.} Del inciso (i) se sigue que $\varphi_n''\varphi_m-\varphi_m''\varphi_n=2(m-n)\varphi_n\varphi_m$.

            \item \textbf{Pruebe} la relación
            \begin{equation*}
                H_n'(x)=2nH_{n-1}(x).
            \end{equation*}
            \textit{Indicación.} Exprese $H_n'(x)$ mediante $D^ne^{-x^2}$ y $D^{n+1}e^{-x^2}$. Calcule $D^{n+1}e^{-x^2}=D^n(-2xe^{-x^2})$ por la fórmula de Leibniz.

            \item \textbf{Demuestre} que
            \begin{equation*}
                \int_{-\infty}^\infty\varphi_n^2(x)dx=2n\int_{-\infty}^\infty\varphi_{n-1}^2(x)dx
            \end{equation*}
            y deduzca que
            \begin{equation*}
                \int_{-\infty}^\infty\varphi_n^2(x)dx=\pi^{1/2}2^nn!.
            \end{equation*}
            Luego el sistema de funciones:
            \begin{equation*}
                \Psi_n=\frac{1}{\pi^{1/2}2^nn!}\varphi_n
            \end{equation*}
            es un sistema ortonormal en $L_2(\mathbb{R},\mathbb{K})$ (En un ejercicio posterior se probará que dicho sistema ortonormal es, de hecho, maximal).

            \textit{Indicación.} Integre por partes
            \begin{equation*}
                \int_{-\infty}^\infty\varphi_n^2(x)dx=(-1)^n\int_{-\infty}^\infty H_n(x)D^ne^{-x^2}dx
            \end{equation*}
            y use (iii).
        \end{enumerate}
    \end{excer}

    \begin{proof}
        De (i): Sea $n\in\mathbb{N}^*$. Veamos que:
        \begin{equation*}
            \begin{split}
                \varphi_n'(x)
                &=\frac{d}{dx}\left((-1)^ne^{\frac{x^2}{2}}D^ne^{-x^2} \right)\\
                &=(-1)^n\frac{d}{dx}\left(e^{\frac{x^2}{2}}D^n e^{-x^2}\right)\\
                &=(-1)^n\left[\frac{d}{dx}\left(e^{\frac{x^2}{2}}\right)D^n e^{-x^2}+e^{\frac{x^2}{2}}\frac{d}{dx}\left(D^n e^{-x^2}\right)\right] \\
                &=(-1)^n\left[xe^{\frac{x^2}{2}}D^n e^{-x^2}+e^{\frac{x^2}{2}}D^{n+1 } e^{-x^2}\right] \\
            \end{split}
        \end{equation*}
        por tanto:
        \begin{equation*}
            \begin{split}
                \varphi_n''(x)
                &=(-1)^n\frac{d}{dx}\left[xe^{\frac{x^2}{2}}D^n e^{-x^2}+e^{\frac{x^2}{2}}D^{n+1 } e^{-x^2}\right] \\
                &=(-1)^n\left[\frac{d}{dx}(xe^{\frac{x^2}{2}})D^n e^{-x^2}+xe^{\frac{x^2}{2}}D^{n+1}e^{-x^2}+\frac{d}{dx}(e^{\frac{x^2}{2}})D^{n+1 } e^{-x^2}+e^{\frac{x^2}{2}}D^{n+2 } e^{-x^2}\right] \\
                &=(-1)^n\left[(x^2+1)e^{\frac{x^2}{2}}D^n e^{-x^2}+xe^{\frac{x^2}{2}}D^{n+1}e^{-x^2}+xe^{\frac{x^2}{2}}D^{n+1 } e^{-x^2}+e^{\frac{x^2}{2}}D^{n+2 } e^{-x^2}\right] \\
                &=(-1)^n\left[(x^2+1)e^{\frac{x^2}{2}}D^n e^{-x^2}+2xe^{\frac{x^2}{2}}D^{n+1}e^{-x^2}+e^{\frac{x^2}{2}}D^{n+2 } e^{-x^2}\right] \\
                &=(-1)^ne^{\frac{x^2}{2}}\left[(x^2+1)D^n e^{-x^2}+2xD^{n+1}e^{-x^2}+D^{n+2 } e^{-x^2}\right] \\
            \end{split}
        \end{equation*}
        por la regla general de Leibniz para la derivada de un producto, se tiene que:
        \begin{equation*}
            \begin{split}
                D^{n+2}e^{-x^2}&=D^{n+1}(D(e^{-x^2})) \\
                &=D^{n+1}(-2xe^{-x^2}) \\
                &=\sum_{ k=0}^{n+1} \binom{n +1}{k} (D^{n+1-k}e^{-x^2})(-2D^k x)\\
                &=-2xD^{n+1}e^{-x^2}-2(n+1)D^{n}e^{-x^2}
            \end{split}
        \end{equation*}
        luego,
        \begin{equation*}
            \begin{split}
                \varphi_n''(x)&=(-1)^ne^{\frac{x^2}{2}}\left[(x^2+1)D^ne^{-x^2}+2xD^{n+1}e^{-x^2}-2xD^{n+1}e^{-x^2}-2(n+1)D^ne^{-x^2} \right]\\
                &=(-1)^ne^{\frac{x^2}{2}}\left[(x^2+1)D^ne^{-x^2}-2(n+1)D^ne^{-x^2} \right]\\
                &=(-1)^ne^{\frac{x^2}{2}}D^ne^{-x^2} \left[x^2+1-2n-2 \right]\\
                &=(x^2-2n-1)(-1)^ne^{\frac{x^2}{2}}D^ne^{-x^2}\\
                &=(x^2-2n-1)\varphi_n(x),\quad\forall x\in\mathbb{R}\\ 
            \end{split}
        \end{equation*}
        como se quería probar.

        De (ii): Observemos que si $m,n\in\mathbb{N}^*$:
        \begin{equation*}
            \begin{split}
                \varphi_n''(x)\varphi_m(x)-\varphi_m''(x)\varphi_n(x)&=(x^2-2n-1)\varphi_n(x)\varphi_m(x)-(x^2-2m-1)\varphi_n(x)\varphi_m(x) \\
                &=(x^2-2n-1-x^2+2m+1)\varphi_n(x)\varphi_m(x) \\ 
                &=2(m-n)\varphi_n(x)\varphi_m(x),\quad\forall x\in\mathbb{R} \\ 
            \end{split}
        \end{equation*}
        Por tanto, si $m\neq n$, se tiene que:
        \begin{equation*}
            \begin{split}
                \pint{\varphi_n}{\varphi_m}&=\int_{-\infty}^\infty \varphi_n(x)\conj{\varphi_m(x)}dx\\
                &=\int_{-\infty}^\infty\varphi_n(x)\varphi_m(x)dx\\
                &=\frac{1}{2(m-n)} \int_{-\infty}^\infty\left[\varphi_n''(x)\varphi_m(x)-\varphi_m''(x)\varphi_n(x)\right]dx\\
                &=\frac{1}{2(m-n)} \lim_{R\rightarrow\infty} \int_{-R}^R\left[\varphi_n''(x)\varphi_m(x)-\varphi_m''(x)\varphi_n(x)\right]dx\\
            \end{split}
        \end{equation*}
        donde:
        \begin{equation*}
            \begin{split}
                \int_{-R}^R&\left[\varphi_n''(x)\varphi_m(x)-\varphi_m''(x) \varphi_n(x)\right]dx\\
                &=\left[\varphi_n'(x)\varphi_m(x)-\varphi_m'(x)\varphi_n(x)\right]\Big|_{-R}^{R}-\int_{-R}^R(\varphi_n'(x)\varphi_m'(x)-\varphi_m'(x)\varphi_n'(x))dx\\
                &=\left[\varphi_n'(R)\varphi_m(R)-\varphi_m'(R)\varphi_n(R)-\varphi_n'(-R)\varphi_m(-R)+\varphi_m'(-R)\varphi_n(-R)\right] \\
            \end{split}
        \end{equation*}
        además,
        \begin{equation*}
            \begin{split}
                \varphi_n'(x)&=\frac{d}{dx}\left(H_n(x)e^{-\frac{x^2}{2}} \right)\\
                &=H_n'(x)e^{-\frac{x^2}{2}}-2xH_n(x)e^{-\frac{x^2}{2}}\\
                &=e^{-\frac{x^2}{2}}\left[H_n'(x)-2xH_n(x)\right] \\
            \end{split}
        \end{equation*}
        Por lo cual:
        \begin{equation*}
            \varphi_n'(x)\varphi_m(x)=e^{-x^2}\left[H_n'(x)-2xH_n(x)\right]H_m(x)
        \end{equation*}
        luego, al tomar el límite cuando $R\rightarrow\infty$, se tiene que el término $e^{-x^2}$ domina a lo que sea que valga el producto de polinomios de la derecha con lo cual, se sigue que:
        \begin{equation*}
            \lim_{R\rightarrow\infty}\left[\varphi_n'(R)\varphi_m(R)-\varphi_m'(R)\varphi_n(R)-\varphi_n'(-R)\varphi_m(-R)+\varphi_m'(-R)\varphi_n(-R)\right]=0
        \end{equation*}
        y, por ende:
        \begin{equation*}
            \lim_{R\rightarrow\infty} \int_{-R}^R\left[\varphi_n''(x)\varphi_m(x)-\varphi_m''(x) \varphi_n(x)\right]dx=0
        \end{equation*}
        así, se tiene que:
        \begin{equation*}
            \pint{\varphi_n}{\varphi_m}=0
        \end{equation*}
        Por tanto, el sistema $\left(\varphi_n \right)_{ n\in\mathbb{N}^*}$ es un sistema ortogonal en $L_2(\mathbb{R},\mathbb{K})$.

        De (iii): Sea $n\in\mathbb{N}$, entonces:
        \begin{equation*}
            \begin{split}
                \varphi_n(x)&=\varphi_n(x)\\
                \Rightarrow H_n(x)e^{-\frac{x^2}{2}}&=(-1)^{n}e^{\frac{x^2}{2}}D^ne^{-x^2}\\
                \Rightarrow H_n(x)&=(-1)^{n}e^{x^2} D^ne^{-x^2}\\
            \end{split}
        \end{equation*}
        derivando se tiene que:
        \begin{equation*}
            \begin{split}
                H_n'(x)&=\frac{d}{dx}\left((-1)^{n}e^{x^2} D^ne^{-x^2}\right) \\
                &=(-1)^{n}\left[\frac{d}{dx}\left(e^{x^2}\right)D^ne^{-x^2}+e^{x^2}D^{n+1} e^{-x^2}\right] \\
                &=(-1)^{n}\left[2xe^{x^2}D^ne^{-x^2}+e^{x^2}D^{n+1} e^{-x^2}\right] \\
                &=(-1)^{n}e^{x^2}\left[2D^ne^{-x^2}+D^{n+1} e^{-x^2}\right] \\
            \end{split}
        \end{equation*}
        con:
        \begin{equation*}
            \begin{split}
                D^{n+1} e^{-x^2}&=D^n(-2xe^{-x^2})\\
                &=\sum_{ k=0}^n \binom{n}{k}(D^{ n-k}e^{-x^2})(D^k(-2x)) \\
                &=-2x D^ne^{-x^2}-2nD^{ n-1}e^{-x^2}\\
            \end{split}
        \end{equation*}
        por tanto:
        \begin{equation*}
            \begin{split}
                H_n'(x)&=(-1)^{n}e^{x^2}\left[2xD^ne^{-x^2}+D^{n+1} e^{-x^2}\right] \\
                &=(-1)^{n}e^{x^2}\left[2xD^ne^{-x^2}-2x D^ne^{-x^2}-2nD^{ n-1}e^{-x^2}\right] \\
                &=(-1)^{n-1}e^{x^2}\left[2nD^{ n-1}e^{-x^2}\right] \\
                &=2n\left[(-1)^{n-1}e^{x^2}D^{ n-1}e^{-x^2}\right] \\
                &=2nH_{n-1}(x)\\
            \end{split}
        \end{equation*}
        como se quería demostrar.

        De (iv): Sea $n\in\mathbb{N}$. Entonces:
        \begin{equation*}
            \begin{split}
                \int_{ -\infty}^\infty\varphi_n(x)^2dx&=\int_{ -\infty}^\infty \overbrace{\left(H_n(x)e^{-\frac{x^2}{2}}\right)}^{\varphi_n(x)} \overbrace{\left((-1)^ne^{\frac{x^2}{2}}D^ne^{-x^2} \right)}^{\varphi_n(x)} dx\\
                &=(-1)^n\int_{ -\infty}^\infty H_n(x)D^ne^{-x^2}dx\\
                &=(-1)^n\lim_{R\rightarrow\infty}\int_{ -R}^R H_n(x)D^ne^{-x^2}dx\\
                &=(-1)^n\lim_{R\rightarrow\infty}\left[H_n(x)D^{n-1} e^{-x^2}\Big|_{x=-R}^{x=R}-\int_{-R}^R H_n'(x)D^{n-1}e^{-x^2} \right]\\
                &=(-1)^n\lim_{R\rightarrow\infty}\left[(-1)^{n-1}H_n(x)H_{ n-1}(x)e^{-x^2} \Big|_{x=-R}^{x=R}-\int_{-R}^R H_n'(x)D^{n-1}e^{-x^2} \right]\\
            \end{split}
        \end{equation*}
        pero:
        \begin{equation*}
            \lim_{R\rightarrow\infty}(-1)^{n-1}H_n(x)H_{ n-1}(x)e^{-x^2} \Big|_{x=-R}^{x=R}=\lim_{R\rightarrow\infty} P(R)e^{-R^2}
        \end{equation*}
        donde $P(x)=(-1)^{n-1}\left(H_n(x)H_{ n-1}(x)-H_n(-x)H_{ n-1}(-x)\right)$. Luego el límite anterior es cero, pues la función exponencial tiende a 0 más rápido. Usando además lo obtenido en (iii), se sigue que:
        \begin{equation*}
            \begin{split}
                \int_{ -\infty}^\infty\varphi_n(x)^2dx&=(-1)^{n-1}\lim_{R\rightarrow\infty}\int_{-R}^R H_n'(x)D^{n-1}e^{-x^2}\\
                &=2n(-1)^{n-1}\lim_{R\rightarrow\infty}\int_{-R}^R H_{ n-1}(x)D^{n-1}e^{-x^2}\\
                &=2n(-1)^{n-1}\int_{-\infty}^\infty H_{ n-1}(x)D^{n-1}e^{-x^2}\\
                &=2n\int_{ -\infty}^\infty\varphi_{ n-1}(x)^2dx\\
                \Rightarrow \int_{ -\infty}^\infty\varphi_n(x)^2dx&=2n\int_{ -\infty}^\infty\varphi_{ n-1}(x)^2dx\\
            \end{split}
        \end{equation*}
        para probar la otra parte, veamos que:
        \begin{equation*}
            \begin{split}
                \int_{-\infty}^\infty\varphi_0(x)^2dx&=\int_{-\infty}^\infty\left((-1)^0e^{\frac{x^2}{2}}e^{-x^2}\right)^2 dx\\
                &=\int_{-\infty}^\infty\left(e^{-\frac{x^2}{2}}\right)^2 dx\\
                &=\int_{-\infty}^\infty e^{-x^2} dx\\
                &=\pi^{1/2}\\
            \end{split}
        \end{equation*}
        Por inducción (no es tan complicado), se prueba que:
        \begin{equation*}
            \int_{ -\infty}^\infty\varphi_n(x)^2dx=\pi^{1/2}2^n n!
        \end{equation*}
        con lo cual, el sistema formado por:
        \begin{equation*}
            \psi_n=\frac{\varphi_n}{\pi^{1/2}2^n n!}
        \end{equation*}
        para todo $n\in\mathbb{N}^*$ es un sistema ortonormal sobre $L_2(\mathbb{R},\mathbb{K})$, como se quería demostrar.

    \end{proof}

\end{document} 